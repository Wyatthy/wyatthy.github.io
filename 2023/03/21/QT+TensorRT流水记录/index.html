<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="QT关于数据读取只是提供了txt格式数据集的载入预览，之后的推理不提供对txt数据的支持。 DataLoader可以参考https:&#x2F;&#x2F;krshrimali.github.io&#x2F;posts&#x2F;2019&#x2F;07&#x2F;custom-data-loading-using-pytorch-c-api&#x2F; 和mat数据文件数据读取有关的代码有：chart.cpp&#39;Chart::readHRRPmat trti">
<meta property="og:type" content="article">
<meta property="og:title" content="QT+TensorRT流水记录">
<meta property="og:url" content="http://example.com/2023/03/21/QT+TensorRT%E6%B5%81%E6%B0%B4%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="WYATT&#39;S CAVE">
<meta property="og:description" content="QT关于数据读取只是提供了txt格式数据集的载入预览，之后的推理不提供对txt数据的支持。 DataLoader可以参考https:&#x2F;&#x2F;krshrimali.github.io&#x2F;posts&#x2F;2019&#x2F;07&#x2F;custom-data-loading-using-pytorch-c-api&#x2F; 和mat数据文件数据读取有关的代码有：chart.cpp&#39;Chart::readHRRPmat trti">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427806.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427158.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427545.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428203.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428712.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428737.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429339.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429773.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429730.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211430005.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211430725.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211433541.png">
<meta property="article:published_time" content="2023-03-21T06:34:50.677Z">
<meta property="article:modified_time" content="2023-03-21T06:52:30.955Z">
<meta property="article:author" content="wyatt">
<meta property="article:tag" content="记录">
<meta property="article:tag" content="项目">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427806.png">

<link rel="canonical" href="http://example.com/2023/03/21/QT+TensorRT%E6%B5%81%E6%B0%B4%E8%AE%B0%E5%BD%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>QT+TensorRT流水记录 | WYATT'S CAVE</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0561f7d8f733c45b7fb37fa9fbc220d3";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WYATT'S CAVE</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/21/QT+TensorRT%E6%B5%81%E6%B0%B4%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="wyatt">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WYATT'S CAVE">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          QT+TensorRT流水记录
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-03-21 14:34:50 / Modified: 14:52:30" itemprop="dateCreated datePublished" datetime="2023-03-21T14:34:50+08:00">2023-03-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%80%E5%8F%91%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">开发记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="QT"><a href="#QT" class="headerlink" title="QT"></a>QT</h1><h2 id="关于数据读取"><a href="#关于数据读取" class="headerlink" title="关于数据读取"></a>关于数据读取</h2><p>只是提供了<code>txt</code>格式数据集的载入预览，之后的推理不提供对<code>txt</code>数据的支持。</p>
<p>DataLoader可以参考<a target="_blank" rel="noopener" href="https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/">https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/</a></p>
<p>和mat数据文件数据读取有关的代码有：<code>chart.cpp&#39;Chart::readHRRPmat</code> <code>trtinfer.cpp&#39;getAllDataFromMat&amp;getDataFromMat</code></p>
<p>目前默认所有mat文件中要用的数据都放在”hrrp128”这个变量里（因为没有找到读取mat文件中所有变量的方法），当前用到这个默认变量名的方法有：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">trtInfer:getAllDataFromMat\getDataFromMat</span><br><span class="line">sensepage::nextBatchChart</span><br><span class="line">modelEvalPage::randSample</span><br><span class="line">Chart::readRadiomat\readHrrpmat</span><br></pre></td></tr></table></figure>

<p>trtInfer里的<code>dataSetClc</code>和加载的模型有关联：<br>在做单样本推理<code>testOneSample</code>的时候，先加载模型，得到模型的输入尺度，这样读数据的时候提供固定大小的数组，用样本数据里的数据填满。<br><code>testAllSample</code>的时候，同样先得到模型输入尺度，传给<code>dataSetClc</code>构造函数，使加载的每一个样本数据都和模型要求的输入大小一致。</p>
<p>使用label、classname字典的文件有SocketClient::run、monitorpage、inferThread</p>
<p>数据集中每个样本必须一样长(牵扯的代码比如<code>customdataset.h:getDataSpecifically</code>)</p>
<span id="more"></span>

<h2 id="关于模型推理"><a href="#关于模型推理" class="headerlink" title="关于模型推理"></a>关于模型推理</h2><p>设置推理批数的时候应该判断是否超过了转trt时<code>--maxShapes</code>参数中设置的批数。</p>
<p>系统载入的afs.trt模型文件来源需要是trainLogs的（trt文件同级目录下要有model文件夹且其中有attention.txt文件，相关代码在<code>ModelEvalPage::testAllSample</code>)。</p>
<h2 id="关于bug"><a href="#关于bug" class="headerlink" title="关于bug"></a>关于bug</h2><p>一定要把训练进程杀死之后再退出程序 不然再次启动程序crashed</p>
<h2 id="关于调用MATLAB函数生成的dll"><a href="#关于调用MATLAB函数生成的dll" class="headerlink" title="关于调用MATLAB函数生成的dll"></a>关于调用MATLAB函数生成的dll</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40647819/article/details/103152131">如何在C++程序（工程）中调用Matlab函数</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/King_zkk/article/details/105783638">Qt调用MATLAB 生成的dll经验分享</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36165459/article/details/81283932">C++调用Matlab生成的DLL动态链接库进行混合编程（win10+VS2015+Matlab2016b）</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011913417/article/details/102679274">C++中调用matlab的dll文件(解决初始化失败的问题)</a></p>
</blockquote>
<p>目前的Radio101转Hrrp128的mat代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">function retn = ToHrrp(sPath,dPath)</span><br><span class="line">    </span><br><span class="line">    retn=0;</span><br><span class="line">    data6 = load(sPath);</span><br><span class="line">    data6 = getfield(data6,&#x27;radio101&#x27;);</span><br><span class="line">    </span><br><span class="line">    X = data6;</span><br><span class="line">    </span><br><span class="line">    mid =size(X);</span><br><span class="line">    down_list = 1:1:mid(1);</span><br><span class="line">    X = X(down_list,:);</span><br><span class="line">    </span><br><span class="line">    [frqNum,dataNum] = size(X) </span><br><span class="line">    win = &#x27;hamming&#x27;;</span><br><span class="line">    N_fft = 2^nextpow2(frqNum);</span><br><span class="line">    </span><br><span class="line">    point = N_fft/frqNum;</span><br><span class="line">    </span><br><span class="line">    w = window(win, frqNum);</span><br><span class="line">    Rng0 = ifftshift(ifft(w,N_fft))*point;</span><br><span class="line">    maxRng0 = max(abs(Rng0));</span><br><span class="line">    </span><br><span class="line">    x = zeros(N_fft, dataNum);</span><br><span class="line">    for n = 1:dataNum</span><br><span class="line">        Xw = X(:,n).*w; % 鎵鏁版嵁鍔犵獥</span><br><span class="line">        x(:,n) = ifftshift(ifft(Xw,N_fft))*point; %IFFT鍙樻崲鍒版椂鍩?</span><br><span class="line">    end</span><br><span class="line">    x = x./maxRng0; %鍘婚櫎鍔犵獥瀵瑰箙搴︾殑褰卞搷</span><br><span class="line">    hrrp128 = log(abs(x));</span><br><span class="line">    % x_dB = log(abs(x))/log(20);</span><br><span class="line">    save(dPath,&#x27;hrrp128&#x27;)</span><br><span class="line">    retn=1;</span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><em><strong>（1）mex -setup</strong></em></p>
<p>在弹出的两行选项中选择： mex -setup C++</p>
<p>​            <img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427806.png" alt="format,png"></p>
<p><em><strong>（2） mbuild -setup</strong></em></p>
<p>在弹出的两行选项中选择： mex -setup C++ -client MBUILD</p>
<p>​        <img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427158.png" alt="format,png-16606396091991"></p>
<p> <strong>2.创建一个.m函数，生成C++文件</strong></p>
<p>根据工程需要编写一个.m文件，并按照下列指示生成相应的C++文件。</p>
<p><strong>（1）编写一个名为ZSLAdd.m的函数实现两个数相加</strong></p>
<p>​                <img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427545.png" alt="format,png-16606396092002"></p>
<p><strong>（2）编译生成C++文件</strong></p>
<p>将Matlab的当前目录打开至存储ZSLAdd.m的文件夹下，在Command Window里输入如下指令：</p>
<p>mcc -W cpplib:<strong>ZAdd</strong> -T link:lib <strong>ZAdd.m</strong> -C</p>
<p>加粗字体处更换为自己对应的m函数即可。</p>
<p>等待一段时间，会在当前目录下生成一系列的文件，其中，以下4个后缀名的文件比较重要： .lib, .h, .dll, .ctf。</p>
<p>在Qt中，pro里引入.h文件，.pri里引入动态库-lxxx，ctf、dll、lib三个文件放到构建目录里（release文件夹下）不然会<code>ToHrrpInitialize()</code>初始化失败。</p>
<h1 id="TensorRT-Example"><a href="#TensorRT-Example" class="headerlink" title="TensorRT Example"></a>TensorRT Example</h1><p>使用pytorch作为runtime的全流程：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/master/quickstart/IntroNotebooks/1.%20Introduction.ipynb">introduce</a> <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/master/quickstart/IntroNotebooks/4.%20Using%20PyTorch%20through%20ONNX.ipynb">introduce2</a>，在图像分割上的加速应用：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/quickstart/SemanticSegmentation/tutorial-runtime.ipynb">tutorial</a></p>
<p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428203.png" alt="image-20220708191053551"></p>
<h2 id="第一步-得到onnx模型（可选"><a href="#第一步-得到onnx模型（可选" class="headerlink" title="第一步 得到onnx模型（可选"></a>第一步 得到onnx模型（可选</h2><p>首先要拿到一个模型的onnx格式，使用pytorch时较简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;E:\\model_only_par.pt&#x27;</span>)</span><br><span class="line">onnx_save_path = <span class="string">&quot;E:\\model.onnx&quot;</span></span><br><span class="line">example_tensor = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>).to(device)</span><br><span class="line">torch.onnx.export(model,  <span class="comment"># model being run</span></span><br><span class="line">                          example_tensor,  <span class="comment"># model input (or a tuple for multiple inputs)</span></span><br><span class="line">                          onnx_save_path,</span><br><span class="line">                          verbose=<span class="literal">False</span>,  <span class="comment"># store the trained parameter weights inside the model file</span></span><br><span class="line">                          training=<span class="literal">False</span>,</span><br><span class="line">                          do_constant_folding=<span class="literal">True</span>,</span><br><span class="line">                          input_names=[<span class="string">&#x27;input&#x27;</span>],</span><br><span class="line">                          output_names=[<span class="string">&#x27;output&#x27;</span>]</span><br><span class="line">                          )</span><br></pre></td></tr></table></figure>

<p>其中<code>model</code>得是初始化了的对象。</p>
<p>使用onnx做一下推理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx,torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> rt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):</span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    outtensor=torch.tensor(file_data)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"><span class="built_in">input</span>=getTensorFromTXT(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\Ball_bottom_cone\\21.txt&quot;</span>).numpy()</span><br><span class="line"><span class="built_in">input</span>=<span class="built_in">input</span>.reshape([<span class="number">1</span>,<span class="number">1</span>,<span class="number">512</span>])</span><br><span class="line">sess = rt.InferenceSession(<span class="string">&#x27;E:\\model.onnx&#x27;</span>)</span><br><span class="line">input_name = sess.get_inputs()[<span class="number">0</span>].name</span><br><span class="line">label_name = sess.get_outputs()[<span class="number">0</span>].name</span><br><span class="line">pred_onx = sess.run([label_name], &#123;input_name:<span class="built_in">input</span>.astype(np.float32)&#125;)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(pred_onx)</span><br><span class="line"><span class="built_in">print</span>(np.argmax(pred_onx))</span><br></pre></td></tr></table></figure>



<h2 id="第二步-得到TensorRt引擎："><a href="#第二步-得到TensorRt引擎：" class="headerlink" title="第二步 得到TensorRt引擎："></a>第二步 得到TensorRt引擎：</h2><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://github.com/onnx/onnx-tensorrt">ONNX models can be converted to serialized TensorRT</a> engines using the <code>onnx2trt</code> executable:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onnx2trt my_model.onnx -o my_engine.trt</span><br></pre></td></tr></table></figure>

<p>也可以通过trtexec命令实现</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec.exe --onnx=E:/model.onnx --saveEngine=E:/resnet_engine.trt --explicitBatch=1</span><br></pre></td></tr></table></figure>

<h3 id="方法一：使用ONNX"><a href="#方法一：使用ONNX" class="headerlink" title="方法一：使用ONNX"></a>方法一：使用ONNX</h3><p>使用现有的ONNX模型，通过TensorRt的Parser解析然后填到网络对象中。步骤：</p>
<ol>
<li><p>建立一个logger日志，必须要有，但又不是那么重要 <code>static Logger gLogger;</code></p>
</li>
<li><p>创建一个builder </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IBuilder* builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br></pre></td></tr></table></figure></li>
<li><p>创建一个netwok，这时候netWork只是一个空架子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvinfer1::INetworkDefinition* network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br></pre></td></tr></table></figure></li>
<li><p>建立一个 Parser，caffe模型、onnx模型和TF模型都有对应的paser，顾名思义，就用用来解析模型文件的.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> parser = nvonnxparser::<span class="built_in">createParser</span>(*network, gLogger);<span class="comment">//这个parser为这个network服务</span></span><br><span class="line"><span class="comment">// 解析ONNX模型</span></span><br><span class="line">std::string onnx_filename = <span class="string">&quot;E:/model.onnx&quot;</span>;</span><br><span class="line">parser-&gt;<span class="built_in">parseFromFile</span>(onnx_filename.<span class="built_in">c_str</span>(), <span class="number">2</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; parser-&gt;<span class="built_in">getNbErrors</span>(); ++i)&#123;</span><br><span class="line">    std::cout &lt;&lt; parser-&gt;<span class="built_in">getError</span>(i)-&gt;<span class="built_in">desc</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>建立 engine，进行层之间融合或者进度校准方式，可以fp32、fp16或者fp8。方法：Builder(Net+Config)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建推理引擎</span></span><br><span class="line">IBuilderConfig* config = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">config-&gt;<span class="built_in">setMaxWorkspaceSize</span>(<span class="number">1</span> &lt;&lt; <span class="number">20</span>);</span><br><span class="line">config-&gt;<span class="built_in">setFlag</span>(nvinfer1::BuilderFlag::kFP16);</span><br><span class="line">ICudaEngine* engine = builder-&gt;<span class="built_in">buildEngineWithConfig</span>(*network, *config);</span><br></pre></td></tr></table></figure></li>
<li><p>建一个context，这个是用来做inference推断的。上面连接engine，下对应推断数据。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br></pre></td></tr></table></figure></li>
<li><p>做Inference（涉及到内存开辟传输，最好自己写函数封起来）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="type">float</span>* input, <span class="type">float</span>* output, <span class="type">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> ICudaEngine&amp; engine = context.<span class="built_in">getEngine</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Pointers to input and output device buffers to pass to engine.</span></span><br><span class="line">    <span class="comment">// Engine requires exactly IEngine::getNbBindings() number of buffers.</span></span><br><span class="line">    <span class="built_in">assert</span>(engine.<span class="built_in">getNbBindings</span>() == <span class="number">2</span>);</span><br><span class="line">    <span class="type">void</span>* buffers[<span class="number">2</span>] = &#123; <span class="literal">NULL</span>,<span class="literal">NULL</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In order to bind the buffers, we need to know the names of the input and output tensors.</span></span><br><span class="line">    <span class="comment">// Note that indices are guaranteed to be less than IEngine::getNbBindings()</span></span><br><span class="line">    <span class="comment">//const int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME);</span></span><br><span class="line">    <span class="comment">//const int outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME);</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create GPU buffers on device</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">0</span>], batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create stream</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(buffers[<span class="number">0</span>], input, batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">//开始推理</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;start to infer image...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    context.<span class="built_in">enqueue</span>(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(output, buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream and buffers</span></span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">0</span>]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">1</span>]));</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference Done.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Run inference</span></span><br><span class="line"><span class="type">float</span> data[<span class="number">512</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line"><span class="type">float</span> prob[<span class="number">5</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line"><span class="built_in">getTensorFromTXT</span>(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\DT\\22.txt&quot;</span>, data);</span><br><span class="line"></span><br><span class="line">LARGE_INTEGER t1, t2, tc;</span><br><span class="line"><span class="built_in">QueryPerformanceFrequency</span>(&amp;tc);</span><br><span class="line"><span class="built_in">QueryPerformanceCounter</span>(&amp;t1);</span><br><span class="line"><span class="built_in">doInference</span>(*context, data, prob, <span class="number">1</span>);</span><br><span class="line"><span class="built_in">QueryPerformanceCounter</span>(&amp;t2);</span><br><span class="line"><span class="type">double</span> time = (<span class="type">double</span>)(t2.QuadPart - t1.QuadPart) / (<span class="type">double</span>)tc.QuadPart;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;time = &quot;</span> &lt;&lt; time &lt;&lt; std::endl;  <span class="comment">//输出时间（单位：ｓ）</span></span><br></pre></td></tr></table></figure></li>
<li></li>
</ol>
<h3 id="方法二：反序列化trt引擎文件"><a href="#方法二：反序列化trt引擎文件" class="headerlink" title="方法二：反序列化trt引擎文件"></a>方法二：反序列化trt引擎文件</h3><p>方法一中如果每次推理都解析onnx会很慢，但在某次创建好engine之后序列化成trt文件保存下来，以后推理可以直接调引擎文件来创引擎，会快一点。</p>
<p><strong>engine保存为.trt文件</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Save .trt</span></span><br><span class="line">nvinfer1::IHostMemory* datas = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">std::ofstream file;</span><br><span class="line">file.<span class="built_in">open</span>(<span class="string">&quot;E:/model.trt&quot;</span>, std::ios::binary | std::ios::out);</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;writing engine file...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">file.<span class="built_in">write</span>((<span class="type">const</span> <span class="type">char</span>*)datas-&gt;<span class="built_in">data</span>(), datas-&gt;<span class="built_in">size</span>());</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;save engine file done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">file.<span class="built_in">close</span>();</span><br></pre></td></tr></table></figure>

<p><strong>调取.trt文件创建engine</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="type">const</span> std::string&amp; engineFile, IHostMemory*&amp; trtModelStream, ICudaEngine*&amp; engine)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::fstream file;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;loading filename from:&quot;</span> &lt;&lt; engineFile &lt;&lt; std::endl;</span><br><span class="line">    nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">    <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">    file.<span class="built_in">open</span>(engineFile, std::ios::binary | std::ios::in);</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::end);</span><br><span class="line">    <span class="type">int</span> length = file.<span class="built_in">tellg</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;length:&quot;</span> &lt;&lt; length &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::beg);</span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">char</span>[]&gt; <span class="title">data</span><span class="params">(<span class="keyword">new</span> <span class="type">char</span>[length])</span></span>;</span><br><span class="line">    file.<span class="built_in">read</span>(data.<span class="built_in">get</span>(), length);</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;load engine done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserializing&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtRuntime = <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>());</span><br><span class="line">    <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">    engine = trtRuntime-&gt;<span class="built_in">deserializeCudaEngine</span>(data.<span class="built_in">get</span>(), length, <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserialize done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The engine in TensorRT.cpp is not nullptr&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtModelStream = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">IHostMemory* modelStream&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">ICudaEngine* engine&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">read_TRT_File</span>(<span class="string">&quot;E:/model.trt&quot;</span>,modelStream, engine)) std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created successfully.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"><span class="keyword">else</span> std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line"><span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br></pre></td></tr></table></figure>



<h3 id="方法二：By网络权重文件wts"><a href="#方法二：By网络权重文件wts" class="headerlink" title="方法二：By网络权重文件wts"></a>方法二：By网络权重文件wts</h3><p>可以参考<a target="_blank" rel="noopener" href="https://github.com/wang-xinyu/tensorrtx/blob/master/lenet/lenet.cpp">Github项目</a>、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/344810135">知乎博客</a>，创建runtime反序列化engine</p>
<h1 id="TensorRt安装"><a href="#TensorRt安装" class="headerlink" title="TensorRt安装"></a>TensorRt安装</h1><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/nvidia-tensorrt-7x-download">下载TensorRt</a>并解压，把lib添加到PATH。听说cuda11.1匹配TensorRt7.2.3，因此本文使用7.2.3版本。</p>
<p>将解压后的bin, include, lib\ 目录复制到cuda安装路径下：</p>
<p>安装必要的包：<code>graphsurgeon</code>和<code>onnx_graphsurgeon</code>,俩包在TensorRt里面都有whl，<code>pip install xx.whl</code>即可。</p>
<p><strong>pycuda</strong>：</p>
<p>需要下载<strong>pycuda</strong>包：<a target="_blank" rel="noopener" href="https://www.lfd.uci.edu/~gohlke/pythonlibs/">https://www.lfd.uci.edu/~gohlke/pythonlibs/</a><br>本机cuda版本为11.1，因此在上面的网站中我下载了<code>pycuda-2021.1+cuda114-cp38-cp38-win_amd64.whl</code><br>在下载目录中执行<code>pip install pycuda-2021.1+cuda114-cp38-cp38-win_amd64.whl</code>以安装此包（取消代理</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a><strong>测试</strong></h3><ol>
<li>用VS打开解压包下的TensorRT-7.2.3\samples\sampleMNIST\sample_mnist.sln工程，然后选择重新生成。</li>
<li>使用python运行TensorRT-7.2.3\data\mnist下的download_pgms.py程序。</li>
<li>进入TensorRT-7.2.3\bin目录下，使用cmd命令来运行<code>sample_mnist.exe --datadir=your\path\to\TensorRT-7.2.3\data\mnist\</code></li>
</ol>
<p>报错发现cuDNN少了cublasLt64_10.dll和Zlib（<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/">Zlib</a> is a data compression software library that is needed by cuDNN）<br>遂<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#prerequisites-windows">下载</a>并安装Zlib；在CUDA的bin文件夹下，有个cublasLt64_11.dll，我就copy了一份改名成cublasLt64_10.dll，就不报它的错了。<br>Add the directory path of zlibwapi.dll to the environment variable PATH. 但是还是报错，于是我直接把dll放进CUDA的bin中。<br>但还是错，我开始觉得是cuDNN版本的问题，于是把cuDNN版本从8.3.3换到了<a target="_blank" rel="noopener" href="https://developer.nvidia.com/rdp/cudnn-archive">8.4.1</a>，成功运行！</p>
<p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428712.png" alt="image-20220707113756252"></p>
<h3 id="win-python38环境-import-tensorrt"><a href="#win-python38环境-import-tensorrt" class="headerlink" title="win+python38环境 import  tensorrt"></a>win+python38环境 import  tensorrt</h3><p>python环境实在导入不了tensorrt，因为按教程要先<code>pip install nvidia-pyindex</code>，但这个包我实在是下不来，最后通过换python版本（3.8.8-&gt;3.9.13)才解决下载pyindex的问题。（历时一下午）</p>
<p>现在pip下载tensorrt还是不行，尝试了几乎所有的方法都不行，也有看到说win下不支持python版本的trt（所以放弃import tensorrt，直接使用C++runtime）</p>
<h1 id="模型训练测试及部署"><a href="#模型训练测试及部署" class="headerlink" title="模型训练测试及部署"></a>模型训练测试及部署</h1><h2 id="pytorch训练识别hrrp模型的代码"><a href="#pytorch训练识别hrrp模型的代码" class="headerlink" title="pytorch训练识别hrrp模型的代码"></a>pytorch训练识别hrrp模型的代码</h2><p><code>getTensorFromTXT</code>中，list转tensor或者numpy呢？实践证明tensor精度没有numpy的高。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch,os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_from_folder</span>(<span class="params">datasetPath</span>):</span><br><span class="line">    ims, labels, class_list = [], [], []</span><br><span class="line">    g = os.walk(<span class="string">r&quot;E:\207Project\Data\HRRP&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> path, dir_list, file_list <span class="keyword">in</span> g:</span><br><span class="line">        <span class="keyword">for</span> dir_name <span class="keyword">in</span> dir_list:</span><br><span class="line">            class_list.append(dir_name)</span><br><span class="line">    class_index = <span class="built_in">dict</span>(<span class="built_in">zip</span>(class_list, <span class="built_in">range</span>(<span class="built_in">len</span>(class_list))))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;类别对应序号：&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(class_index)</span><br><span class="line">    g = os.walk(<span class="string">r&quot;E:\207Project\Data\HRRP&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> path, dir_list, file_list <span class="keyword">in</span> g:</span><br><span class="line">        <span class="keyword">for</span> file_name <span class="keyword">in</span> file_list:</span><br><span class="line">            <span class="keyword">if</span> (file_name[file_name.rfind(<span class="string">&#x27;.&#x27;</span>) + <span class="number">1</span>:] != <span class="string">&quot;txt&quot;</span>):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            ims.append(os.path.join(path, file_name))</span><br><span class="line">            im_class = path[path.rfind(<span class="string">&#x27;\\&#x27;</span>) + <span class="number">1</span>:]</span><br><span class="line">            labels.append(<span class="built_in">int</span>(class_index[im_class]))</span><br><span class="line">    <span class="keyword">return</span> ims, labels</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):</span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    outtensor=torch.tensor(file_data)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, datasetPath, trainOrtest</span>):</span><br><span class="line">        self.ims, self.labels = load_data_from_folder(datasetPath)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        im = getTensorFromTXT(self.ims[index])</span><br><span class="line">        label = self.labels[index]</span><br><span class="line">        <span class="keyword">return</span> im, label</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.ims)</span><br><span class="line"></span><br><span class="line">train_dataset= mDataset(<span class="string">r&quot;E:\207Project\Data\HRRP&quot;</span>,<span class="number">0</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = <span class="number">1</span>, shuffle = <span class="literal">True</span>, num_workers = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv1d(<span class="number">1</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv1d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4000</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">5</span>) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=F.relu(self.conv1(x))</span><br><span class="line">        x=F.max_pool1d(x,<span class="number">2</span>)</span><br><span class="line">        x=F.relu(self.conv2(x))</span><br><span class="line">        x=F.max_pool1d(x,<span class="number">2</span>)</span><br><span class="line">        x=x.view(-<span class="number">1</span>,<span class="number">4000</span>)</span><br><span class="line">        x=F.relu(self.fc1(x))</span><br><span class="line">        x=self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line"><span class="comment">#device=torch.device(&quot;cpu&quot;)</span></span><br><span class="line">model=CNN().to(device)</span><br><span class="line">optimizer=optim.Adam(model.parameters(),lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model,device,train_loader,optimizer,epoch,losses</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> idx,(t_data,t_target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="built_in">input</span> = Variable(t_data).cuda()</span><br><span class="line">        target = Variable(t_target).cuda().long()</span><br><span class="line">        pred=model(<span class="built_in">input</span>)</span><br><span class="line">        loss=F.nll_loss(pred,target)</span><br><span class="line">        <span class="comment">#Adam</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> idx%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoch:&#123;&#125;,iteration:&#123;&#125;,loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch,idx,loss.item()))</span><br><span class="line">            losses.append(loss.item())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model,device,test_loader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct=<span class="number">0</span><span class="comment">#预测对了几个。</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> idx,(t_data,t_target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">            t_data,t_target=t_data.to(device),t_target.to(device)</span><br><span class="line">            pred=model(t_data)<span class="comment">#batch_size*2</span></span><br><span class="line">            pred_class=pred.argmax(dim=<span class="number">1</span>)<span class="comment">#batch_size*2-&gt;batch_size*1</span></span><br><span class="line">            correct+=pred_class.eq(t_target.view_as(pred_class)).<span class="built_in">sum</span>().item()</span><br><span class="line">    acc=correct/<span class="built_in">len</span>(test_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;accuracy:&#123;&#125;,average_loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(acc,average_loss))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs=<span class="number">5</span></span><br><span class="line">losses=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    train(model,device,train_loader,optimizer,epoch,losses)</span><br><span class="line"></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;E:\\model_only_par.pt&#x27;</span>)</span><br><span class="line">onnx_save_path = <span class="string">&quot;E:\\model.onnx&quot;</span></span><br><span class="line">example_tensor = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>).to(device)</span><br><span class="line">torch.onnx.export(model,  <span class="comment"># model being run</span></span><br><span class="line">                  example_tensor,  <span class="comment"># model input (or a tuple for multiple inputs)</span></span><br><span class="line">                  onnx_save_path,</span><br><span class="line">                  verbose=<span class="literal">False</span>,  <span class="comment"># store the trained parameter weights inside the model file</span></span><br><span class="line">                  training=<span class="literal">False</span>,</span><br><span class="line">                  do_constant_folding=<span class="literal">True</span>,</span><br><span class="line">                  input_names=[<span class="string">&#x27;input&#x27;</span>],</span><br><span class="line">                  output_names=[<span class="string">&#x27;output&#x27;</span>]</span><br><span class="line">                 )</span><br></pre></td></tr></table></figure>

<h2 id="读取pytorch模型-进行推理识别"><a href="#读取pytorch模型-进行推理识别" class="headerlink" title="读取pytorch模型 进行推理识别"></a>读取pytorch模型 进行推理识别</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch,os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv1d(<span class="number">1</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv1d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4000</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">5</span>)  <span class="comment"># 这个也不一样，因为是2分类问题。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=F.relu(self.conv1(x))</span><br><span class="line">        x=F.max_pool1d(x,<span class="number">2</span>)</span><br><span class="line">        x=F.relu(self.conv2(x))</span><br><span class="line">        x=F.max_pool1d(x,<span class="number">2</span>)</span><br><span class="line">        x=x.view(-<span class="number">1</span>,<span class="number">4000</span>)</span><br><span class="line">        x=F.relu(self.fc1(x))</span><br><span class="line">        x=self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line"><span class="comment">#device=torch.device(&quot;cpu&quot;)</span></span><br><span class="line">model=CNN().to(device)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;E:\\model_only_par.pt&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):</span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    outtensor=torch.tensor(file_data)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"><span class="built_in">input</span>=getTensorFromTXT(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\DT\\21.txt&quot;</span>)</span><br><span class="line"><span class="built_in">input</span>=<span class="built_in">input</span>.reshape([<span class="number">1</span>,<span class="number">1</span>,<span class="number">512</span>]).to(device)</span><br><span class="line">output=model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<h2 id="调用onnx模型进行推理"><a href="#调用onnx模型进行推理" class="headerlink" title="调用onnx模型进行推理"></a>调用onnx模型进行推理</h2><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428737.png" alt="image-20220710165612452"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx,torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> rt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):</span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    <span class="comment">#outtensor=torch.tensor(file_data)</span></span><br><span class="line">    outtensor = np.array(file_data, np.float32)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"><span class="built_in">input</span>=getTensorFromTXT(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\Ball_bottom_cone\\21.txt&quot;</span>).numpy()</span><br><span class="line"><span class="built_in">input</span>=<span class="built_in">input</span>.reshape([<span class="number">1</span>,<span class="number">1</span>,<span class="number">512</span>])</span><br><span class="line">sess = rt.InferenceSession(<span class="string">&#x27;E:\\model.onnx&#x27;</span>)</span><br><span class="line">input_name = sess.get_inputs()[<span class="number">0</span>].name</span><br><span class="line">label_name = sess.get_outputs()[<span class="number">0</span>].name</span><br><span class="line">pred_onx = sess.run([label_name], &#123;input_name:<span class="built_in">input</span>.astype(np.float32)&#125;)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(pred_onx)</span><br><span class="line"><span class="built_in">print</span>(np.argmax(pred_onx))</span><br></pre></td></tr></table></figure>

<h2 id="初次加速推理"><a href="#初次加速推理" class="headerlink" title="初次加速推理"></a>初次加速推理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">Input filename:   E:/model.onnx</span><br><span class="line">ONNX IR version:  0.0.7</span><br><span class="line">Opset version:    9</span><br><span class="line">Producer name:    pytorch</span><br><span class="line">Producer version: 1.10</span><br><span class="line">Domain:</span><br><span class="line">Model version:    0</span><br><span class="line">Doc string:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">[07/09/2022-16:20:16] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">tensorRT load onnx mnist model...</span><br><span class="line">[07/09/2022-16:20:17] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[07/09/2022-16:20:57] [W] [TRT] Try increasing the workspace size to 4194304 bytes to get better performance.</span><br><span class="line">[07/09/2022-16:21:07] [W] [TRT] Try increasing the workspace size to 4194304 bytes to get better performance.</span><br><span class="line">[07/09/2022-16:21:11] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[07/09/2022-16:21:11] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">input_blob_name : input</span><br><span class="line">output_blob_name : output</span><br><span class="line">inputH : 1, inputW: 512</span><br><span class="line">start to infer image...</span><br><span class="line">Inference Done.</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"></span><br><span class="line">-385.375, -181.5, -289.125, -621.375, 0,</span><br><span class="line"></span><br><span class="line">D:\code\CPP\tensorrtProj\build\Debug\tensorrtProj.exe (进程 8908)已退出，代码为 0。</span><br><span class="line">要在调试停止时自动关闭控制台，请启用“工具”-&gt;“选项”-&gt;“调试”-&gt;“调试停止时自动关闭控制台”。</span><br><span class="line">按任意键关闭此窗口. . .</span><br></pre></td></tr></table></figure>

<h2 id="V2修改tensor为numpy"><a href="#V2修改tensor为numpy" class="headerlink" title="V2修改tensor为numpy"></a>V2修改tensor为numpy</h2><p>torch版本换了一下，换成GPU的</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">Input filename:   E:/model.onnx</span><br><span class="line">ONNX IR version:  0.0.7</span><br><span class="line">Opset version:    13</span><br><span class="line">Producer name:    pytorch</span><br><span class="line">Producer version: 1.12.0</span><br><span class="line">Domain:</span><br><span class="line">Model version:    0</span><br><span class="line">Doc string:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">[07/10/2022-18:26:03] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">tensorRT load onnx mnist model...</span><br><span class="line">[07/10/2022-18:26:05] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[07/10/2022-18:27:00] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[07/10/2022-18:27:00] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">input_blob_name : input</span><br><span class="line">output_blob_name : output</span><br><span class="line">inputH : 1, inputW: 512</span><br><span class="line">start to infer image...</span><br><span class="line">Inference Done.</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"></span><br><span class="line">-143.479, -205.623, -4.53847, -619.006, -0.0107473,</span><br></pre></td></tr></table></figure>

<p>V3使用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">loading filename from:E:/model.trt</span><br><span class="line">length:4129238</span><br><span class="line">load engine done</span><br><span class="line">deserializing</span><br><span class="line">[07/11/2022-14:23:37] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">deserialize done</span><br><span class="line">The engine in TensorRT.cpp is not nullptr</span><br><span class="line">tensorRT engine created successfully.</span><br><span class="line">[07/11/2022-14:23:37] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">start to infer </span><br><span class="line">Inference Done.</span><br><span class="line">time = 0.0018287</span><br><span class="line">Output:</span><br><span class="line">-9.46875, -17.5312, -10.1875, -0.000114679, -21.8438,</span><br></pre></td></tr></table></figure>

<h2 id="C-onnx转trt和推理"><a href="#C-onnx转trt和推理" class="headerlink" title="C++ onnx转trt和推理"></a>C++ onnx转trt和推理</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// stuff we know about the network and the input/output blobs</span></span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_H = <span class="number">1</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_W = <span class="number">512</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> OUTPUT_SIZE = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* INPUT_BLOB_NAME = <span class="string">&quot;data&quot;</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* OUTPUT_BLOB_NAME = <span class="string">&quot;prob&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Load weights from files shared with TensorRT samples.</span></span><br><span class="line"><span class="comment">// TensorRT weight files have a simple space delimited format:</span></span><br><span class="line"><span class="comment">// [type] [size] &lt;data x size in hex&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">getTensorFromTXT</span><span class="params">(std::string data_path,<span class="type">float</span>* y)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> r, n = <span class="number">0</span>; <span class="type">double</span> d; FILE* f;</span><br><span class="line">    <span class="type">float</span> temp[<span class="number">1024</span>];</span><br><span class="line">    f = <span class="built_in">fopen</span>(data_path.<span class="built_in">c_str</span>(), <span class="string">&quot;r&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) <span class="built_in">fscanf</span>(f, <span class="string">&quot;%*[^\n]%*c&quot;</span>); <span class="comment">// 跳两行</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1024</span>; i++) &#123;</span><br><span class="line">        r = <span class="built_in">fscanf</span>(f, <span class="string">&quot;%lf&quot;</span>, &amp;d);</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">1</span> == r) temp[n++] = d;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (<span class="number">0</span> == r) <span class="built_in">fscanf</span>(f, <span class="string">&quot;%*c&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">fclose</span>(f);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">512</span>; i++) &#123;</span><br><span class="line">        y[i] = temp[i*<span class="number">2</span> + <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; features; <span class="comment">//临时特征向量</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; <span class="number">512</span>; ++d)</span><br><span class="line">        features.<span class="built_in">push_back</span>(y[d]);</span><br><span class="line">    <span class="comment">//特征归一化</span></span><br><span class="line">    <span class="type">float</span> dMaxValue = *std::<span class="built_in">max_element</span>(features.<span class="built_in">begin</span>(), features.<span class="built_in">end</span>());  <span class="comment">//求最大值</span></span><br><span class="line">    <span class="type">float</span> dMinValue = *std::<span class="built_in">min_element</span>(features.<span class="built_in">begin</span>(), features.<span class="built_in">end</span>());  <span class="comment">//求最小值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> f = <span class="number">0</span>; f &lt; features.<span class="built_in">size</span>(); ++f) &#123;</span><br><span class="line">        y[f] = (y[f] - dMinValue) / (dMaxValue - dMinValue + <span class="number">1e-8</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    features.<span class="built_in">clear</span>();<span class="comment">//删除容器</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="type">float</span>* input, <span class="type">float</span>* output, <span class="type">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//const ICudaEngine&amp; engine = context.getEngine();</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//// Pointers to input and output device buffers to pass to engine.</span></span><br><span class="line">    <span class="comment">//// Engine requires exactly IEngine::getNbBindings() number of buffers.</span></span><br><span class="line">    <span class="comment">//assert(engine.getNbBindings() == 2);</span></span><br><span class="line">    <span class="type">void</span>* buffers[<span class="number">2</span>] = &#123; <span class="literal">NULL</span>,<span class="literal">NULL</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In order to bind the buffers, we need to know the names of the input and output tensors.</span></span><br><span class="line">    <span class="comment">// Note that indices are guaranteed to be less than IEngine::getNbBindings()</span></span><br><span class="line">    <span class="comment">//const int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME);</span></span><br><span class="line">    <span class="comment">//const int outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME);</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create GPU buffers on device</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">0</span>], batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*for (int i = 0; i &lt; batchSize * INPUT_H * INPUT_W; i++) &#123;</span></span><br><span class="line"><span class="comment">        std::cout &lt;&lt; input[i] &lt;&lt; &quot; &quot;;</span></span><br><span class="line"><span class="comment">    &#125;std::cout &lt;&lt; std::endl&lt;&lt;&quot;输出向量展示完毕&quot;&lt;&lt;std::endl;*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create stream</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(buffers[<span class="number">0</span>], input, batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">//开始推理</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;start to infer ...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    context.<span class="built_in">enqueue</span>(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(output, buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream and buffers</span></span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">0</span>]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">1</span>]));</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference Done.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="type">const</span> std::string&amp; engineFile, IHostMemory*&amp; trtModelStream, ICudaEngine*&amp; engine)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::fstream file;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;loading filename from:&quot;</span> &lt;&lt; engineFile &lt;&lt; std::endl;</span><br><span class="line">    nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">    <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">    file.<span class="built_in">open</span>(engineFile, std::ios::binary | std::ios::in);</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::end);</span><br><span class="line">    <span class="type">int</span> length = file.<span class="built_in">tellg</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;length:&quot;</span> &lt;&lt; length &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::beg);</span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">char</span>[]&gt; <span class="title">data</span><span class="params">(<span class="keyword">new</span> <span class="type">char</span>[length])</span></span>;</span><br><span class="line">    file.<span class="built_in">read</span>(data.<span class="built_in">get</span>(), length);</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;load engine done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserializing&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtRuntime = <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>());</span><br><span class="line">    <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">    engine = trtRuntime-&gt;<span class="built_in">deserializeCudaEngine</span>(data.<span class="built_in">get</span>(), length, <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserialize done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The engine in TensorRT.cpp is not nullptr&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtModelStream = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//IBuilder* builder = createInferBuilder(gLogger);</span></span><br><span class="line">    <span class="comment">//nvinfer1::INetworkDefinition* network = builder-&gt;createNetworkV2(1U &lt;&lt; static_cast&lt;uint32_t&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span></span><br><span class="line">    <span class="comment">//auto parser = nvonnxparser::createParser(*network, gLogger);</span></span><br><span class="line">    <span class="comment">//// 解析ONNX模型</span></span><br><span class="line">    <span class="comment">//std::string onnx_filename = &quot;E:/model.onnx&quot;;</span></span><br><span class="line">    <span class="comment">//parser-&gt;parseFromFile(onnx_filename.c_str(), 2);</span></span><br><span class="line">    <span class="comment">//for (int i = 0; i &lt; parser-&gt;getNbErrors(); ++i)</span></span><br><span class="line">    <span class="comment">//&#123;</span></span><br><span class="line">    <span class="comment">//    std::cout &lt;&lt; parser-&gt;getError(i)-&gt;desc() &lt;&lt; std::endl;</span></span><br><span class="line">    <span class="comment">//&#125;</span></span><br><span class="line">    <span class="comment">//printf(&quot;tensorRT load onnx model...\n&quot;);</span></span><br><span class="line">    <span class="comment">//// 创建推理引擎</span></span><br><span class="line">    <span class="comment">//IBuilderConfig* config = builder-&gt;createBuilderConfig();</span></span><br><span class="line">    <span class="comment">//assert(config != nullptr);</span></span><br><span class="line">    <span class="comment">//config-&gt;setMaxWorkspaceSize(1 &lt;&lt; 22);//4194304</span></span><br><span class="line">    <span class="comment">//config-&gt;setFlag(nvinfer1::BuilderFlag::kFP16);</span></span><br><span class="line">    <span class="comment">//ICudaEngine* engine = builder-&gt;buildEngineWithConfig(*network, *config);</span></span><br><span class="line">    <span class="comment">//assert(engine != nullptr);</span></span><br><span class="line">    <span class="comment">//IExecutionContext* context = engine-&gt;createExecutionContext();</span></span><br><span class="line">    <span class="comment">//assert(context != nullptr);</span></span><br><span class="line">    IHostMemory* modelStream&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    ICudaEngine* engine&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">read_TRT_File</span>(<span class="string">&quot;E:/model.trt&quot;</span>,modelStream, engine)) std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created successfully.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">else</span> std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//// 获取输入与输出名称，格式</span></span><br><span class="line">    <span class="comment">//const char* input_blob_name = network-&gt;getInput(0)-&gt;getName();</span></span><br><span class="line">    <span class="comment">//const char* output_blob_name = network-&gt;getOutput(0)-&gt;getName();</span></span><br><span class="line">    <span class="comment">//printf(&quot;input_blob_name : %s \n&quot;, input_blob_name);</span></span><br><span class="line">    <span class="comment">//printf(&quot;output_blob_name : %s \n&quot;, output_blob_name);</span></span><br><span class="line">    <span class="comment">//const int inputH = network-&gt;getInput(0)-&gt;getDimensions().d[1];</span></span><br><span class="line">    <span class="comment">//const int inputW = network-&gt;getInput(0)-&gt;getDimensions().d[2];</span></span><br><span class="line">    <span class="comment">//printf(&quot;inputH : %d, inputW: %d \n&quot;, inputH, inputW);</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Run inference</span></span><br><span class="line">    <span class="type">float</span> data[<span class="number">512</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="type">float</span> prob[<span class="number">5</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="built_in">getTensorFromTXT</span>(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\Ball_bottom_cone\\21.txt&quot;</span>, data);</span><br><span class="line"></span><br><span class="line">    LARGE_INTEGER t1, t2, tc;</span><br><span class="line">    <span class="built_in">QueryPerformanceFrequency</span>(&amp;tc);</span><br><span class="line">    <span class="built_in">QueryPerformanceCounter</span>(&amp;t1);</span><br><span class="line">    <span class="built_in">doInference</span>(*context, data, prob, <span class="number">1</span>);</span><br><span class="line">    <span class="built_in">QueryPerformanceCounter</span>(&amp;t2);</span><br><span class="line">    <span class="type">double</span> time = (<span class="type">double</span>)(t2.QuadPart - t1.QuadPart) / (<span class="type">double</span>)tc.QuadPart;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;time = &quot;</span> &lt;&lt; time &lt;&lt; std::endl;  <span class="comment">//输出时间（单位：ｓ）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print histogram of the output distribution</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output:\n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; prob[i] &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//Save .trt</span></span><br><span class="line">    <span class="comment">/*nvinfer1::IHostMemory* datas = engine-&gt;serialize();</span></span><br><span class="line"><span class="comment">    std::ofstream file;</span></span><br><span class="line"><span class="comment">    file.open(&quot;E:/model.trt&quot;, std::ios::binary | std::ios::out);</span></span><br><span class="line"><span class="comment">    std::cout &lt;&lt; &quot;writing engine file...&quot; &lt;&lt; std::endl;</span></span><br><span class="line"><span class="comment">    file.write((const char*)datas-&gt;data(), datas-&gt;size());</span></span><br><span class="line"><span class="comment">    std::cout &lt;&lt; &quot;save engine file done&quot; &lt;&lt; std::endl;</span></span><br><span class="line"><span class="comment">    file.close();*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Onnx转trt"><a href="#Onnx转trt" class="headerlink" title="Onnx转trt"></a>Onnx转trt</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    IBuilder* builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br><span class="line">    nvinfer1::INetworkDefinition* network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br><span class="line">    <span class="keyword">auto</span> parser = nvonnxparser::<span class="built_in">createParser</span>(*network, gLogger);</span><br><span class="line">    <span class="comment">// 解析ONNX模型</span></span><br><span class="line">    std::string onnx_filename = <span class="string">&quot;E:/model.onnx&quot;</span>;</span><br><span class="line">    parser-&gt;<span class="built_in">parseFromFile</span>(onnx_filename.<span class="built_in">c_str</span>(), <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; parser-&gt;<span class="built_in">getNbErrors</span>(); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; parser-&gt;<span class="built_in">getError</span>(i)-&gt;<span class="built_in">desc</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;tensorRT load onnx model...\n&quot;</span>);</span><br><span class="line">    <span class="comment">// 创建推理引擎</span></span><br><span class="line">    IBuilderConfig* config = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">    <span class="built_in">assert</span>(config != <span class="literal">nullptr</span>);</span><br><span class="line">    config-&gt;<span class="built_in">setMaxWorkspaceSize</span>(<span class="number">1</span> &lt;&lt; <span class="number">22</span>);<span class="comment">//4194304</span></span><br><span class="line">    config-&gt;<span class="built_in">setFlag</span>(nvinfer1::BuilderFlag::kFP16);</span><br><span class="line">    ICudaEngine* engine = builder-&gt;<span class="built_in">buildEngineWithConfig</span>(*network, *config);</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Save .trt</span></span><br><span class="line">    nvinfer1::IHostMemory* datas = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    std::ofstream file;</span><br><span class="line">    file.<span class="built_in">open</span>(<span class="string">&quot;E:/model.trt&quot;</span>, std::ios::binary | std::ios::out);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;writing engine file...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">write</span>((<span class="type">const</span> <span class="type">char</span>*)datas-&gt;<span class="built_in">data</span>(), datas-&gt;<span class="built_in">size</span>());</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;save engine file done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>pytorch的onnx转trt：</p>
<p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429339.png" alt="image-20220720214726506"></p>
<h2 id="读trt文件然后推理"><a href="#读trt文件然后推理" class="headerlink" title="读trt文件然后推理"></a>读trt文件然后推理</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// stuff we know about the network and the input/output blobs</span></span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_H = <span class="number">1</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_W = <span class="number">512</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> OUTPUT_SIZE = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* INPUT_BLOB_NAME = <span class="string">&quot;data&quot;</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* OUTPUT_BLOB_NAME = <span class="string">&quot;prob&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">getTensorFromTXT</span><span class="params">(std::string data_path, <span class="type">float</span>* y)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> r, n = <span class="number">0</span>; <span class="type">double</span> d; FILE* f;</span><br><span class="line">    <span class="type">float</span> temp[<span class="number">1024</span>];</span><br><span class="line">    f = <span class="built_in">fopen</span>(data_path.<span class="built_in">c_str</span>(), <span class="string">&quot;r&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) <span class="built_in">fscanf</span>(f, <span class="string">&quot;%*[^\n]%*c&quot;</span>); <span class="comment">// 跳两行</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1024</span>; i++) &#123;</span><br><span class="line">        r = <span class="built_in">fscanf</span>(f, <span class="string">&quot;%lf&quot;</span>, &amp;d);</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">1</span> == r) temp[n++] = d;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (<span class="number">0</span> == r) <span class="built_in">fscanf</span>(f, <span class="string">&quot;%*c&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">fclose</span>(f);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">512</span>; i++) &#123;</span><br><span class="line">        y[i] = temp[i * <span class="number">2</span> + <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; features; <span class="comment">//临时特征向量</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; <span class="number">512</span>; ++d)</span><br><span class="line">        features.<span class="built_in">push_back</span>(y[d]);</span><br><span class="line">    <span class="comment">//特征归一化</span></span><br><span class="line">    <span class="type">float</span> dMaxValue = *std::<span class="built_in">max_element</span>(features.<span class="built_in">begin</span>(), features.<span class="built_in">end</span>());  <span class="comment">//求最大值</span></span><br><span class="line">    <span class="type">float</span> dMinValue = *std::<span class="built_in">min_element</span>(features.<span class="built_in">begin</span>(), features.<span class="built_in">end</span>());  <span class="comment">//求最小值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> f = <span class="number">0</span>; f &lt; features.<span class="built_in">size</span>(); ++f) &#123;</span><br><span class="line">        y[f] = (y[f] - dMinValue) / (dMaxValue - dMinValue + <span class="number">1e-8</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    features.<span class="built_in">clear</span>();<span class="comment">//删除容器</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="type">float</span>* input, <span class="type">float</span>* output, <span class="type">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//const ICudaEngine&amp; engine = context.getEngine();</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//// Pointers to input and output device buffers to pass to engine.</span></span><br><span class="line">    <span class="comment">//// Engine requires exactly IEngine::getNbBindings() number of buffers.</span></span><br><span class="line">    <span class="comment">//assert(engine.getNbBindings() == 2);</span></span><br><span class="line">    <span class="type">void</span>* buffers[<span class="number">2</span>] = &#123; <span class="literal">NULL</span>,<span class="literal">NULL</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In order to bind the buffers, we need to know the names of the input and output tensors.</span></span><br><span class="line">    <span class="comment">// Note that indices are guaranteed to be less than IEngine::getNbBindings()</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">0</span>], batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="comment">/*for (int i = 0; i &lt; batchSize * INPUT_H * INPUT_W; i++) &#123;</span></span><br><span class="line"><span class="comment">        std::cout &lt;&lt; input[i] &lt;&lt; &quot; &quot;;</span></span><br><span class="line"><span class="comment">    &#125;std::cout &lt;&lt; std::endl&lt;&lt;&quot;输出向量展示完毕&quot;&lt;&lt;std::endl;*/</span></span><br><span class="line">    <span class="comment">// Create stream</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line">    <span class="comment">// DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(buffers[<span class="number">0</span>], input, batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">//开始推理</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;start to infer ...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    context.<span class="built_in">enqueue</span>(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(output, buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream and buffers</span></span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">0</span>]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">1</span>]));</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference Done.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="type">const</span> std::string&amp; engineFile, IHostMemory*&amp; trtModelStream, ICudaEngine*&amp; engine)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::fstream file;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;loading filename from:&quot;</span> &lt;&lt; engineFile &lt;&lt; std::endl;</span><br><span class="line">    nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">    <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">    file.<span class="built_in">open</span>(engineFile, std::ios::binary | std::ios::in);</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::end);</span><br><span class="line">    <span class="type">int</span> length = file.<span class="built_in">tellg</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;length:&quot;</span> &lt;&lt; length &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::beg);</span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">char</span>[]&gt; <span class="title">data</span><span class="params">(<span class="keyword">new</span> <span class="type">char</span>[length])</span></span>;</span><br><span class="line">    file.<span class="built_in">read</span>(data.<span class="built_in">get</span>(), length);</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;load engine done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserializing&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtRuntime = <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>());</span><br><span class="line">    <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">    engine = trtRuntime-&gt;<span class="built_in">deserializeCudaEngine</span>(data.<span class="built_in">get</span>(), length, <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserialize done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The engine in TensorRT.cpp is not nullptr&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtModelStream = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    IHostMemory* modelStream&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    ICudaEngine* engine&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">read_TRT_File</span>(<span class="string">&quot;E:/model.trt&quot;</span>, modelStream, engine)) std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created successfully.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">else</span> std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="comment">// Run inference</span></span><br><span class="line">    <span class="type">float</span> data[<span class="number">512</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="type">float</span> prob[<span class="number">5</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="built_in">getTensorFromTXT</span>(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\Ball_bottom_cone\\21.txt&quot;</span>, data);</span><br><span class="line"></span><br><span class="line">    LARGE_INTEGER t1, t2, tc;</span><br><span class="line">    <span class="built_in">QueryPerformanceFrequency</span>(&amp;tc);</span><br><span class="line">    <span class="built_in">QueryPerformanceCounter</span>(&amp;t1);</span><br><span class="line">    <span class="built_in">doInference</span>(*context, data, prob, <span class="number">1</span>);</span><br><span class="line">    <span class="built_in">QueryPerformanceCounter</span>(&amp;t2);</span><br><span class="line">    <span class="type">double</span> time = (<span class="type">double</span>)(t2.QuadPart - t1.QuadPart) / (<span class="type">double</span>)tc.QuadPart;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;time = &quot;</span> &lt;&lt; time &lt;&lt; std::endl;  <span class="comment">//输出时间（单位：ｓ）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print histogram of the output distribution</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output:\n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; prob[i] &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="设置为动态输入"><a href="#设置为动态输入" class="headerlink" title="设置为动态输入"></a>设置为动态输入</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    nvinfer1::Dims mPredictionInputDims;  <span class="comment">//!&lt; The dimensions of the input of the model.</span></span><br><span class="line">    nvinfer1::Dims mPredictionOutputDims; <span class="comment">//!&lt; The dimensions of the output of the model.</span></span><br><span class="line"></span><br><span class="line">    IBuilder* builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br><span class="line">    <span class="comment">//Creating the preprocessing network</span></span><br><span class="line">    <span class="keyword">auto</span> preprocessorNetwork = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">int32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br><span class="line">    <span class="keyword">auto</span> input = preprocessorNetwork-&gt;<span class="built_in">addInput</span>(<span class="string">&quot;input&quot;</span>, nvinfer1::DataType::kFLOAT, Dims4&#123; <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span> &#125;);</span><br><span class="line">    <span class="keyword">auto</span> resizeLayer = preprocessorNetwork-&gt;<span class="built_in">addResize</span>(*input);</span><br><span class="line">    resizeLayer-&gt;<span class="built_in">setOutputDimensions</span>(mPredictionInputDims);</span><br><span class="line">    preprocessorNetwork-&gt;<span class="built_in">markOutput</span>(*resizeLayer-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>));</span><br><span class="line">    <span class="comment">//create an empty full-dims network, and parser</span></span><br><span class="line">    nvinfer1::INetworkDefinition* network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br><span class="line">    <span class="keyword">auto</span> parser = nvonnxparser::<span class="built_in">createParser</span>(*network, gLogger);</span><br><span class="line">    <span class="comment">//parse the model file to populate the network</span></span><br><span class="line">    std::string onnx_filename = <span class="string">&quot;E:/tfmodel_speciInput.onnx&quot;</span>;</span><br><span class="line">    parser-&gt;<span class="built_in">parseFromFile</span>(onnx_filename.<span class="built_in">c_str</span>(), <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; parser-&gt;<span class="built_in">getNbErrors</span>(); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; parser-&gt;<span class="built_in">getError</span>(i)-&gt;<span class="built_in">desc</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;tensorRT load onnx model...\n&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//configure optimization profile &amp; preprocess engine</span></span><br><span class="line">    <span class="keyword">auto</span> preprocessorConfig = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">    <span class="keyword">auto</span> profile = builder-&gt;<span class="built_in">createOptimizationProfile</span>();</span><br><span class="line">    profile-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMIN, Dims4&#123; <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span> &#125;);</span><br><span class="line">    profile-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kOPT, Dims4&#123; <span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span> &#125;);</span><br><span class="line">    profile-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMAX, Dims4&#123; <span class="number">1</span>, <span class="number">1</span>, <span class="number">56</span>, <span class="number">56</span> &#125;);</span><br><span class="line">    preprocessorConfig-&gt;<span class="built_in">addOptimizationProfile</span>(profile);</span><br><span class="line">    <span class="comment">//Create an optimization profile for calibration</span></span><br><span class="line">    <span class="keyword">auto</span> profileCalib = builder-&gt;<span class="built_in">createOptimizationProfile</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> calibBatchSize&#123; <span class="number">256</span> &#125;;</span><br><span class="line">    profileCalib-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMIN, Dims4&#123; calibBatchSize, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span> &#125;);</span><br><span class="line">    profileCalib-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kOPT, Dims4&#123; calibBatchSize, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span> &#125;);</span><br><span class="line">    profileCalib-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMAX, Dims4&#123; calibBatchSize, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span> &#125;);</span><br><span class="line">    preprocessorConfig-&gt;<span class="built_in">setCalibrationProfile</span>(profileCalib);</span><br><span class="line">    <span class="comment">//Run engine build with config</span></span><br><span class="line">    <span class="keyword">auto</span> proprocessEngine=builder-&gt;<span class="built_in">buildEngineWithConfig</span>(*preprocessorNetwork, *preprocessorConfig);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建推理引擎</span></span><br><span class="line">    IBuilderConfig* config = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">    <span class="built_in">assert</span>(config != <span class="literal">nullptr</span>);</span><br><span class="line">    config-&gt;<span class="built_in">setMaxWorkspaceSize</span>(<span class="number">1</span> &lt;&lt; <span class="number">22</span>);<span class="comment">//4194304</span></span><br><span class="line">    config-&gt;<span class="built_in">setFlag</span>(nvinfer1::BuilderFlag::kFP16);</span><br><span class="line">    ICudaEngine* engine = builder-&gt;<span class="built_in">buildEngineWithConfig</span>(*network, *config);</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Save .trt</span></span><br><span class="line">    nvinfer1::IHostMemory* datas = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    std::ofstream file;</span><br><span class="line">    file.<span class="built_in">open</span>(<span class="string">&quot;E:/tfmodel_speciInput.trt&quot;</span>, std::ios::binary | std::ios::out);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;writing engine file...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">write</span>((<span class="type">const</span> <span class="type">char</span>*)datas-&gt;<span class="built_in">data</span>(), datas-&gt;<span class="built_in">size</span>());</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;save engine file done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="TensorFlow模型保存与载入"><a href="#TensorFlow模型保存与载入" class="headerlink" title="TensorFlow模型保存与载入"></a>TensorFlow模型保存与载入</h2><p>TF1使用Session 图模式，需要先定义图再执行，流行于工业界。TF2则是Eager模式，同Pytorch，写到哪执行到哪儿，在TF2通过Keras搭配<code>tf.function</code>的训练推理性能很差。</p>
<h3 id="保存为ckpt模型"><a href="#保存为ckpt模型" class="headerlink" title="保存为ckpt模型"></a>保存为ckpt模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">&quot;w1-name&quot;</span>)</span><br><span class="line">w2 = tf.Variable(tf.constant(<span class="number">3.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">&quot;w2-name&quot;</span>)</span><br><span class="line"></span><br><span class="line">a = tf.placeholder(dtype=tf.float32, name=<span class="string">&quot;a-name&quot;</span>)</span><br><span class="line">b = tf.placeholder(dtype=tf.float32, name=<span class="string">&quot;b-name&quot;</span>)</span><br><span class="line"></span><br><span class="line">y = a * w1 + b * w2</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="built_in">print</span>(a)  <span class="comment"># Tensor(&quot;a-name:0&quot;, dtype=float32)</span></span><br><span class="line">    <span class="built_in">print</span>(b)  <span class="comment"># Tensor(&quot;b-name:0&quot;, dtype=float32)</span></span><br><span class="line">    <span class="built_in">print</span>(y)  <span class="comment"># Tensor(&quot;add:0&quot;, dtype=float32)</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(y, feed_dict=&#123;a: <span class="number">10</span>, b: <span class="number">10</span>&#125;))</span><br><span class="line">    saver.save(sess, <span class="string">&quot;./model/model.ckpt&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>TensorFlow模型会保存在后缀为.ckpt的文件中。保存后在save这个文件夹中实际会出现3个文件，因为TensorFlow会将计算图的结构和图上参数取值分开保存。</p>
<ul>
<li><code>model.ckpt.meta</code>文件保存了TensorFlow计算图的结构，可以理解为神经网络的网络结构</li>
<li><code>model.ckpt</code>文件保存了TensorFlow程序中每一个变量的取值</li>
<li><code>checkpoint</code>文件保存了一个目录下所有的模型文件列表</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429773.png" alt="image-20220727160501389"></p>
<h3 id="加载ckpt模型"><a href="#加载ckpt模型" class="headerlink" title="加载ckpt模型"></a>加载ckpt模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">&quot;./model/model.ckpt.meta&quot;</span>)</span><br><span class="line">graph = tf.get_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 Tensor 名获取变量</span></span><br><span class="line">a = graph.get_tensor_by_name(<span class="string">&quot;a-name:0&quot;</span>)</span><br><span class="line">b = graph.get_tensor_by_name(<span class="string">&quot;b-name:0&quot;</span>)</span><br><span class="line">y = graph.get_tensor_by_name(<span class="string">&quot;add:0&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">&quot;./model/model.ckpt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(sess.run(y, feed_dict=&#123;a: <span class="number">10</span>, b: <span class="number">10</span>&#125;))</span><br></pre></td></tr></table></figure>



<h1 id="TensorFlow模型部署记录"><a href="#TensorFlow模型部署记录" class="headerlink" title="TensorFlow模型部署记录"></a>TensorFlow模型部署记录</h1><blockquote>
<p>TensorRT和Tensorflow的数据格式不一样，Tensorflow是<strong>NHWC</strong>格式，即channel_last，而TensorRT中是<strong>NCHW</strong>格式，即channel_first，比如一张RGB图像，在Tensorflow中表示为（224， 224， 3），在TensorRT中就是（3，224， 224）。所以使用TensorRT时，请一定确认图像的格式。</p>
</blockquote>
<p>TensorFlow模型保存为三种格式：saved_model、checkpoint、graphdef。其中graphdef保存得到的.pb文件网络模型中均为冻结了的常量。</p>
<p>saved_model–&gt;onnx–&gt;trt    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#saved_model格式保存，保存生成的是一个文件夹my_model</span></span><br><span class="line">model.save(<span class="string">&#x27;saved_model/my_model&#x27;</span>) </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="language-bash">saved_model--&gt;onnx</span></span><br><span class="line">python -m tf2onnx.convert --saved-model saved_model/my_model --output saved_model/tfmodel.onnx</span><br></pre></td></tr></table></figure>

<p>最后在C++用onnx生成trt引擎的时候报错，因为是输入量为动态的。saved model保存的是一整个训练图，并且参数没有冻结。而只用于模型推理serving并不需要完整的训练图，并且参数不冻结无法进行转TensorRT等极致优化。当然也可以saved_model-&gt;frozen pb-&gt;saved model来同时利用两者的优点。</p>
<blockquote>
<p>补：后来发现命令行直接使用<code>trtexec --onnx=E:/tfmodel.onnx --saveEngine=E:/tfmodel.trt</code>也转成了，就是没测是不是能推理。</p>
</blockquote>
<p>两种解决办法：</p>
<p>一：写成动态的(尝试了没成功，参考<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/main/samples/sampleDynamicReshape/README.md">TensorRTSample</a>)</p>
<p>二：设置成静态的，写死的NCHW；  可以通过保存为.pb类型的模型，pb转onnx再转trt</p>
<p>在这之前我尝试了通过修改onnx输入层的维度，把本来的unk改成了确定的1。但还是识别为动态的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx.checker</span><br><span class="line"><span class="keyword">import</span> onnx.utils</span><br><span class="line"><span class="keyword">from</span> onnx.tools <span class="keyword">import</span> update_model_dims</span><br><span class="line"></span><br><span class="line">model = onnx.load(<span class="string">&#x27;E:/tfmodel.onnx&#x27;</span>)</span><br><span class="line"><span class="comment"># 此处可以理解为获得了一个维度 “引用”，通过该 “引用“可以修改其对应的维度</span></span><br><span class="line">dim_proto0 = model.graph.<span class="built_in">input</span>[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 将该维度赋值为字符串，其维度不再为和dummy_input绑定的值</span></span><br><span class="line">dim_proto0.dim_param = <span class="string">&#x27;1&#x27;</span></span><br><span class="line">dim_proto_0 = model.graph.output[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">0</span>]</span><br><span class="line">dim_proto_0.dim_param = <span class="string">&#x27;1&#x27;</span></span><br><span class="line">onnx.save(model, <span class="string">&#x27;E:/tfmodel_hardInput0.onnx&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>但解决不了问题，还是会报错   <strong>↑</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#TensorFlow在目标文件夹下保存为.pb模型</span></span><br><span class="line">tf.keras.models.save_model(model,<span class="string">&quot;E:/tfmodels/&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="language-bash">.pb--&gt;onnx</span></span><br><span class="line">python -m tf2onnx.convert --graphdef tensorflow-model-graphdef-file --output model.onnx --inputs input0:0,input1:0 --outputs output0:0</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-graphdef：需要进行转换的pb模型</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">--output：转换后的onnx模型名称</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">-inputs：pb模型输入层的名字</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">--outputs：pb模型输出层的名字</span></span><br></pre></td></tr></table></figure>

<p><strong>↑</strong>  问题在于pb转onnx的时候需要提供输入输出层的结点名称。因此需要使用TensorFlow自带的工具：**<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#using-the-graph-transform-tool">summarize_graph</a><strong>，summarize_graph可以查看网络节点，在只有一个固化的权重文件而不知道具体的网络结构时非常有用。下载及使用教程</strong>↓**：</p>
<p>首先需要<a target="_blank" rel="noopener" href="https://docs.bazel.build/versions/main/install-windows.html">安装<strong>bazel</strong></a>，它的功能类似于make。然后下载TensorFlow的源码（推荐使用git clone）。进入源码目录中，执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel build tensorflow/tools/graph_transforms:summarize_graph</span><br></pre></td></tr></table></figure>

<p>没成功，报错：<code>ERROR: An error occurred during the fetch of repository &#39;local_execution_config_python&#39;:</code><br>根据他的<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/issues/48264">解决办法</a>，是<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/source_windows">TensorFlow构建</a>的问题,这个网页里提到的都要做到，下载缺少的<a target="_blank" rel="noopener" href="https://www.msys2.org/">MSYS2</a>（我是缺少了这个）。（在捣鼓MSY32时，记得使用管理员权限）。其中在configure.py的时候，ROCm和CUDA不能同时选，否则有错。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">PS D:\code\python\tensorflow\tensorflow&gt; python ./configure.py</span><br><span class="line">You have bazel 6.0.0-pre.20220630.1 installed.</span><br><span class="line">Please specify the location of python. [Default is D:\evn\Python39\python.exe]:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Found possible Python library paths:</span><br><span class="line">  D:\evn\Python39\lib\site-packages</span><br><span class="line">Please input the desired Python library path to use.  Default is [D:\evn\Python39\lib\site-packages]</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with ROCm support? [y/N]: N</span><br><span class="line">No ROCm support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N]: y</span><br><span class="line">CUDA support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with TensorRT support? [y/N]: y</span><br><span class="line">TensorRT support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">WARNING: TensorRT support on Windows is experimental</span><br><span class="line"></span><br><span class="line">Found CUDA 11.1 in:</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib/x64</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include</span><br><span class="line">Found cuDNN 8 in:</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib/x64</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include</span><br><span class="line">Found TensorRT 7 in:</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please specify a list of comma-separated CUDA compute capabilities you want to build with.</span><br><span class="line">You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as &quot;x.y&quot; or &quot;compute_xy&quot; to include both virtual and binary GPU code, or as &quot;sm_xy&quot; to only include the binary code.</span><br><span class="line">Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities &gt;= 3.5 [Default is: 3.5,7.0]:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is /arch:AVX]:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y</span><br><span class="line">Eigen strong inline overridden.</span><br><span class="line"></span><br><span class="line">Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N</span><br><span class="line">Not configuring the WORKSPACE for Android builds.</span><br><span class="line"></span><br><span class="line">Preconfigured Bazel build configs. You can use any of the below by adding &quot;--config=&lt;&gt;&quot; to your build command. See .bazelrc for more details.</span><br><span class="line">        --config=mkl            # Build with MKL support.</span><br><span class="line">        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).</span><br><span class="line">        --config=monolithic     # Config for mostly static monolithic build.</span><br><span class="line">        --config=numa           # Build with NUMA support.</span><br><span class="line">        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.</span><br><span class="line">        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.</span><br><span class="line">Preconfigured Bazel build configs to DISABLE default on features:</span><br><span class="line">        --config=nogcp          # Disable GCP support.</span><br><span class="line">        --config=nonccl         # Disable NVIDIA NCCL support.</span><br><span class="line">PS D:\code\python\tensorflow\tensorflow&gt;</span><br></pre></td></tr></table></figure>

<p>然后再执行 还是不行…..：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ERROR: An error occurred during the fetch of repository &#x27;local_config_python&#x27;:</span><br><span class="line">   Traceback (most recent call last):</span><br><span class="line">        File &quot;D:/code/python/tensorflow/tensorflow/third_party/py/python_configure.bzl&quot;, line 271, column 40, in _python_autoconf_impl</span><br><span class="line">                _create_local_python_repository(repository_ctx)</span><br><span class="line">        File &quot;D:/code/python/tensorflow/tensorflow/third_party/py/python_configure.bzl&quot;, line 213, column 33, in _create_local_python_repository</span><br><span class="line">                python_lib = _get_python_lib(repository_ctx, python_bin)</span><br><span class="line">        File &quot;D:/code/python/tensorflow/tensorflow/third_party/py/python_configure.bzl&quot;, line 130, column 21, in _get_python_lib</span><br><span class="line">                result = execute(repository_ctx, [python_bin, &quot;-c&quot;, cmd])</span><br><span class="line">        File &quot;D:/code/python/tensorflow/tensorflow/third_party/remote_config/common.bzl&quot;, line 230, column 13, in execute</span><br><span class="line">                fail(</span><br><span class="line">Error in fail: Repository command failed</span><br></pre></td></tr></table></figure>

<p><strong>保存为pb：</strong><br>freeze graph（需要output_node_names</p>
<p>通过代码获取到的output_node_names TensorFlow说不对：<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/issues/3986#">Freeze graph: node is not in graph</a></p>
<p>看了很多解决方法，指定了一些output_node_names，但都说图中不存在，因此必须得看pb文件才能确定。</p>
<p>于是去查看saved_model文件夹下的<code>saved_model.pb</code>文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = &#x27;saved_model/dense121/saved_model/saved_model.pb&#x27;   #请将这里的model.pb文件路径改为自己的</span></span><br><span class="line"><span class="comment"># graph = tf.compat.v1.get_default_graph()</span></span><br><span class="line"><span class="comment"># graph_def = graph.as_graph_def()</span></span><br><span class="line"><span class="comment"># graph_def.ParseFromString(tf.compat.v1.gfile.FastGFile(model, &#x27;rb&#x27;).read())</span></span><br><span class="line"><span class="comment"># tf.graph_util.import_graph_def(graph_def, name=&#x27;graph&#x27;)</span></span><br><span class="line"><span class="comment"># summaryWriter = tf.summary.FileWriter(&#x27;log/&#x27;, graph)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;saved_model/dense121/saved_model/saved_model.pb&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.compat.v1.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        <span class="built_in">print</span> (graph_def)</span><br></pre></td></tr></table></figure>

<p>报错：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph_def.ParseFromString(f.read())</span><br><span class="line">google.protobuf.message.DecodeError: Error parsing message with type &#x27;tensorflow.GraphDef&#x27;</span><br></pre></td></tr></table></figure>

<p>编码错误，有人说可能是这个pb不全，所以我干脆…不知道</p>
<p>ckpt转pb：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64099452">https://zhuanlan.zhihu.com/p/64099452</a></p>
<p>从NVIDIA里的<a target="_blank" rel="noopener" href="https://blogs.nvidia.com.tw/2022/01/04/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/">一个博客</a>成功导出了pb和onnx。</p>
<blockquote>
<p>將 TensorFlow 模型轉換成 ONNX 檔案的方式有很多種。其中之一是 ResNet50 一節中解釋的方式。Keras 也擁有本身的 Keras 轉 ONNX 檔案轉換器。有時候，TensorFlow 轉 ONNX 不支援某些層，但是 Keras 轉 ONNX 轉換器支援。視 Keras 框架和使用的層類型而定，可能必須在轉換器之間選擇。</p>
</blockquote>
<h2 id="model2pb-py"><a href="#model2pb-py" class="headerlink" title="model2pb.py"></a>model2pb.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">keras_to_pb</span>(<span class="params">model, output_filename, output_node_names</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This is the function to convert the keras model to pb.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">       model: The keras model.</span></span><br><span class="line"><span class="string">       output_filename: The output .pb file name.</span></span><br><span class="line"><span class="string">       output_node_names: The output nodes of the network (if None,</span></span><br><span class="line"><span class="string">       the function gets the last layer name as the output node).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sess = tf.compat.v1.keras.backend.get_session()</span><br><span class="line">    graph = sess.graph</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> graph.as_default():</span><br><span class="line">        <span class="comment"># Get names of input and output nodes.</span></span><br><span class="line">        in_name = model.layers[<span class="number">0</span>].get_output_at(<span class="number">0</span>).name.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_node_names <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            output_node_names = [model.layers[-<span class="number">1</span>].get_output_at(<span class="number">0</span>).name.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">        graph_def = graph.as_graph_def()</span><br><span class="line">        frozen_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(</span><br><span class="line">            sess,</span><br><span class="line">            graph_def,</span><br><span class="line">            output_node_names)</span><br><span class="line"></span><br><span class="line">    sess.close()</span><br><span class="line">    wkdir = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    tf.compat.v1.train.write_graph(frozen_graph_def, wkdir, output_filename, as_text=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> in_name, output_node_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="comment"># Disable eager execution in tensorflow 2 is required.</span></span><br><span class="line">    tf.compat.v1.disable_eager_execution()</span><br><span class="line">    <span class="comment"># Set learning phase to Test.</span></span><br><span class="line">    tf.compat.v1.keras.backend.set_learning_phase(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load ResNet50 model pre-trained on imagenet</span></span><br><span class="line">    model = tf.keras.applications.ResNet50(</span><br><span class="line">        include_top=<span class="literal">True</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>, input_tensor=<span class="literal">None</span>,</span><br><span class="line">        input_shape=<span class="literal">None</span>, pooling=<span class="literal">None</span>, classes=<span class="number">1000</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert keras ResNet50 model to .pb file</span></span><br><span class="line">    in_tensor_name, out_tensor_names = keras_to_pb(model, args.output_pb_file, <span class="literal">None</span>)</span><br><span class="line">    <span class="built_in">print</span>(in_tensor_name)</span><br><span class="line">    <span class="built_in">print</span>(out_tensor_names)</span><br><span class="line">    <span class="comment"># # You can also use keras2onnx</span></span><br><span class="line">    <span class="comment"># onnx_model = keras2onnx.convert_keras(model, model.name, target_opset=11)</span></span><br><span class="line">    <span class="comment"># keras2onnx.save_model(onnx_model, &quot;resnet.onnx&quot;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--output_pb_file&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;saved_model/dense121/pb_model/resnet50.pb&#x27;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    main(args)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">input_1</span></span><br><span class="line"><span class="string">[&#x27;predictions/Softmax&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#得到pb文件后执行以下命令得到onnx</span></span><br><span class="line"><span class="comment">#python -m tf2onnx.convert  --input saved_model/dense121/pb_model/resnet50.pb --inputs input_1:0 --outputs predictions/Softmax:0 --output saved_model/dense121/onnx_model/resnet50.onnx --opset 11</span></span><br><span class="line"><span class="comment">#python -m tf2onnx.convert  --input saved_model/dense121/pb_model/lxb.pb --inputs Placeholder --outputs save/restore_all --output saved_model/dense121/onnx_model/lxb.onnx --opset 11</span></span><br><span class="line"><span class="comment">#python -m tf2onnx.convert  --input E:/cifar10.pb --inputs Input:0 --outputs Identity:0 --output E:/cifar10.onnx --opset 11</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>从onnx文件中得到输入向量维度信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> engine <span class="keyword">as</span> eng</span><br><span class="line"><span class="keyword">from</span> onnx <span class="keyword">import</span> ModelProto</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">engine_name = <span class="string">&#x27;semantic.plan&#x27;</span></span><br><span class="line">onnx_path = <span class="string">&quot;semantic.onnx&quot;</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">model = ModelProto()</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(onnx_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">  model.ParseFromString(f.read())</span><br><span class="line"></span><br><span class="line">d0 = model.graph.<span class="built_in">input</span>[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">1</span>].dim_value</span><br><span class="line">d1 = model.graph.<span class="built_in">input</span>[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">2</span>].dim_value</span><br><span class="line">d2 = model.graph.<span class="built_in">input</span>[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">3</span>].dim_value</span><br><span class="line">shape = [batch_size , d0, d1 ,d2]</span><br><span class="line">engine = eng.build_engine(onnx_path, shape= shape)</span><br><span class="line">eng.save_engine(engine, engine_name)</span><br></pre></td></tr></table></figure>



<h2 id="onnx转trt"><a href="#onnx转trt" class="headerlink" title="onnx转trt"></a>onnx转trt</h2><p>此时onnx模型的输入向量维度可以通过netron看到是**<code>float32[unk__1220,224,224,3]</code>**,格式是TF的NHWC.</p>
<blockquote>
<p>（<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_29007291/article/details/116135737">trtexec的用法</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/HW140701/article/details/120360642">TensorRT - 自带工具trtexec的参数使用说明</a>，<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec">官方介绍文档</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011622208/article/details/120132973?spm=1001.2014.3001.5502">测试博客</a>）</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./trtexec --onnx=xxx.onnx --saveEngine=xxx.trt --workspace=1024 --minShapes=inputx:1x3x480x640 --optShapes=inputx:16x3x480x640 --maxShapes=inputx:32x3x480x640 --fp16</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=dense121_output100.onnx --saveEngine=dense121_output100.trt --workspace=4096 --minShapes=input_1:0:1x224x224x3 --optShapes=input_1:0:1x224x224x3 --maxShapes=input_1:0:32x224x224x3 --fp16</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=cifar10.onnx --saveEngine=cifar10.trt --workspace=4096 --minShapes=Input:0:1x32x32x3 --optShapes=Input:0:1x32x32x3 --maxShapes=Input:0:50x32x32x3 --fp16</span><br></pre></td></tr></table></figure>



<p>关于报错：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[W] Dynamic dimensions required for input: input_1:0, but no shapes were provided. Automatically overriding shape to: 1x224x224x3</span><br><span class="line"><span class="meta">#</span><span class="language-bash">这是因为Shapes参数处，输入节点的名字有错误，应该是input_1:0而不是input_1。直接和netron上显示的结点name保持一致即可</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[E] [TRT] input_1:0: for dimension number 1 in profile 0 does not match network definition (got min=3, opt=3, max=3), expected min=opt=max=224).</span><br><span class="line"><span class="meta">#</span><span class="language-bash">Shapes参数1x3x224x224改成1x224x224x3即可</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ERROR: builtin_op_importers.cpp:2593 In function importResize:</span><br><span class="line">[8] Assertion failed: (mode != &quot;nearest&quot; || nearest_mode == &quot;floor&quot;) &amp;&amp; &quot;This version of TensorRT only supports floor nearest_mode!&quot;</span><br><span class="line">[07/28/2022-12:54:39] [E] Failed to parse onnx file</span><br><span class="line"><span class="meta">#</span><span class="language-bash">模型中resize(nearest-ceil model)算子不支持</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[E] [TRT] C:\source\rtSafe\cuda\cudaConvolutionRunner.cpp (483) - Cudnn Error in nvinfer1::rt::cuda::CudnnConvolutionRunner::executeConv: 2 (CUDNN_STATUS_ALLOC_FAILED)</span><br><span class="line"><span class="meta">#</span><span class="language-bash">--workspace参数设置的太大了  调小一点</span></span><br></pre></td></tr></table></figure>



<ul>
<li>onnx: 输入的onnx模型</li>
<li>saveEngine：转换好后保存的tensorrt engine</li>
<li>workspace：使用的gpu内存，有时候不够，需要手动增大点   单位是MB</li>
<li>minShapes：动态尺寸时的最小尺寸，格式为<strong>NCHW</strong>，需要给定输入node的名字，</li>
<li>optShapes：推理测试的尺寸，trtexec会执行推理测试，该shape就是测试时的输入shape</li>
<li>maxShapes：动态尺寸时的最大尺寸，这里只有batch是动态的，其他维度都是写死的</li>
<li>fp16：float16推理</li>
</ul>
<p>【要点】动态输入的onnx此时需要指定输入的shape范围，<strong>注意</strong>只是范围，得到的trt经过deserialize得到engine，在调用engine时需要指定维度。否则报错：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[E] [TRT] Parameter check failed at: engine.cpp::nvinfer1::rt::ShapeMachineContext::resolveSlots::1318, condition: allInputDimensionsSpecified(routine)</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/XCCCCZ/article/details/123009816">解决办法：</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查看engine的输入输出维度</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; engine-&gt;<span class="built_in">getNbBindings</span>(); i++)</span><br><span class="line">&#123;</span><br><span class="line">    nvinfer1::Dims dims = engine-&gt;<span class="built_in">getBindingDimensions</span>(i);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;index %d, dims: (&quot;</span>,i);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; dims.nbDims; d++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (d &lt; dims.nbDims - <span class="number">1</span>)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d,&quot;</span>, dims.d[d]);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, dims.d[d]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;)\n&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>以DenseNet121的trt文件为例，以上程序输出</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index 0, dims: (-1,224,224,3)</span><br><span class="line">index 1, dims: (-1,100)</span><br></pre></td></tr></table></figure>

<p>所以我们得把输入的动态维度写死，在python里，在调用engine推理前做这样的设置即可:<code>context.set_binding_shape(0, (BATCH, 3, INPUT_H, INPUT_W))</code>，C++代码里应该调用IExecutionContext类型的实例的setBindingDimensions(int bindingIndex, Dims dimensions)方法。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//确定动态维度</span></span><br><span class="line">nvinfer1::Dims dims4;</span><br><span class="line">dims4.d[<span class="number">0</span>] = <span class="number">1</span>;    <span class="comment">// replace dynamic batch size with 1</span></span><br><span class="line">dims4.d[<span class="number">1</span>] = <span class="number">224</span>;</span><br><span class="line">dims4.d[<span class="number">2</span>] = <span class="number">224</span>;</span><br><span class="line">dims4.d[<span class="number">3</span>] = <span class="number">3</span>;</span><br><span class="line">dims4.nbDims = <span class="number">4</span>;</span><br><span class="line">context-&gt;<span class="built_in">setBindingDimensions</span>(<span class="number">0</span>, dims4);</span><br></pre></td></tr></table></figure>

<p>然后再执行推理就可以了。</p>
<p>总体思路是：拿到一个对维度未知的模型engine文件后，首先读入文件内容并做deserialize获得engine。<br>然后调用getBindingDimensions()查看engine的输入输出维度(如果知道维度就不用)。<br>在调用context-&gt;executeV2()做推理前把维度值为-1的动态维度值替换成具体的维度并调用context-&gt;setBindingDimensions()设置具体维度，然后在数据填入input buffer准备好后调用context-&gt;executeV2()做推理即可:</p>
<p>为什么是V2，V1V2有什么区别：</p>
<blockquote>
<p>execute/enqueue are for <strong>implicit</strong> batch networks, and executeV2/enqueue<strong>V2 are for explicit</strong> batch networks. The V2 versions don’t take a batch_size argument since it’s taken from the explicit batch dimension of the network / or from the optimization profile if used.</p>
<p>In TensorRT 7, the ONNX parser <strong>requires</strong> that you <strong>create an explicit batch network</strong>, so you’ll have to use V2 methods.</p>
</blockquote>
<h2 id="以CIFAR10为例，训练模型并部署测试（C-）"><a href="#以CIFAR10为例，训练模型并部署测试（C-）" class="headerlink" title="以CIFAR10为例，训练模型并部署测试（C++）"></a>以CIFAR10为例，训练模型并部署测试（C++）</h2><p>参考文章：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ea8d1n3/article/details/123430217">tensorflow2 cifar10 模型训练 demo</a><br><a target="_blank" rel="noopener" href="https://blogs.nvidia.com.tw/2022/01/04/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/">使用 TensorFlow、ONNX 和 NVIDIA TensorRT 加快深度學習推論</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64099452">[深度学习] TensorFlow中模型的freeze_graph</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011026329/article/details/79190347">TensorFlow模型保存和加载方法</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/daydream13580130043/article/details/110405077">使用TF实现DenseNet并在CIFAR10数据集上进行分类任务</a><br><a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100164830/74e1ab0c">TensorFlow网络模型移植&amp;训练指南 01</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/thehappysheep/article/details/106247808">tensorflow 模型持久化（ckpt转pb模型）</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/IT_xiao_bai/article/details/108953938">Keras训练的h5文件转pb文件并用Tensorflow加载</a><br><a target="_blank" rel="noopener" href="https://daimajiaoliu.com/daima/569e03e71656401">TensorFlow2.0模型格式转换为.pb格式</a><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1800743">推理演示 | 八步助你搞定tensorRT C++ SDK调用！</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011622208/article/details/120132973?spm=1001.2014.3001.5502">【tensorrt】——trtexec动态batch支持与batch推理耗时评测</a><br><a target="_blank" rel="noopener" href="https://leimao.github.io/blog/Save-Load-Inference-From-TF2-Frozen-Graph/">Save, Load and Inference From TensorFlow 2.x Frozen Graph</a></p>
</blockquote>
<h3 id="成功的单样本测试"><a href="#成功的单样本测试" class="headerlink" title="成功的单样本测试"></a>成功的单样本测试</h3><p>保存了<strong>随机初始化权重</strong>的pb模型：<code>dense121_output100.pb</code>，根据上面的流程转成了<code>dense121_output100.trt</code>引擎文件。python调取pb对网上下载的一个plane图片做推理，代码如下：（<a target="_blank" rel="noopener" href="https://blog.csdn.net/pengpengloveqiaoqiao/article/details/110391113">参考</a>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line"><span class="keyword">with</span> gfile.FastGFile(<span class="string">r&#x27;D:\code\python\pycharmProject\PytorchProj\saved_model\dense121\pb_model\dense121_output100.pb&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    graph_def = tf.GraphDef()</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line">    sess.graph.as_default()</span><br><span class="line">    tf.import_graph_def(graph_def, name=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    opname = [tensor.name <span class="keyword">for</span> tensor <span class="keyword">in</span> tf.get_default_graph().as_graph_def().node]</span><br><span class="line">    <span class="built_in">print</span>(opname)   <span class="comment">#查看pb nodename</span></span><br><span class="line"><span class="comment"># 获取输入tensor</span></span><br><span class="line">x = tf.get_default_graph().get_tensor_by_name(</span><br><span class="line">    <span class="string">&quot;input_1:0&quot;</span>)  <span class="comment"># 不知道输入名时通过节点名查，一般情况下是每一个节点tf.get_default_graph().as_graph_def().node[0].name,名字构成后有个:0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input:&quot;</span>, x)</span><br><span class="line"><span class="comment"># 获取预测tensor</span></span><br><span class="line">pred = tf.get_default_graph().get_tensor_by_name(</span><br><span class="line">    <span class="string">&quot;predictions/Softmax:0&quot;</span>)  <span class="comment"># tf.get_default_graph().as_graph_def().node[-1].name，有可能不是是最后一一个</span></span><br><span class="line"><span class="built_in">print</span>(pred)</span><br><span class="line"></span><br><span class="line">tx=cv2.imread(<span class="string">&quot;E:/plane.jpg&quot;</span>)</span><br><span class="line">pre = sess.run(pred, feed_dict=&#123;x: tx.reshape(<span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>) / <span class="number">255</span>&#125;)  <span class="comment"># 预测直接run输出，传入输入</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Prediction: &quot;</span> + <span class="built_in">str</span>(pre))</span><br><span class="line"><span class="built_in">print</span>(pre.<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;input_1&#x27;</span>, <span class="string">&#x27;zero_padding2d/Pad/paddings&#x27;</span>, <span class="string">&#x27;zero_padding2d/Pad&#x27;</span>, <span class="string">&#x27;conv1/conv/kernel&#x27;</span>, </span><br><span class="line">。。。 </span><br><span class="line"><span class="string">&#x27;predictions/MatMul/ReadVariableOp&#x27;</span>, <span class="string">&#x27;predictions/MatMul&#x27;</span>, <span class="string">&#x27;predictions/BiasAdd/ReadVariableOp&#x27;</span>, <span class="string">&#x27;predictions/BiasAdd&#x27;</span>, <span class="string">&#x27;predictions/Softmax&#x27;</span>]</span><br><span class="line"><span class="built_in">input</span>: Tensor(<span class="string">&quot;input_1:0&quot;</span>, shape=(<span class="literal">None</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>), dtype=float32)</span><br><span class="line">Tensor(<span class="string">&quot;predictions/Softmax:0&quot;</span>, shape=(<span class="literal">None</span>, <span class="number">100</span>), dtype=float32)</span><br><span class="line"></span><br><span class="line">Prediction: [[<span class="number">0.00983849</span> <span class="number">0.01052127</span> <span class="number">0.00983031</span> <span class="number">0.01043278</span> <span class="number">0.00971659</span> <span class="number">0.01052411</span></span><br><span class="line">  <span class="number">0.00976097</span> <span class="number">0.01017774</span> <span class="number">0.01003768</span> <span class="number">0.01022743</span> <span class="number">0.01027896</span> <span class="number">0.00987673</span></span><br><span class="line">  <span class="number">0.01036694</span> <span class="number">0.00980142</span> <span class="number">0.01010968</span> <span class="number">0.01021501</span> <span class="number">0.00979544</span> <span class="number">0.00993549</span></span><br><span class="line">  <span class="number">0.00994751</span> <span class="number">0.01062134</span> <span class="number">0.0098254</span>  <span class="number">0.01007287</span> <span class="number">0.0099517</span>  <span class="number">0.01028203</span></span><br><span class="line">  <span class="number">0.00993329</span> <span class="number">0.01002692</span> <span class="number">0.01005279</span> <span class="number">0.01040414</span> <span class="number">0.00987132</span> <span class="number">0.00988404</span></span><br><span class="line">  <span class="number">0.01029295</span> <span class="number">0.01014602</span> <span class="number">0.00990441</span> <span class="number">0.00971152</span> <span class="number">0.00996019</span> <span class="number">0.00965257</span></span><br><span class="line">  <span class="number">0.01010645</span> <span class="number">0.00970931</span> <span class="number">0.00982063</span> <span class="number">0.00973994</span> <span class="number">0.01010571</span> <span class="number">0.00984999</span></span><br><span class="line">  <span class="number">0.00968821</span> <span class="number">0.01060284</span> <span class="number">0.00984734</span> <span class="number">0.01027847</span> <span class="number">0.00975892</span> <span class="number">0.00997673</span></span><br><span class="line">  <span class="number">0.00992283</span> <span class="number">0.00980057</span> <span class="number">0.01023249</span> <span class="number">0.00982915</span> <span class="number">0.01070345</span> <span class="number">0.00975009</span></span><br><span class="line">  <span class="number">0.00978433</span> <span class="number">0.01057807</span> <span class="number">0.0097995</span>  <span class="number">0.00960496</span> <span class="number">0.01003811</span> <span class="number">0.0094706</span></span><br><span class="line">  <span class="number">0.00983578</span> <span class="number">0.00977461</span> <span class="number">0.01003506</span> <span class="number">0.00966216</span> <span class="number">0.01028053</span> <span class="number">0.01002804</span></span><br><span class="line">  <span class="number">0.01030125</span> <span class="number">0.01011671</span> <span class="number">0.00976537</span> <span class="number">0.0093752</span>  <span class="number">0.00992731</span> <span class="number">0.00997646</span></span><br><span class="line">  <span class="number">0.01008964</span> <span class="number">0.00983203</span> <span class="number">0.00982056</span> <span class="number">0.01011153</span> <span class="number">0.01021339</span> <span class="number">0.01072151</span></span><br><span class="line">  <span class="number">0.00976963</span> <span class="number">0.01050529</span> <span class="number">0.01019201</span> <span class="number">0.01032242</span> <span class="number">0.01020801</span> <span class="number">0.00998539</span></span><br><span class="line">  <span class="number">0.00993438</span> <span class="number">0.00952398</span> <span class="number">0.00938275</span> <span class="number">0.00991478</span> <span class="number">0.01002662</span> <span class="number">0.01032722</span></span><br><span class="line">  <span class="number">0.01019795</span> <span class="number">0.00952248</span> <span class="number">0.00968466</span> <span class="number">0.0100937</span>  <span class="number">0.00989739</span> <span class="number">0.009971</span></span><br><span class="line">  <span class="number">0.01018309</span> <span class="number">0.00970648</span> <span class="number">0.01000668</span> <span class="number">0.00979022</span>]]</span><br><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>关于查看.pbnodename</p>
<p>除了上面的<code>opname = [tensor.name for tensor in tf.get_default_graph().as_graph_def().node]</code></p>
<p>还可以通过<code>tf.train.write_graph(sess.graph_def, &#39;./pb_model&#39;, &#39;lxbmodel.pb&#39;)</code>生成模型文件（文本文档），打开就能看结点。</p>
<p>还可以：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.get_default_graph().as_graph_def())</span><br><span class="line"><span class="comment">#返回各个计算节点的详细信息,下面展示其中一个节点的信息</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>上面的代码中对图片的处理只是cv.imread后<code>tx.reshape(1, 224, 224, 3) / 255&#125;</code>，C++中（mat数据转换<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/26681713/convert-mat-to-array-vector-in-opencv">参考</a>）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cv::Mat image = cv::<span class="built_in">imread</span>(<span class="string">&quot;E:/plane.jpg&quot;</span>);</span><br><span class="line">cv::Mat img2;</span><br><span class="line">image.<span class="built_in">convertTo</span>(img2, CV_32F);</span><br><span class="line">img2 = img2 / <span class="number">255</span>;</span><br><span class="line">std::vector&lt;<span class="type">float</span>&gt; vecHeight;</span><br><span class="line">vecHeight.<span class="built_in">assign</span>((<span class="type">float</span>*)img2.data, (<span class="type">float</span>*)img2.data + img2.<span class="built_in">total</span>() * img2.<span class="built_in">channels</span>());</span><br><span class="line"><span class="type">float</span>* input = <span class="keyword">new</span> <span class="type">float</span>[vecHeight.<span class="built_in">size</span>()];</span><br><span class="line"><span class="keyword">if</span> (!vecHeight.<span class="built_in">empty</span>())&#123;</span><br><span class="line">    <span class="built_in">memcpy</span>(input, &amp;vecHeight[<span class="number">0</span>], vecHeight.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//input即要传给context的float[]</span></span><br></pre></td></tr></table></figure>

<p>这个array打印出来是和python无差的。现在做到输入数据一致了。目前python调<code>.pb</code>和C++调<code>.trt</code>对<code>plane.jpg</code>图像推理得到的概率向量一致。使用的C++代码：（其中要注意容易错过的简单错误 比如<code>INPUT_H</code>等全局变量要改。）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// stuff we know about the network and the input/output blobs</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INPUT_C = <span class="number">3</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_H = <span class="number">224</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_W = <span class="number">224</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> OUTPUT_SIZE = <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* INPUT_BLOB_NAME = <span class="string">&quot;data&quot;</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* OUTPUT_BLOB_NAME = <span class="string">&quot;prob&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="type">float</span>* input, <span class="type">float</span>* output, <span class="type">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">void</span>* buffers[<span class="number">2</span>] = &#123; <span class="literal">NULL</span>,<span class="literal">NULL</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">0</span>], batchSize * INPUT_H * INPUT_W * INPUT_C* <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line"></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(buffers[<span class="number">0</span>], input, batchSize * INPUT_H * INPUT_W * INPUT_C* <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;start to infer ...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    context.<span class="built_in">enqueue</span>(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(output, buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream and buffers</span></span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">0</span>]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">1</span>]));</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference Done.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="type">const</span> std::string&amp; engineFile, IHostMemory*&amp; trtModelStream, ICudaEngine*&amp; engine)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::fstream file;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;loading filename from:&quot;</span> &lt;&lt; engineFile &lt;&lt; std::endl;</span><br><span class="line">    nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">    <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">    file.<span class="built_in">open</span>(engineFile, std::ios::binary | std::ios::in);</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::end);</span><br><span class="line">    <span class="type">int</span> length = file.<span class="built_in">tellg</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;length:&quot;</span> &lt;&lt; length &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::beg);</span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">char</span>[]&gt; <span class="title">data</span><span class="params">(<span class="keyword">new</span> <span class="type">char</span>[length])</span></span>;</span><br><span class="line">    file.<span class="built_in">read</span>(data.<span class="built_in">get</span>(), length);</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;load engine done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserializing&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtRuntime = <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>());</span><br><span class="line">    <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">    engine = trtRuntime-&gt;<span class="built_in">deserializeCudaEngine</span>(data.<span class="built_in">get</span>(), length, <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserialize done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The engine in TensorRT.cpp is not nullptr&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtModelStream = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span>&#123;</span><br><span class="line">    IHostMemory* modelStream&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    ICudaEngine* engine&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">read_TRT_File</span>(<span class="string">&quot;D:/code/python/pycharmProject/PytorchProj/saved_model/dense121/onnx_model/dense121_output100.trt&quot;</span>, modelStream, engine)) std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created successfully.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">//if (read_TRT_File(&quot;E:/SampleONNX-master/mobilenetv2.trt&quot;, modelStream, engine)) std::cout &lt;&lt; &quot;tensorRT engine created successfully.&quot; &lt;&lt; std::endl;</span></span><br><span class="line">    <span class="keyword">else</span> std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="comment">//查看engine的输入输出维度</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; engine-&gt;<span class="built_in">getNbBindings</span>(); i++)&#123;</span><br><span class="line">        nvinfer1::Dims dims = engine-&gt;<span class="built_in">getBindingDimensions</span>(i);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;index %d, dims: (&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; dims.nbDims; d++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (d &lt; dims.nbDims - <span class="number">1</span>)</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%d,&quot;</span>, dims.d[d]);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, dims.d[d]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;)\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//确定动态维度</span></span><br><span class="line">    nvinfer1::Dims dims4;</span><br><span class="line">    dims4.d[<span class="number">0</span>] = <span class="number">1</span>;    <span class="comment">// replace dynamic batch size with 1</span></span><br><span class="line">    dims4.d[<span class="number">1</span>] = <span class="number">224</span>;</span><br><span class="line">    dims4.d[<span class="number">2</span>] = <span class="number">224</span>;</span><br><span class="line">    dims4.d[<span class="number">3</span>] = <span class="number">3</span>;</span><br><span class="line">    dims4.nbDims = <span class="number">4</span>;</span><br><span class="line">    context-&gt;<span class="built_in">setBindingDimensions</span>(<span class="number">0</span>, dims4);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//get data</span></span><br><span class="line">    <span class="type">float</span> data[<span class="number">512</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="type">float</span> prob[<span class="number">100</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    cv::Mat image = cv::<span class="built_in">imread</span>(<span class="string">&quot;E:/plane.jpg&quot;</span>);</span><br><span class="line">    cv::Mat img2;</span><br><span class="line">    image.<span class="built_in">convertTo</span>(img2, CV_32F);</span><br><span class="line">    img2 = img2 / <span class="number">255</span>;</span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; vecHeight;</span><br><span class="line">    <span class="comment">//这里多维的mat文件转一维的float是在图像数据连续的情况下，等价于三层逐层压进去，具体可以看上方参考博客</span></span><br><span class="line">    vecHeight.<span class="built_in">assign</span>((<span class="type">float</span>*)img2.data, (<span class="type">float</span>*)img2.data + img2.<span class="built_in">total</span>() * img2.<span class="built_in">channels</span>());</span><br><span class="line">    <span class="type">float</span>* input = <span class="keyword">new</span> <span class="type">float</span>[vecHeight.<span class="built_in">size</span>()];</span><br><span class="line">    <span class="keyword">if</span> (!vecHeight.<span class="built_in">empty</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memcpy</span>(input, &amp;vecHeight[<span class="number">0</span>], vecHeight.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">500</span>; i++) &#123;</span><br><span class="line">        std::cout &lt;&lt; input[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Run inference</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">doInference</span>(*context, input, prob, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print histogram of the output distribution</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output:\n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)&#123;</span><br><span class="line">        std::cout &lt;&lt; prob[i] &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>加载cifar10数据集的dataloader，查看它的数据维度：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; batch : test_data_loader)&#123;</span><br><span class="line">    torch::Tensor inputs_tensor = batch.data;</span><br><span class="line">    torch::Tensor labels_tensor = batch.target;</span><br><span class="line">    torch::Tensor outputs_tensor;</span><br><span class="line">    <span class="type">float</span> outputs[<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">auto</span> a_size = inputs_tensor.<span class="built_in">sizes</span>();</span><br><span class="line">    <span class="type">int</span> num_ = inputs_tensor.<span class="built_in">numel</span>();</span><br><span class="line">    std::cout &lt;&lt; a_size &lt;&lt; std::endl &lt;&lt; num_ &lt;&lt; std::endl &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">auto</span> a_size2 = labels_tensor.<span class="built_in">sizes</span>();</span><br><span class="line">    <span class="type">int</span> num_2 = labels_tensor.<span class="built_in">numel</span>();</span><br><span class="line">    std::cout &lt;&lt; a_size2 &lt;&lt; std::endl &lt;&lt; num_2 &lt;&lt; std::endl &lt;&lt; std::endl;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>参考<a target="_blank" rel="noopener" href="https://bouzouitina-hamdi.medium.com/transfer-learning-with-keras-using-densenet121-fffc6bb0c233">这篇博客</a>，训练一个基于keras.application中的DenseNet网络的、处理Cifar10的模型，保存为了<code>.h5</code>格式。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> keras <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> datasets, layers, models</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">X, Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;pre-processes the data&quot;&quot;&quot;</span></span><br><span class="line">    X_p = X_p = K.applications.densenet.preprocess_input(X)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;one hot encode target values&quot;&quot;&quot;</span></span><br><span class="line">    Y_p = K.utils.to_categorical(Y, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> X_p, Y_p</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;load dataset&quot;&quot;&quot;</span></span><br><span class="line">(trainX, trainy), (testX, testy) = K.datasets.cifar10.load_data()</span><br><span class="line">x_train, y_train = preprocess_data(trainX, trainy)</span><br><span class="line">x_test, y_test = preprocess_data(testX, testy)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; USE DenseNet121&quot;&quot;&quot;</span></span><br><span class="line">OldModel = K.applications.DenseNet121(include_top=<span class="literal">False</span>,input_tensor=<span class="literal">None</span>,weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> OldModel.layers[:<span class="number">149</span>]:</span><br><span class="line">    layer.trainable = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> OldModel.layers[<span class="number">149</span>:]:</span><br><span class="line">    layer.trainable = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">model = K.models.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;a lambda layer that scales up the data to the correct size&quot;&quot;&quot;</span></span><br><span class="line">model.add(K.layers.Lambda(<span class="keyword">lambda</span> x:K.backend.resize_images(x,height_factor=<span class="number">7</span>,width_factor=<span class="number">7</span>,data_format=<span class="string">&#x27;channels_last&#x27;</span>)))</span><br><span class="line"></span><br><span class="line">model.add(OldModel)</span><br><span class="line">model.add(K.layers.Flatten())</span><br><span class="line">model.add(K.layers.BatchNormalization())</span><br><span class="line">model.add(K.layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(K.layers.Dropout(<span class="number">0.7</span>))</span><br><span class="line">model.add(K.layers.BatchNormalization())</span><br><span class="line">model.add(K.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(K.layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(K.layers.BatchNormalization())</span><br><span class="line">model.add(K.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(K.layers.Dropout(<span class="number">0.3</span>))</span><br><span class="line">model.add(K.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;callbacks&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># cbacks =  K.callbacks.CallbackList()</span></span><br><span class="line"><span class="comment"># cbacks.append(K.callbacks.ModelCheckpoint(filepath=&#x27;cifar10.h5&#x27;,monitor=&#x27;val_accuracy&#x27;,save_best_only=True))</span></span><br><span class="line"><span class="comment"># cbacks.append(K.callbacks.EarlyStopping(monitor=&#x27;val_accuracy&#x27;,patience=2))</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="string">&quot;&quot;&quot;train&quot;&quot;&quot;</span></span><br><span class="line">model.fit(x=x_train,y=y_train,batch_size=<span class="number">128</span>,epochs=<span class="number">5</span>,validation_data=(x_test, y_test))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.save(<span class="string">&#x27;cifar10.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="h5模型转pb"><a href="#h5模型转pb" class="headerlink" title="h5模型转pb"></a>h5模型转pb</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework.convert_to_constants <span class="keyword">import</span> convert_variables_to_constants_v2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_h5to_pb</span>():</span><br><span class="line">    model = tf.keras.models.load_model(<span class="string">&quot;E:/cifar10.h5&quot;</span>,<span class="built_in">compile</span>=<span class="literal">False</span>)</span><br><span class="line">    model.summary()</span><br><span class="line">    full_model = tf.function(<span class="keyword">lambda</span> Input: model(Input))</span><br><span class="line">    full_model = full_model.get_concrete_function(tf.TensorSpec(model.inputs[<span class="number">0</span>].shape, model.inputs[<span class="number">0</span>].dtype))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get frozen ConcreteFunction</span></span><br><span class="line">    frozen_func = convert_variables_to_constants_v2(full_model)</span><br><span class="line">    frozen_func.graph.as_graph_def()</span><br><span class="line"></span><br><span class="line">    layers = [op.name <span class="keyword">for</span> op <span class="keyword">in</span> frozen_func.graph.get_operations()]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Frozen model layers: &quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">        <span class="built_in">print</span>(layer)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Frozen model inputs: &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(frozen_func.inputs)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Frozen model outputs: &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(frozen_func.outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save frozen graph from frozen ConcreteFunction to hard drive</span></span><br><span class="line">    tf.io.write_graph(graph_or_graph_def=frozen_func.graph,</span><br><span class="line">                      logdir=<span class="string">&quot;E:/&quot;</span>,</span><br><span class="line">                      name=<span class="string">&quot;cifar10.pb&quot;</span>,</span><br><span class="line">                      as_text=<span class="literal">False</span>)</span><br><span class="line">convert_h5to_pb()</span><br><span class="line"></span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">--------------------------------------------------</span><br><span class="line">Frozen model inputs: </span><br><span class="line">[&lt;tf.Tensor <span class="string">&#x27;Input:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>) dtype=float32&gt;]</span><br><span class="line">Frozen model outputs: </span><br><span class="line">[&lt;tf.Tensor <span class="string">&#x27;Identity:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">10</span>) dtype=float32&gt;]</span><br></pre></td></tr></table></figure>





<h3 id="pb转onnx"><a href="#pb转onnx" class="headerlink" title="pb转onnx"></a>pb转onnx</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python -m tf2onnx.convert  --input E:/cifar10.pb --inputs Input:0 --outputs Identity:0 --output E:/cifar10.onnx --opset 11</span><br><span class="line"></span><br><span class="line">python -m tf2onnx.convert  --input E:/cifar102.pb --inputs Input:0 --outputs Identity:0 --output E:/cifar102.onnx --opset 11</span><br><span class="line"></span><br><span class="line">python -m tf2onnx.convert  --input cifar10fix.pb --inputs Input:0 --outputs Identity:0 --output cifar10fix.onnx --opset 11</span><br></pre></td></tr></table></figure>

<h3 id="onnx转trt-1"><a href="#onnx转trt-1" class="headerlink" title="onnx转trt"></a>onnx转trt</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=cifar10.onnx --saveEngine=cifar10.trt --workspace=4096 --minShapes=Input:0:1x32x32x3 --optShapes=Input:0:1x32x32x3 --maxShapes=Input:0:50x32x32x3 --fp16</span><br><span class="line"></span><br><span class="line">trtexec --onnx=afs.onnx --saveEngine=afs.trt --workspace=4096 --minShapes=Input:0:1x5 --optShapes=Input:0:1x5 --maxShapes=Input:0:50x5 --fp16</span><br><span class="line"></span><br><span class="line">trtexec --onnx=dense121_6class.onnx --saveEngine=dense121_6class500.trt --workspace=3072 --minShapes=Input:0:1x128x64x1 --optShapes=Input:0:20x128x64x1 --maxShapes=Input:0:400x128x64x1 --fp16</span><br></pre></td></tr></table></figure>

<p>失败，shell报错：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">Input filename:   cifar10.onnx</span><br><span class="line">ONNX IR version:  0.0.6</span><br><span class="line">Opset version:    11</span><br><span class="line">Producer name:    tf2onnx</span><br><span class="line">Producer version: 1.11.1 1915fb</span><br><span class="line">Domain:</span><br><span class="line">Model version:    0</span><br><span class="line">Doc string:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">[07/28/2022-12:54:39] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">ERROR: builtin_op_importers.cpp:2593 In function importResize:</span><br><span class="line">[8] Assertion failed: (mode != &quot;nearest&quot; || nearest_mode == &quot;floor&quot;) &amp;&amp; &quot;This version of TensorRT only supports floor nearest_mode!&quot;</span><br><span class="line">[07/28/2022-12:54:39] [E] Failed to parse onnx file</span><br><span class="line">[07/28/2022-12:54:39] [E] Parsing model failed</span><br><span class="line">[07/28/2022-12:54:39] [E] Engine creation failed</span><br><span class="line">[07/28/2022-12:54:39] [E] Engine set up failed</span><br><span class="line">&amp;&amp;&amp;&amp; FAILED TensorRT.trtexec </span><br></pre></td></tr></table></figure>

<p>这是因为目前TensorRt的BUG：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/974#issuecomment-754323987">#974 (comment)</a>，不支持resize_image。（不支持的还有NonZero op is not supported in TRT yet。）</p>
<p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429730.png" alt="image-20220728203714761"></p>
<p>代码里使用的<code>keras.backend.resize_images</code><a target="_blank" rel="noopener" href="https://docs.w3cub.com/tensorflow~2.3/keras/backend/resize_images">这个方法</a>使用的是 the <code>nearest</code> model + <code>half_pixel</code> + <code>round_prefer_ceil</code>，</p>
<p>一模一样的<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/1061">issue</a> 。</p>
<p>解决：Lambda式子改成<code>model.add(K.layers.Lambda(lambda x:tf.image.resize(x,[224,224])))</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[07/29/2022-17:25:34] [I] Host Latency</span><br><span class="line">[07/29/2022-17:25:34] [I] min: 1.82153 ms (end to end 2.79663 ms)</span><br><span class="line">[07/29/2022-17:25:34] [I] max: 7.05655 ms (end to end 13.8956 ms)</span><br><span class="line">[07/29/2022-17:25:34] [I] mean: 1.93649 ms (end to end 3.66704 ms)</span><br><span class="line">[07/29/2022-17:25:34] [I] median: 1.90527 ms (end to end 3.60721 ms)</span><br><span class="line">[07/29/2022-17:25:34] [I] percentile: 2.2793 ms at 99% (end to end 4.26883 ms at 99%)</span><br><span class="line">[07/29/2022-17:25:34] [I] throughput: 0 qps</span><br><span class="line">[07/29/2022-17:25:34] [I] walltime: 3.00986 s</span><br><span class="line">[07/29/2022-17:25:34] [I] Enqueue Time</span><br><span class="line">[07/29/2022-17:25:34] [I] min: 0.943115 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] max: 1.9104 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] median: 0.970215 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] GPU Compute</span><br><span class="line">[07/29/2022-17:25:34] [I] min: 1.79199 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] max: 7.01645 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] mean: 1.89984 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] median: 1.86963 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] percentile: 2.24359 ms at 99%</span><br><span class="line">[07/29/2022-17:25:34] [I] total compute time: 2.96756 s</span><br><span class="line">&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin\trtexec.exe --onnx=cifar10.onnx --saveEngine=cifar10.trt --workspace=4096 --minShapes=Input:0:1x32x32x3 --optShapes=Input:0:1x32x32x3 --maxShapes=Input:0:50x32x32x3 --fp16</span><br></pre></td></tr></table></figure>

<p>现在就得到了trt，可以开始跑测试集了~</p>
<blockquote>
<p>可能遇到的问题</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/66355477/could-not-load-library-cudnn-ops-infer64-8-dll-error-code-126-please-make-sure">Please make sure cudnn_ops_infer64_8.dll is in your library path</a></p>
<p>Could not load library cudnn_cnn_infer64_8.dll. Error code 1455<br>Please make sure cudnn_cnn_infer64_8.dll is in your library path!<br>or <strong>context null</strong><br>原因：内存不足，重启VS或者电脑就OK。</p>
</blockquote>
<p>得到的准确率很低，然后跑出来的准确率还不是固定的。<br>看代码发现是float[]、vector、tensor相互转换的时候出了问题，还包括GPU内存拷贝上。</p>
<p>目前，output[]转output_tensor是一定有问题的，输出的值不一样。<br>其次是，每次得到output[]都不一样，有时会有nan的结果。比如像</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">outputs_vector</span><span class="params">(outputs, outputs + <span class="keyword">sizeof</span>(outputs) / <span class="keyword">sizeof</span>(<span class="type">float</span>))</span></span>;</span><br></pre></td></tr></table></figure>

<p>想把outputs[]转成vector，但转后只有前几个数一致。于是我通过<code>torch::form_blob</code>将outputs[]直接转成了tensor，而不是以vector为中介。</p>
<p>nan的原因是我初始化input[]之后没有给它赋值就传给context了。</p>
<p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211430005.png" alt="image-20220803111138976"></p>
<p>happy个锤锤</p>
<p>目前多次测试得到的loss和acc不变了，这么低我想原因要么是精度问题要么是数组载入有问题，下一步打算c++和python测同样的几个样本看得到的输出向量情况，如果有出入大概率是精度问题。</p>
<h2 id="AFS模型保存转换记录"><a href="#AFS模型保存转换记录" class="headerlink" title="AFS模型保存转换记录"></a>AFS模型保存转换记录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m tf2onnx.convert  --input lxbtest.pb --inputs Placeholder:0 --outputs save/restore_all:0 --output lxbtest.onnx --opset 11</span><br></pre></td></tr></table></figure>

<p>把学波保存的pb模型转为onnx时报错：<code>ValueError: Input 0 of node save/AssignVariableOp was passed int32 from Variable:0 incompatible with expected resource.</code>。把学波保存的ckpt读完转成pb，然后再转onnx一样报错。然后发现，读取pb中计算图结点名称这样的一个操作都不行，也报上面的错。而其他正常的<code>.pb</code>文件则不会。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#打印出pb文件中所有node的name  一般来说，opname[0]就是输出结点name，opname[-1]则是输出节点name。</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line"><span class="keyword">with</span> gfile.FastGFile(<span class="string">r&#x27;saved_model/dense121/pb_model/dense121_output100.pb&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    graph_def = tf.GraphDef()</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line">    sess.graph.as_default()</span><br><span class="line">    tf.import_graph_def(graph_def, name=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    opname = [tensor.name <span class="keyword">for</span> tensor <span class="keyword">in</span> tf.get_default_graph().as_graph_def().node]</span><br><span class="line">    <span class="built_in">print</span>(opname)</span><br></pre></td></tr></table></figure>

<p>原因分析：<a target="_blank" rel="noopener" href="https://github.com/onnx/tensorflow-onnx/issues/1152">不是所有的graph都能冻结</a>，可以冻结推理图但训练图不行，因为训练图除了执行变量读取外还有变量赋值。The error is saying that a node (likely a variable assignment node) was given a float (the frozen value of the variable) but was expecting a resource (the mutable variable).</p>
<h2 id="HRRP模型转换记录"><a href="#HRRP模型转换记录" class="headerlink" title="HRRP模型转换记录"></a>HRRP模型转换记录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">Input filename:   dense121_6class.onnx</span><br><span class="line">ONNX IR version:  0.0.6</span><br><span class="line">Opset version:    11</span><br><span class="line">Producer name:    tf2onnx</span><br><span class="line">Producer version: 1.11.1 1915fb</span><br><span class="line">Domain:</span><br><span class="line">Model version:    0</span><br><span class="line">Doc string:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">[08/03/2022-17:53:29] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">[08/03/2022-17:53:30] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[08/03/2022-18:05:17] [I] [TRT] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[08/03/2022-18:05:17] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[08/03/2022-18:05:17] [I] Engine built in 715.435 sec.</span><br><span class="line"></span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.61235 ms - Host latency: 1.65127 ms (end to end 3.09309 ms, enqueue 0.889355 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.72634 ms - Host latency: 1.76223 ms (end to end 3.32732 ms, enqueue 0.8948 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.61448 ms - Host latency: 1.65164 ms (end to end 3.10913 ms, enqueue 0.896533 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.71921 ms - Host latency: 1.75725 ms (end to end 3.31399 ms, enqueue 0.932813 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.61421 ms - Host latency: 1.64998 ms (end to end 3.10967 ms, enqueue 0.88999 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Host Latency</span><br><span class="line">[08/03/2022-18:05:21] [I] min: 1.56018 ms (end to end 1.66077 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] max: 2.76453 ms (end to end 4.55258 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] mean: 1.72398 ms (end to end 3.229 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] median: 1.65479 ms (end to end 3.11627 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] percentile: 2.35864 ms at 99% (end to end 4.04886 ms at 99%)</span><br><span class="line">[08/03/2022-18:05:21] [I] throughput: 0 qps</span><br><span class="line">[08/03/2022-18:05:21] [I] walltime: 3.00615 s</span><br><span class="line">[08/03/2022-18:05:21] [I] Enqueue Time</span><br><span class="line">[08/03/2022-18:05:21] [I] min: 0.859131 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] max: 2.17993 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] median: 0.897461 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] GPU Compute</span><br><span class="line">[08/03/2022-18:05:21] [I] min: 1.52576 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] max: 2.72894 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] mean: 1.68527 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] median: 1.61768 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] percentile: 2.32031 ms at 99%</span><br><span class="line">[08/03/2022-18:05:21] [I] total compute time: 2.96102 s</span><br><span class="line">&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin\trtexec.exe --onnx=dense121_6class.onnx --saveEngine=dense121_6class.trt --workspace=4096 --minShapes=Input:0:1x128x64x1 --optShapes=Input:0:1x128x64x1 --maxShapes=Input:0:100x128x64x1 --fp16</span><br></pre></td></tr></table></figure>

<p>拿到的<code>hdf5</code>模型，按之前的步骤转到trt很顺利，就是onnx to trt时间比较长。</p>
<h1 id="Pytorch模型的转换部署"><a href="#Pytorch模型的转换部署" class="headerlink" title="Pytorch模型的转换部署"></a>Pytorch模型的转换部署</h1><p>128reduce模型是<code>AlexNet_128</code>，256reduce模型是<code>IncrementalModel(256)</code><br>128模型是<code>AlexNet_128</code>，256模型是<code>AlexNet_256</code></p>
<p>首先是对pt文件到onnx的转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch_model = torch.load(<span class="string">&quot;save.pt&quot;</span>) <span class="comment"># pytorch模型加载</span></span><br><span class="line">batch_size = <span class="number">1</span>  <span class="comment">#批处理大小</span></span><br><span class="line">input_shape = (<span class="number">3</span>,<span class="number">244</span>,<span class="number">244</span>)   <span class="comment">#输入数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set the model to inference mode</span></span><br><span class="line">torch_model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">x = torch.randn(batch_size,*input_shape)		<span class="comment"># 生成张量</span></span><br><span class="line">export_onnx_file = <span class="string">&quot;test.onnx&quot;</span>					<span class="comment"># 目的ONNX文件名</span></span><br><span class="line">torch.onnx.export(torch_model,</span><br><span class="line">                    x,</span><br><span class="line">                    export_onnx_file,</span><br><span class="line">                    opset_version=<span class="number">10</span>,</span><br><span class="line">                    do_constant_folding=<span class="literal">True</span>,	<span class="comment"># 是否执行常量折叠优化</span></span><br><span class="line">                    input_names=[<span class="string">&quot;input&quot;</span>],		<span class="comment"># 输入名</span></span><br><span class="line">                    output_names=[<span class="string">&quot;output&quot;</span>],	<span class="comment"># 输出名</span></span><br><span class="line">                    dynamic_axes=&#123;<span class="string">&quot;input&quot;</span>:&#123;<span class="number">0</span>:<span class="string">&quot;batch_size&quot;</span>&#125;,		<span class="comment"># 批处理变量</span></span><br><span class="line">                                  <span class="string">&quot;output&quot;</span>:&#123;<span class="number">0</span>:<span class="string">&quot;batch_size&quot;</span>&#125;&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>onnx到trt：</p>
<p>要注意，上面导出onnx时指定的批处理变量名要和下面转trt命令中的保持一致。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec.exe --explicitBatch --workspace=3072 --minShapes=input:1x1x128x1 --optShapes=input:20x1x128x1 --maxShapes=input:512x1x128x1 --onnx=increment_6_128_save_reduce.onnx --saveEngine=temp.trt --fp16</span><br></pre></td></tr></table></figure>



<h1 id="QT中配置并运行"><a href="#QT中配置并运行" class="headerlink" title="QT中配置并运行"></a>QT中配置并运行</h1><p>动态链接库不仅要在<code>LIBS += \</code>后面添加CUDA和TensorRT的lib文件夹路径，还要手动添加其中的必要库，否则在使用TensorRt推理时会报各种LNK无法解析的外部符号错误：</p>
<p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211430725.png" alt="image-20220712131604604"></p>
<p>以上的错误手动添加<code>-lcudart</code>、<code>-lnvinfer</code>两个库就解决了。</p>
<h1 id="关于Python"><a href="#关于Python" class="headerlink" title="关于Python"></a>关于Python</h1><h2 id="Embedding-Python"><a href="#Embedding-Python" class="headerlink" title="Embedding Python"></a>Embedding Python</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zong596568821xp/article/details/115690713">起步代码博客</a></p>
<p>环境上 只是在编译命令添加了链接库：<code>g++ test2.cpp -o test2 -ID:/evn/Python39/include -LD:/evn/Python39/libs -lpython39</code></p>
<p>注意一些函数在不同的python版本中也不同，比如<a target="_blank" rel="noopener" href="https://docs.python.org/3.9/c-api/call.html#c.PyObject_CallObject">PyObject_CallObject</a>的用法之类的。</p>
<p>有参数传递的调用：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// test2.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;Python.h&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">Py_Initialize</span>(); <span class="comment">//1、初始化python接口</span></span><br><span class="line">    <span class="comment">//初始化使用的变量</span></span><br><span class="line">    PyObject* pModule = <span class="literal">NULL</span>;</span><br><span class="line">    PyObject* pFunc = <span class="literal">NULL</span>;</span><br><span class="line">    PyObject* pName = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="comment">//2、初始化python系统文件路径，保证可以访问到 .py文件</span></span><br><span class="line">    <span class="built_in">PyRun_SimpleString</span>(<span class="string">&quot;import sys&quot;</span>);</span><br><span class="line">    <span class="built_in">PyRun_SimpleString</span>(<span class="string">&quot;sys.path.append(&#x27;./&#x27;)&quot;</span>);</span><br><span class="line">    <span class="comment">//3、调用python文件名。当前的测试python文件名是 myadd.py</span></span><br><span class="line">    <span class="comment">// 在使用这个函数的时候，只需要写文件的名称就可以了。不用写后缀。</span></span><br><span class="line">    pModule = <span class="built_in">PyImport_ImportModule</span>(<span class="string">&quot;myadd&quot;</span>);</span><br><span class="line">    <span class="comment">//4、调用函数</span></span><br><span class="line">    pFunc = <span class="built_in">PyObject_GetAttrString</span>(pModule, <span class="string">&quot;AdditionFc&quot;</span>);</span><br><span class="line">    <span class="comment">//5、给python传参数</span></span><br><span class="line">    <span class="comment">// 函数调用的参数传递均是以元组的形式打包的,2表示参数个数</span></span><br><span class="line">    <span class="comment">// 如果AdditionFc中只有一个参数时，写1就可以了</span></span><br><span class="line">    PyObject* pArgs = <span class="built_in">PyTuple_New</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="comment">// 0：第一个参数，传入 int 类型的值 2</span></span><br><span class="line">    <span class="built_in">PyTuple_SetItem</span>(pArgs, <span class="number">0</span>, <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;i&quot;</span>, <span class="number">2</span>)); </span><br><span class="line">    <span class="comment">// 1：第二个参数，传入 int 类型的值 4</span></span><br><span class="line">    <span class="built_in">PyTuple_SetItem</span>(pArgs, <span class="number">1</span>, <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;i&quot;</span>, <span class="number">4</span>)); </span><br><span class="line">    <span class="comment">// 6、使用C++的python接口调用该函数</span></span><br><span class="line">    PyObject* pReturn = <span class="built_in">PyEval_CallObject</span>(pFunc, pArgs);</span><br><span class="line">    <span class="comment">// 7、接收python计算好的返回值</span></span><br><span class="line">    <span class="type">int</span> nResult;</span><br><span class="line">    <span class="comment">// i表示转换成int型变量。</span></span><br><span class="line">    <span class="comment">// 在这里，最需要注意的是：PyArg_Parse的最后一个参数，必须加上“&amp;”符号</span></span><br><span class="line">    <span class="built_in">PyArg_Parse</span>(pReturn, <span class="string">&quot;i&quot;</span>, &amp;nResult);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;return result is &quot;</span> &lt;&lt; nResult &lt;&lt; endl;</span><br><span class="line">    <span class="comment">//8、结束python接口初始化</span></span><br><span class="line">    <span class="built_in">Py_Finalize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myadd.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AdditionFc</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Now is in python module&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; + &#123;&#125; = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(a, b, a+b))</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br></pre></td></tr></table></figure>


<p>虽然使用了<code>PyRun_SimpleString(&quot;import sys&quot;);PyRun_SimpleString(&quot;sys.path.append(&#39;./&#39;)&quot;);</code>但是会优先搜索本目录下同名的py文件进行调用，所以<strong>注意不要重名了</strong>。<br>现在发现一个导致不能调用的原因，py文件中import了环境中没有包，这个包你可能是在虚拟环境中安了，但global中没有。<br>这之后就搞定了，下面在cpp中调用 调用onnx执行推理的py文件。代码如下，正常执行。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// test.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;Python.h&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">Py_Initialize</span>(); </span><br><span class="line"></span><br><span class="line">    PyObject* pModule = <span class="literal">NULL</span>;</span><br><span class="line">    PyObject* pFunc = <span class="literal">NULL</span>;</span><br><span class="line">    PyObject* pName = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">PyRun_SimpleString</span>(<span class="string">&quot;import sys&quot;</span>);</span><br><span class="line">    <span class="built_in">PyRun_SimpleString</span>(<span class="string">&quot;sys.path.append(&#x27;D:/code/python/pycharmProject/PytorchProj/&#x27;)&quot;</span>);</span><br><span class="line">    <span class="comment">//PyRun_SimpleString(&quot;sys.path.append(&#x27;&#x27;)&quot;);</span></span><br><span class="line"></span><br><span class="line">    pModule = <span class="built_in">PyImport_ImportModule</span>(<span class="string">&quot;test3&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span>( pModule == <span class="literal">NULL</span> )&#123;</span><br><span class="line">		cout &lt;&lt;<span class="string">&quot;pModule not found&quot;</span> &lt;&lt; endl;</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">    pFunc = <span class="built_in">PyObject_GetAttrString</span>(pModule, <span class="string">&quot;doinfer&quot;</span>);</span><br><span class="line">    PyObject* pArgs = <span class="built_in">PyTuple_New</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="type">char</span> datapath[]=<span class="string">&quot;E:/207Project/Data/HRRP/Ball_bottom_cone/00.txt&quot;</span>;</span><br><span class="line">    <span class="type">char</span> modelpath[]=<span class="string">&quot;E:/tfmodels/model.onnx&quot;</span>;</span><br><span class="line">    <span class="built_in">PyTuple_SetItem</span>(pArgs, <span class="number">0</span>, <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;s&quot;</span>, datapath)); </span><br><span class="line">    <span class="built_in">PyTuple_SetItem</span>(pArgs, <span class="number">1</span>, <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;s&quot;</span>, modelpath)); </span><br><span class="line">    PyObject* pReturn = <span class="built_in">PyObject_CallObject</span>(pFunc, pArgs);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nResult;</span><br><span class="line">    <span class="comment">// i表示转换成int型变量。</span></span><br><span class="line">    <span class="comment">// PyArg_Parse的最后一个参数，必须加上“&amp;”符号</span></span><br><span class="line">    <span class="built_in">PyArg_Parse</span>(pReturn, <span class="string">&quot;i&quot;</span>, &amp;nResult);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;return result is &quot;</span> &lt;&lt; nResult &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">Py_Finalize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#test3.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> rt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):<span class="comment">#仅限于旧数据   数字与数字之间是空格</span></span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    <span class="comment">#outtensor=torch.tensor(file_data)</span></span><br><span class="line">    outtensor = np.array(file_data, np.float32)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">doinfer</span>(<span class="params">a,b</span>):</span><br><span class="line">    inputfile_path=<span class="built_in">str</span>(a)</span><br><span class="line">    modelfile_path=<span class="built_in">str</span>(b)</span><br><span class="line">    <span class="built_in">input</span> = getTensorFromTXT(inputfile_path)</span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    sess = rt.InferenceSession(modelfile_path)</span><br><span class="line">    input_name = sess.get_inputs()[<span class="number">0</span>].name</span><br><span class="line">    label_name = sess.get_outputs()[<span class="number">0</span>].name</span><br><span class="line">    pred_onx = sess.run([label_name], &#123;input_name: <span class="built_in">input</span>.astype(np.float32)&#125;)[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(pred_onx)</span><br><span class="line">    <span class="built_in">print</span>(np.argmax(pred_onx))</span><br><span class="line">    <span class="keyword">return</span> np.argmax(pred_onx)</span><br><span class="line">doinfer(<span class="string">&quot;E:/207Project/Data/HRRP/Cone/00.txt&quot;</span>,<span class="string">&quot;E:/tfmodels/model.onnx&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="模型转换的脚本编写"><a href="#模型转换的脚本编写" class="headerlink" title="模型转换的脚本编写"></a>模型转换的脚本编写</h2><p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://blog.51cto.com/zhou123/1312791">python学习——python中执行shell命令</a><br><a target="_blank" rel="noopener" href="https://tendcode.com/article/python-shell/">Python 命令行参数的3种传入方式</a></p>
<h1 id="常用API"><a href="#常用API" class="headerlink" title="常用API"></a>常用API</h1><h3 id="tensor转vector"><a href="#tensor转vector" class="headerlink" title="tensor转vector"></a>tensor转vector</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">output</span><span class="params">(output_tensor.data_ptr&lt;<span class="type">float</span>&gt;(),output_tensor.data_ptr&lt;<span class="type">float</span>&gt;()+output_tensor.numel())</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="vector与数组相互转"><a href="#vector与数组相互转" class="headerlink" title="vector与数组相互转"></a>vector与数组相互转</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Sagittarius_Warrior/article/details/54089242">https://blog.csdn.net/Sagittarius_Warrior/article/details/54089242</a></p>
<h3 id="vector转数组"><a href="#vector转数组" class="headerlink" title="vector转数组"></a>vector转数组</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> *buffer = <span class="keyword">new</span> <span class="type">float</span>[vecHeight.<span class="built_in">size</span>()];</span><br><span class="line"><span class="keyword">if</span> (!vecHeight.<span class="built_in">empty</span>())&#123;</span><br><span class="line">    <span class="built_in">memcpy</span>(buffer, &amp;vecHeight[<span class="number">0</span>], vecHeight.<span class="built_in">size</span>()*<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>1，vector作为动态数组，它的实现方法是：预先分配一个内存块，当感觉不够用的时候，再分配一个更大的内存块，然后自动将之前的数据拷贝到新的内存块中。</p>
<p>所以，出于效率考虑，如果实现知道待存储的数据长度，可以使用resize函数开辟足够的内存，避免后续的内存拷贝。</p>
<p>2，如果数组的元素是字符，建议使用string，而不是vector<char>。</p>
<h3 id="CV-Mat转Vector"><a href="#CV-Mat转Vector" class="headerlink" title="CV.Mat转Vector"></a>CV.Mat转Vector</h3><p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/26681713/convert-mat-to-array-vector-in-opencv">https://stackoverflow.com/questions/26681713/convert-mat-to-array-vector-in-opencv</a></p>
<h3 id="找到tensor转float数组的方法了！！！"><a href="#找到tensor转float数组的方法了！！！" class="headerlink" title="找到tensor转float数组的方法了！！！"></a>找到tensor转float数组的方法了！！！</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43742643/article/details/116307036">LibTorch使用 accessor 快速访问 tensor</a></p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * x 的类型为 CPUFloatType &#123; 100, 100 &#125; </span></span><br><span class="line"><span class="comment"> * x_data.size(0) = 100</span></span><br><span class="line"><span class="comment"> * x_data.size(1) = 100</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">auto</span> x_data = x.<span class="built_in">accessor</span>&lt;<span class="type">float</span>, <span class="number">2</span>&gt;();</span><br><span class="line"><span class="comment">/* 访问单个元素 */</span></span><br><span class="line"><span class="type">float</span> x = x_data[<span class="number">50</span>][<span class="number">50</span>];</span><br><span class="line"><span class="comment">/* x_data.data() 是数据首地址 */</span></span><br><span class="line"><span class="type">float</span> array[<span class="number">100</span>][<span class="number">100</span>];</span><br><span class="line"><span class="built_in">memcpy</span>(array, x_data.<span class="built_in">data</span>(), <span class="number">100</span>*<span class="number">100</span>*<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="float数组转tensor"><a href="#float数组转tensor" class="headerlink" title="float数组转tensor"></a>float数组转tensor</h3><p>用torch::form_blob</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h1 id="一些Cmake文件"><a href="#一些Cmake文件" class="headerlink" title="一些Cmake文件"></a>一些Cmake文件</h1><h2 id="经典文件（3070）"><a href="#经典文件（3070）" class="headerlink" title="经典文件（3070）"></a>经典文件（3070）</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.1</span> FATAL_ERROR)</span><br><span class="line"><span class="keyword">project</span>(cmakeProj)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_STANDARD <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#option(CUDA_USE_STATIC_CUDA_RUNTIME OFF)</span></span><br><span class="line"><span class="comment">#set(CMAKE_CXX_FLAGS &quot;-fsanitize=undefined -fsanitize=address&quot;)</span></span><br><span class="line"><span class="keyword">set</span>(CUDA_NVRTC_SHORTHASH <span class="string">&quot;XXXXXXXX&quot;</span>) <span class="comment">#resolve Failed to compute shorthash for libnvrtc.so</span></span><br><span class="line"><span class="comment">#find_package(CUDA REQUIRED)</span></span><br><span class="line"><span class="comment"># cuda</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include&quot;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorRT</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;D:/evn/TensorRT-7.2.3.4.Windows10.x86_64.cuda-11.1.cudnn8.1/TensorRT-7.2.3.4/include&quot;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;D:/evn/TensorRT-7.2.3.4.Windows10.x86_64.cuda-11.1.cudnn8.1/TensorRT-7.2.3.4/lib&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#OpenCV</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_PREFIX_PATH <span class="string">&quot;D:/evn/OpenCV/opencv-3.4.13-exe/opencv/build&quot;</span>)</span><br><span class="line"><span class="comment">#set(OpenCV_DIR /home/User/opencv/build/)</span></span><br><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;OPENCV_INCLUDE_DIRS&#125;</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment">#libtorch</span></span><br><span class="line"><span class="keyword">set</span>(Torch_DIR <span class="string">&quot;D:/evn/libtorch-1.8.2+cu111/libtorch&quot;</span>)</span><br><span class="line"><span class="keyword">find_package</span>(Torch REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="comment">#matlab</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;D:/softs/MATLAB/R2022a/extern/include&quot;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;D:/softs/MATLAB/R2022a/extern/lib/win64/microsoft&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;E:/207Project/GUI207_V2.0/lib/TRANSFER&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Python</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;D:/evn/Python39/include&quot;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;D:/evn/Python39/libs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(cmakeProj main.cpp ToHRRP.h)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_compile_features</span>(cmakeProj PUBLIC cxx_range_for)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(cmakeProj <span class="variable">$&#123;OpenCV_LIBS&#125;</span> <span class="variable">$&#123;TORCH_LIBRARIES&#125;</span> nvinfer libmat libmx libmex libeng  mclmcr mclmcrrt ToHRRP python39)</span><br></pre></td></tr></table></figure>

<h1 id="实时监测分类"><a href="#实时监测分类" class="headerlink" title="实时监测分类"></a>实时监测分类</h1><h2 id="关于Socket："><a href="#关于Socket：" class="headerlink" title="关于Socket："></a>关于Socket：</h2><p>代码参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44184049/article/details/122291617">socket编程TCP/IP通信（windows下，C++实现）</a></p>
<p>环境配置参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Tona_ZM/article/details/82014294">vs C++实现Socket通信、添加ws2_32.lib 静态链接库</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/oliver_xi/article/details/115366307">用VScode 在Windows下写简单的socket通讯</a></p>
<p>理论参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/yskn/p/9335608.html">c++ 实时通信系统(基础知识TCP/IP篇)</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/JMW1407/article/details/108637540">计算机网络——网络字节序(大端字节序（Big Endian）\小端字节序（Little Endian）)</a></p>
<h2 id="关于多线程："><a href="#关于多线程：" class="headerlink" title="关于多线程："></a>关于多线程：</h2><h3 id="C-Thread"><a href="#C-Thread" class="headerlink" title="C++Thread"></a>C++Thread</h3><h3 id="QThread"><a href="#QThread" class="headerlink" title="QThread"></a>QThread</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//下面两种链接方式都可以捏</span></span><br><span class="line"><span class="built_in">connect</span>(inferThread, &amp;InferThread::sigInferResult,<span class="keyword">this</span>,&amp;MonitorPage::showInferResult);</span><br><span class="line"><span class="built_in">connect</span>(inferThread, <span class="built_in">SIGNAL</span>(<span class="built_in">sigInferResult</span>(QString)),<span class="keyword">this</span>,<span class="built_in">SLOT</span>(<span class="built_in">showInferResult</span>(QString)));</span><br></pre></td></tr></table></figure>

<h3 id="多线程之间传信号"><a href="#多线程之间传信号" class="headerlink" title="多线程之间传信号"></a>多线程之间传信号</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.365seal.com/y/YNv9zwOVGd.html">【Qt】 Qt中实时更新UI程序示例</a></p>
</blockquote>
<p><strong>BUG：</strong></p>
<p>在子线程中连续调用terminal print 会导致没报错的错误（线程堵住？</p>
<p>在线程中连续使用matOpen会在第508次时打开失败。</p>
<h2 id="关于C-调用python"><a href="#关于C-调用python" class="headerlink" title="关于C++调用python"></a>关于C++调用python</h2><p>调用python函数，C传数据矩阵给python，Python绘制。参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41202834/article/details/118058413">c++调用python脚本，指针快速传递</a></p>
<p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211433541.png" alt="ixcuvgas"></p>
<h1 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h1><p>不能调用GPU训练的问题历程：</p>
<p>一开始是在cuda11.1的环境，cudnn版本忘了。conda TensorFlow2.3.0的环境下import TensorFlow然后<code>tf.config.list_physical_devices(&#39;GPU&#39;)</code>返回了空的设备列表。创了2.2.0的环境，提示说少各种cuxx.dll，网上找来往release目录放，最终还是有个dll提示缺少（虽然已经在了）。</p>
<p>于是转向根本问题：cuda、cudnn和TensorFlow的版本关系。参考博客<a target="_blank" rel="noopener" href="https://blog.csdn.net/p_memory/article/details/121872480">快速配置tensorflow gpu环境（使用conda安装CUDA）</a></p>
<p>竟然可以在conda下隔离cuda环境，使用python3.8+cuda11.0+tensorflow2.4.0遂成功（conda install cudnn是不行的，不用单独下它。没关系）</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AE%B0%E5%BD%95/" rel="tag"><i class="fa fa-tag"></i> 记录</a>
              <a href="/tags/%E9%A1%B9%E7%9B%AE/" rel="tag"><i class="fa fa-tag"></i> 项目</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/01/29/SHAP/" rel="prev" title="精读SHAP">
      <i class="fa fa-chevron-left"></i> 精读SHAP
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#QT"><span class="nav-number">1.</span> <span class="nav-text">QT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="nav-number">1.1.</span> <span class="nav-text">关于数据读取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86"><span class="nav-number">1.2.</span> <span class="nav-text">关于模型推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8Ebug"><span class="nav-number">1.3.</span> <span class="nav-text">关于bug</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E8%B0%83%E7%94%A8MATLAB%E5%87%BD%E6%95%B0%E7%94%9F%E6%88%90%E7%9A%84dll"><span class="nav-number">1.4.</span> <span class="nav-text">关于调用MATLAB函数生成的dll</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT-Example"><span class="nav-number">2.</span> <span class="nav-text">TensorRT Example</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E5%BE%97%E5%88%B0onnx%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8F%AF%E9%80%89"><span class="nav-number">2.1.</span> <span class="nav-text">第一步 得到onnx模型（可选</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E5%BE%97%E5%88%B0TensorRt%E5%BC%95%E6%93%8E%EF%BC%9A"><span class="nav-number">2.2.</span> <span class="nav-text">第二步 得到TensorRt引擎：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.2.1.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E4%BD%BF%E7%94%A8ONNX"><span class="nav-number">2.2.2.</span> <span class="nav-text">方法一：使用ONNX</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96trt%E5%BC%95%E6%93%8E%E6%96%87%E4%BB%B6"><span class="nav-number">2.2.3.</span> <span class="nav-text">方法二：反序列化trt引擎文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9ABy%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E6%96%87%E4%BB%B6wts"><span class="nav-number">2.2.4.</span> <span class="nav-text">方法二：By网络权重文件wts</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRt%E5%AE%89%E8%A3%85"><span class="nav-number">3.</span> <span class="nav-text">TensorRt安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95"><span class="nav-number">3.0.1.</span> <span class="nav-text">测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#win-python38%E7%8E%AF%E5%A2%83-import-tensorrt"><span class="nav-number">3.0.2.</span> <span class="nav-text">win+python38环境 import  tensorrt</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%8B%E8%AF%95%E5%8F%8A%E9%83%A8%E7%BD%B2"><span class="nav-number">4.</span> <span class="nav-text">模型训练测试及部署</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch%E8%AE%AD%E7%BB%83%E8%AF%86%E5%88%ABhrrp%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="nav-number">4.1.</span> <span class="nav-text">pytorch训练识别hrrp模型的代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BB%E5%8F%96pytorch%E6%A8%A1%E5%9E%8B-%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86%E8%AF%86%E5%88%AB"><span class="nav-number">4.2.</span> <span class="nav-text">读取pytorch模型 进行推理识别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E7%94%A8onnx%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86"><span class="nav-number">4.3.</span> <span class="nav-text">调用onnx模型进行推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E6%AC%A1%E5%8A%A0%E9%80%9F%E6%8E%A8%E7%90%86"><span class="nav-number">4.4.</span> <span class="nav-text">初次加速推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#V2%E4%BF%AE%E6%94%B9tensor%E4%B8%BAnumpy"><span class="nav-number">4.5.</span> <span class="nav-text">V2修改tensor为numpy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-onnx%E8%BD%ACtrt%E5%92%8C%E6%8E%A8%E7%90%86"><span class="nav-number">4.6.</span> <span class="nav-text">C++ onnx转trt和推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Onnx%E8%BD%ACtrt"><span class="nav-number">4.7.</span> <span class="nav-text">Onnx转trt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BBtrt%E6%96%87%E4%BB%B6%E7%84%B6%E5%90%8E%E6%8E%A8%E7%90%86"><span class="nav-number">4.8.</span> <span class="nav-text">读trt文件然后推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E4%B8%BA%E5%8A%A8%E6%80%81%E8%BE%93%E5%85%A5"><span class="nav-number">4.9.</span> <span class="nav-text">设置为动态输入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorFlow%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%BD%BD%E5%85%A5"><span class="nav-number">4.10.</span> <span class="nav-text">TensorFlow模型保存与载入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E4%B8%BAckpt%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.10.1.</span> <span class="nav-text">保存为ckpt模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BDckpt%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.10.2.</span> <span class="nav-text">加载ckpt模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95"><span class="nav-number">5.</span> <span class="nav-text">TensorFlow模型部署记录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#model2pb-py"><span class="nav-number">5.1.</span> <span class="nav-text">model2pb.py</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#onnx%E8%BD%ACtrt"><span class="nav-number">5.2.</span> <span class="nav-text">onnx转trt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A5CIFAR10%E4%B8%BA%E4%BE%8B%EF%BC%8C%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%B9%B6%E9%83%A8%E7%BD%B2%E6%B5%8B%E8%AF%95%EF%BC%88C-%EF%BC%89"><span class="nav-number">5.3.</span> <span class="nav-text">以CIFAR10为例，训练模型并部署测试（C++）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%88%90%E5%8A%9F%E7%9A%84%E5%8D%95%E6%A0%B7%E6%9C%AC%E6%B5%8B%E8%AF%95"><span class="nav-number">5.3.1.</span> <span class="nav-text">成功的单样本测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">5.3.2.</span> <span class="nav-text">模型训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#h5%E6%A8%A1%E5%9E%8B%E8%BD%ACpb"><span class="nav-number">5.3.3.</span> <span class="nav-text">h5模型转pb</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pb%E8%BD%AConnx"><span class="nav-number">5.3.4.</span> <span class="nav-text">pb转onnx</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#onnx%E8%BD%ACtrt-1"><span class="nav-number">5.3.5.</span> <span class="nav-text">onnx转trt</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AFS%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E8%BD%AC%E6%8D%A2%E8%AE%B0%E5%BD%95"><span class="nav-number">5.4.</span> <span class="nav-text">AFS模型保存转换记录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HRRP%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E8%AE%B0%E5%BD%95"><span class="nav-number">5.5.</span> <span class="nav-text">HRRP模型转换记录</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BD%AC%E6%8D%A2%E9%83%A8%E7%BD%B2"><span class="nav-number">6.</span> <span class="nav-text">Pytorch模型的转换部署</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QT%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%B9%B6%E8%BF%90%E8%A1%8C"><span class="nav-number">7.</span> <span class="nav-text">QT中配置并运行</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E4%BA%8EPython"><span class="nav-number">8.</span> <span class="nav-text">关于Python</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding-Python"><span class="nav-number">8.1.</span> <span class="nav-text">Embedding Python</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E7%9A%84%E8%84%9A%E6%9C%AC%E7%BC%96%E5%86%99"><span class="nav-number">8.2.</span> <span class="nav-text">模型转换的脚本编写</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8API"><span class="nav-number">9.</span> <span class="nav-text">常用API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tensor%E8%BD%ACvector"><span class="nav-number">9.0.1.</span> <span class="nav-text">tensor转vector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vector%E4%B8%8E%E6%95%B0%E7%BB%84%E7%9B%B8%E4%BA%92%E8%BD%AC"><span class="nav-number">9.0.2.</span> <span class="nav-text">vector与数组相互转</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vector%E8%BD%AC%E6%95%B0%E7%BB%84"><span class="nav-number">9.0.3.</span> <span class="nav-text">vector转数组</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CV-Mat%E8%BD%ACVector"><span class="nav-number">9.0.4.</span> <span class="nav-text">CV.Mat转Vector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%BE%E5%88%B0tensor%E8%BD%ACfloat%E6%95%B0%E7%BB%84%E7%9A%84%E6%96%B9%E6%B3%95%E4%BA%86%EF%BC%81%EF%BC%81%EF%BC%81"><span class="nav-number">9.0.5.</span> <span class="nav-text">找到tensor转float数组的方法了！！！</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#float%E6%95%B0%E7%BB%84%E8%BD%ACtensor"><span class="nav-number">9.0.6.</span> <span class="nav-text">float数组转tensor</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#-1"><span class="nav-number">10.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E4%BA%9BCmake%E6%96%87%E4%BB%B6"><span class="nav-number">11.</span> <span class="nav-text">一些Cmake文件</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E6%96%87%E4%BB%B6%EF%BC%883070%EF%BC%89"><span class="nav-number">11.1.</span> <span class="nav-text">经典文件（3070）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E6%97%B6%E7%9B%91%E6%B5%8B%E5%88%86%E7%B1%BB"><span class="nav-number">12.</span> <span class="nav-text">实时监测分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8ESocket%EF%BC%9A"><span class="nav-number">12.1.</span> <span class="nav-text">关于Socket：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%9A"><span class="nav-number">12.2.</span> <span class="nav-text">关于多线程：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C-Thread"><span class="nav-number">12.2.1.</span> <span class="nav-text">C++Thread</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QThread"><span class="nav-number">12.2.2.</span> <span class="nav-text">QThread</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E9%97%B4%E4%BC%A0%E4%BF%A1%E5%8F%B7"><span class="nav-number">12.2.3.</span> <span class="nav-text">多线程之间传信号</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8EC-%E8%B0%83%E7%94%A8python"><span class="nav-number">12.3.</span> <span class="nav-text">关于C++调用python</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-1"><span class="nav-number">13.</span> <span class="nav-text">模型训练</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="wyatt"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">wyatt</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wyatt</span>
</div>




        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
