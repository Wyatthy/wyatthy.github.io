<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>QT+TensorRT流水记录</title>
      <link href="/2023/03/21/QT+TensorRT%E6%B5%81%E6%B0%B4%E8%AE%B0%E5%BD%95/"/>
      <url>/2023/03/21/QT+TensorRT%E6%B5%81%E6%B0%B4%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="QT"><a href="#QT" class="headerlink" title="QT"></a>QT</h1><h2 id="关于数据读取"><a href="#关于数据读取" class="headerlink" title="关于数据读取"></a>关于数据读取</h2><p>只是提供了<code>txt</code>格式数据集的载入预览，之后的推理不提供对<code>txt</code>数据的支持。</p><p>DataLoader可以参考<a href="https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/">https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/</a></p><p>和mat数据文件数据读取有关的代码有：<code>chart.cpp&#39;Chart::readHRRPmat</code> <code>trtinfer.cpp&#39;getAllDataFromMat&amp;getDataFromMat</code></p><p>目前默认所有mat文件中要用的数据都放在”hrrp128”这个变量里（因为没有找到读取mat文件中所有变量的方法），当前用到这个默认变量名的方法有：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">trtInfer:getAllDataFromMat\getDataFromMat</span><br><span class="line">sensepage::nextBatchChart</span><br><span class="line">modelEvalPage::randSample</span><br><span class="line">Chart::readRadiomat\readHrrpmat</span><br></pre></td></tr></table></figure><p>trtInfer里的<code>dataSetClc</code>和加载的模型有关联：<br>在做单样本推理<code>testOneSample</code>的时候，先加载模型，得到模型的输入尺度，这样读数据的时候提供固定大小的数组，用样本数据里的数据填满。<br><code>testAllSample</code>的时候，同样先得到模型输入尺度，传给<code>dataSetClc</code>构造函数，使加载的每一个样本数据都和模型要求的输入大小一致。</p><p>使用label、classname字典的文件有SocketClient::run、monitorpage、inferThread</p><p>数据集中每个样本必须一样长(牵扯的代码比如<code>customdataset.h:getDataSpecifically</code>)</p><span id="more"></span><h2 id="关于模型推理"><a href="#关于模型推理" class="headerlink" title="关于模型推理"></a>关于模型推理</h2><p>设置推理批数的时候应该判断是否超过了转trt时<code>--maxShapes</code>参数中设置的批数。</p><p>系统载入的afs.trt模型文件来源需要是trainLogs的（trt文件同级目录下要有model文件夹且其中有attention.txt文件，相关代码在<code>ModelEvalPage::testAllSample</code>)。</p><h2 id="关于bug"><a href="#关于bug" class="headerlink" title="关于bug"></a>关于bug</h2><p>一定要把训练进程杀死之后再退出程序 不然再次启动程序crashed</p><h2 id="关于调用MATLAB函数生成的dll"><a href="#关于调用MATLAB函数生成的dll" class="headerlink" title="关于调用MATLAB函数生成的dll"></a>关于调用MATLAB函数生成的dll</h2><blockquote><p><a href="https://blog.csdn.net/weixin_40647819/article/details/103152131">如何在C++程序（工程）中调用Matlab函数</a><br><a href="https://blog.csdn.net/King_zkk/article/details/105783638">Qt调用MATLAB 生成的dll经验分享</a><br><a href="https://blog.csdn.net/qq_36165459/article/details/81283932">C++调用Matlab生成的DLL动态链接库进行混合编程（win10+VS2015+Matlab2016b）</a><br><a href="https://blog.csdn.net/u011913417/article/details/102679274">C++中调用matlab的dll文件(解决初始化失败的问题)</a></p></blockquote><p>目前的Radio101转Hrrp128的mat代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">function retn = ToHrrp(sPath,dPath)</span><br><span class="line">    </span><br><span class="line">    retn=0;</span><br><span class="line">    data6 = load(sPath);</span><br><span class="line">    data6 = getfield(data6,&#x27;radio101&#x27;);</span><br><span class="line">    </span><br><span class="line">    X = data6;</span><br><span class="line">    </span><br><span class="line">    mid =size(X);</span><br><span class="line">    down_list = 1:1:mid(1);</span><br><span class="line">    X = X(down_list,:);</span><br><span class="line">    </span><br><span class="line">    [frqNum,dataNum] = size(X) </span><br><span class="line">    win = &#x27;hamming&#x27;;</span><br><span class="line">    N_fft = 2^nextpow2(frqNum);</span><br><span class="line">    </span><br><span class="line">    point = N_fft/frqNum;</span><br><span class="line">    </span><br><span class="line">    w = window(win, frqNum);</span><br><span class="line">    Rng0 = ifftshift(ifft(w,N_fft))*point;</span><br><span class="line">    maxRng0 = max(abs(Rng0));</span><br><span class="line">    </span><br><span class="line">    x = zeros(N_fft, dataNum);</span><br><span class="line">    for n = 1:dataNum</span><br><span class="line">        Xw = X(:,n).*w; % 鎵鏁版嵁鍔犵獥</span><br><span class="line">        x(:,n) = ifftshift(ifft(Xw,N_fft))*point; %IFFT鍙樻崲鍒版椂鍩?</span><br><span class="line">    end</span><br><span class="line">    x = x./maxRng0; %鍘婚櫎鍔犵獥瀵瑰箙搴︾殑褰卞搷</span><br><span class="line">    hrrp128 = log(abs(x));</span><br><span class="line">    % x_dB = log(abs(x))/log(20);</span><br><span class="line">    save(dPath,&#x27;hrrp128&#x27;)</span><br><span class="line">    retn=1;</span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><em><strong>（1）mex -setup</strong></em></p><p>在弹出的两行选项中选择： mex -setup C++</p><p>​            <img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427806.png" alt="format,png"></p><p><em><strong>（2） mbuild -setup</strong></em></p><p>在弹出的两行选项中选择： mex -setup C++ -client MBUILD</p><p>​        <img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427158.png" alt="format,png-16606396091991"></p><p> <strong>2.创建一个.m函数，生成C++文件</strong></p><p>根据工程需要编写一个.m文件，并按照下列指示生成相应的C++文件。</p><p><strong>（1）编写一个名为ZSLAdd.m的函数实现两个数相加</strong></p><p>​                <img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211427545.png" alt="format,png-16606396092002"></p><p><strong>（2）编译生成C++文件</strong></p><p>将Matlab的当前目录打开至存储ZSLAdd.m的文件夹下，在Command Window里输入如下指令：</p><p>mcc -W cpplib:<strong>ZAdd</strong> -T link:lib <strong>ZAdd.m</strong> -C</p><p>加粗字体处更换为自己对应的m函数即可。</p><p>等待一段时间，会在当前目录下生成一系列的文件，其中，以下4个后缀名的文件比较重要： .lib, .h, .dll, .ctf。</p><p>在Qt中，pro里引入.h文件，.pri里引入动态库-lxxx，ctf、dll、lib三个文件放到构建目录里（release文件夹下）不然会<code>ToHrrpInitialize()</code>初始化失败。</p><h1 id="TensorRT-Example"><a href="#TensorRT-Example" class="headerlink" title="TensorRT Example"></a>TensorRT Example</h1><p>使用pytorch作为runtime的全流程：<a href="https://github.com/NVIDIA/TensorRT/blob/master/quickstart/IntroNotebooks/1.%20Introduction.ipynb">introduce</a> <a href="https://github.com/NVIDIA/TensorRT/blob/master/quickstart/IntroNotebooks/4.%20Using%20PyTorch%20through%20ONNX.ipynb">introduce2</a>，在图像分割上的加速应用：<a href="https://github.com/NVIDIA/TensorRT/blob/main/quickstart/SemanticSegmentation/tutorial-runtime.ipynb">tutorial</a></p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428203.png" alt="image-20220708191053551"></p><h2 id="第一步-得到onnx模型（可选"><a href="#第一步-得到onnx模型（可选" class="headerlink" title="第一步 得到onnx模型（可选"></a>第一步 得到onnx模型（可选</h2><p>首先要拿到一个模型的onnx格式，使用pytorch时较简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;E:\\model_only_par.pt&#x27;</span>)</span><br><span class="line">onnx_save_path = <span class="string">&quot;E:\\model.onnx&quot;</span></span><br><span class="line">example_tensor = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>).to(device)</span><br><span class="line">torch.onnx.export(model,  <span class="comment"># model being run</span></span><br><span class="line">                          example_tensor,  <span class="comment"># model input (or a tuple for multiple inputs)</span></span><br><span class="line">                          onnx_save_path,</span><br><span class="line">                          verbose=<span class="literal">False</span>,  <span class="comment"># store the trained parameter weights inside the model file</span></span><br><span class="line">                          training=<span class="literal">False</span>,</span><br><span class="line">                          do_constant_folding=<span class="literal">True</span>,</span><br><span class="line">                          input_names=[<span class="string">&#x27;input&#x27;</span>],</span><br><span class="line">                          output_names=[<span class="string">&#x27;output&#x27;</span>]</span><br><span class="line">                          )</span><br></pre></td></tr></table></figure><p>其中<code>model</code>得是初始化了的对象。</p><p>使用onnx做一下推理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx,torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> rt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):</span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    outtensor=torch.tensor(file_data)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"><span class="built_in">input</span>=getTensorFromTXT(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\Ball_bottom_cone\\21.txt&quot;</span>).numpy()</span><br><span class="line"><span class="built_in">input</span>=<span class="built_in">input</span>.reshape([<span class="number">1</span>,<span class="number">1</span>,<span class="number">512</span>])</span><br><span class="line">sess = rt.InferenceSession(<span class="string">&#x27;E:\\model.onnx&#x27;</span>)</span><br><span class="line">input_name = sess.get_inputs()[<span class="number">0</span>].name</span><br><span class="line">label_name = sess.get_outputs()[<span class="number">0</span>].name</span><br><span class="line">pred_onx = sess.run([label_name], &#123;input_name:<span class="built_in">input</span>.astype(np.float32)&#125;)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(pred_onx)</span><br><span class="line"><span class="built_in">print</span>(np.argmax(pred_onx))</span><br></pre></td></tr></table></figure><h2 id="第二步-得到TensorRt引擎："><a href="#第二步-得到TensorRt引擎：" class="headerlink" title="第二步 得到TensorRt引擎："></a>第二步 得到TensorRt引擎：</h2><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p><a href="https://github.com/onnx/onnx-tensorrt">ONNX models can be converted to serialized TensorRT</a> engines using the <code>onnx2trt</code> executable:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onnx2trt my_model.onnx -o my_engine.trt</span><br></pre></td></tr></table></figure><p>也可以通过trtexec命令实现</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec.exe --onnx=E:/model.onnx --saveEngine=E:/resnet_engine.trt --explicitBatch=1</span><br></pre></td></tr></table></figure><h3 id="方法一：使用ONNX"><a href="#方法一：使用ONNX" class="headerlink" title="方法一：使用ONNX"></a>方法一：使用ONNX</h3><p>使用现有的ONNX模型，通过TensorRt的Parser解析然后填到网络对象中。步骤：</p><ol><li><p>建立一个logger日志，必须要有，但又不是那么重要 <code>static Logger gLogger;</code></p></li><li><p>创建一个builder </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IBuilder* builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br></pre></td></tr></table></figure></li><li><p>创建一个netwok，这时候netWork只是一个空架子</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvinfer1::INetworkDefinition* network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br></pre></td></tr></table></figure></li><li><p>建立一个 Parser，caffe模型、onnx模型和TF模型都有对应的paser，顾名思义，就用用来解析模型文件的.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> parser = nvonnxparser::<span class="built_in">createParser</span>(*network, gLogger);<span class="comment">//这个parser为这个network服务</span></span><br><span class="line"><span class="comment">// 解析ONNX模型</span></span><br><span class="line">std::string onnx_filename = <span class="string">&quot;E:/model.onnx&quot;</span>;</span><br><span class="line">parser-&gt;<span class="built_in">parseFromFile</span>(onnx_filename.<span class="built_in">c_str</span>(), <span class="number">2</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; parser-&gt;<span class="built_in">getNbErrors</span>(); ++i)&#123;</span><br><span class="line">    std::cout &lt;&lt; parser-&gt;<span class="built_in">getError</span>(i)-&gt;<span class="built_in">desc</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>建立 engine，进行层之间融合或者进度校准方式，可以fp32、fp16或者fp8。方法：Builder(Net+Config)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建推理引擎</span></span><br><span class="line">IBuilderConfig* config = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">config-&gt;<span class="built_in">setMaxWorkspaceSize</span>(<span class="number">1</span> &lt;&lt; <span class="number">20</span>);</span><br><span class="line">config-&gt;<span class="built_in">setFlag</span>(nvinfer1::BuilderFlag::kFP16);</span><br><span class="line">ICudaEngine* engine = builder-&gt;<span class="built_in">buildEngineWithConfig</span>(*network, *config);</span><br></pre></td></tr></table></figure></li><li><p>建一个context，这个是用来做inference推断的。上面连接engine，下对应推断数据。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br></pre></td></tr></table></figure></li><li><p>做Inference（涉及到内存开辟传输，最好自己写函数封起来）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="type">float</span>* input, <span class="type">float</span>* output, <span class="type">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> ICudaEngine&amp; engine = context.<span class="built_in">getEngine</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Pointers to input and output device buffers to pass to engine.</span></span><br><span class="line">    <span class="comment">// Engine requires exactly IEngine::getNbBindings() number of buffers.</span></span><br><span class="line">    <span class="built_in">assert</span>(engine.<span class="built_in">getNbBindings</span>() == <span class="number">2</span>);</span><br><span class="line">    <span class="type">void</span>* buffers[<span class="number">2</span>] = &#123; <span class="literal">NULL</span>,<span class="literal">NULL</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In order to bind the buffers, we need to know the names of the input and output tensors.</span></span><br><span class="line">    <span class="comment">// Note that indices are guaranteed to be less than IEngine::getNbBindings()</span></span><br><span class="line">    <span class="comment">//const int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME);</span></span><br><span class="line">    <span class="comment">//const int outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME);</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create GPU buffers on device</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">0</span>], batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create stream</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(buffers[<span class="number">0</span>], input, batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">//开始推理</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;start to infer image...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    context.<span class="built_in">enqueue</span>(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(output, buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream and buffers</span></span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">0</span>]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">1</span>]));</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference Done.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Run inference</span></span><br><span class="line"><span class="type">float</span> data[<span class="number">512</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line"><span class="type">float</span> prob[<span class="number">5</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line"><span class="built_in">getTensorFromTXT</span>(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\DT\\22.txt&quot;</span>, data);</span><br><span class="line"></span><br><span class="line">LARGE_INTEGER t1, t2, tc;</span><br><span class="line"><span class="built_in">QueryPerformanceFrequency</span>(&amp;tc);</span><br><span class="line"><span class="built_in">QueryPerformanceCounter</span>(&amp;t1);</span><br><span class="line"><span class="built_in">doInference</span>(*context, data, prob, <span class="number">1</span>);</span><br><span class="line"><span class="built_in">QueryPerformanceCounter</span>(&amp;t2);</span><br><span class="line"><span class="type">double</span> time = (<span class="type">double</span>)(t2.QuadPart - t1.QuadPart) / (<span class="type">double</span>)tc.QuadPart;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;time = &quot;</span> &lt;&lt; time &lt;&lt; std::endl;  <span class="comment">//输出时间（单位：ｓ）</span></span><br></pre></td></tr></table></figure></li><li></li></ol><h3 id="方法二：反序列化trt引擎文件"><a href="#方法二：反序列化trt引擎文件" class="headerlink" title="方法二：反序列化trt引擎文件"></a>方法二：反序列化trt引擎文件</h3><p>方法一中如果每次推理都解析onnx会很慢，但在某次创建好engine之后序列化成trt文件保存下来，以后推理可以直接调引擎文件来创引擎，会快一点。</p><p><strong>engine保存为.trt文件</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Save .trt</span></span><br><span class="line">nvinfer1::IHostMemory* datas = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">std::ofstream file;</span><br><span class="line">file.<span class="built_in">open</span>(<span class="string">&quot;E:/model.trt&quot;</span>, std::ios::binary | std::ios::out);</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;writing engine file...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">file.<span class="built_in">write</span>((<span class="type">const</span> <span class="type">char</span>*)datas-&gt;<span class="built_in">data</span>(), datas-&gt;<span class="built_in">size</span>());</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;save engine file done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">file.<span class="built_in">close</span>();</span><br></pre></td></tr></table></figure><p><strong>调取.trt文件创建engine</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="type">const</span> std::string&amp; engineFile, IHostMemory*&amp; trtModelStream, ICudaEngine*&amp; engine)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::fstream file;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;loading filename from:&quot;</span> &lt;&lt; engineFile &lt;&lt; std::endl;</span><br><span class="line">    nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">    <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">    file.<span class="built_in">open</span>(engineFile, std::ios::binary | std::ios::in);</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::end);</span><br><span class="line">    <span class="type">int</span> length = file.<span class="built_in">tellg</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;length:&quot;</span> &lt;&lt; length &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::beg);</span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">char</span>[]&gt; <span class="title">data</span><span class="params">(<span class="keyword">new</span> <span class="type">char</span>[length])</span></span>;</span><br><span class="line">    file.<span class="built_in">read</span>(data.<span class="built_in">get</span>(), length);</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;load engine done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserializing&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtRuntime = <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>());</span><br><span class="line">    <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">    engine = trtRuntime-&gt;<span class="built_in">deserializeCudaEngine</span>(data.<span class="built_in">get</span>(), length, <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserialize done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The engine in TensorRT.cpp is not nullptr&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtModelStream = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">IHostMemory* modelStream&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">ICudaEngine* engine&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">read_TRT_File</span>(<span class="string">&quot;E:/model.trt&quot;</span>,modelStream, engine)) std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created successfully.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"><span class="keyword">else</span> std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line"><span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br></pre></td></tr></table></figure><h3 id="方法二：By网络权重文件wts"><a href="#方法二：By网络权重文件wts" class="headerlink" title="方法二：By网络权重文件wts"></a>方法二：By网络权重文件wts</h3><p>可以参考<a href="https://github.com/wang-xinyu/tensorrtx/blob/master/lenet/lenet.cpp">Github项目</a>、<a href="https://zhuanlan.zhihu.com/p/344810135">知乎博客</a>，创建runtime反序列化engine</p><h1 id="TensorRt安装"><a href="#TensorRt安装" class="headerlink" title="TensorRt安装"></a>TensorRt安装</h1><p><a href="https://developer.nvidia.com/nvidia-tensorrt-7x-download">下载TensorRt</a>并解压，把lib添加到PATH。听说cuda11.1匹配TensorRt7.2.3，因此本文使用7.2.3版本。</p><p>将解压后的bin, include, lib\ 目录复制到cuda安装路径下：</p><p>安装必要的包：<code>graphsurgeon</code>和<code>onnx_graphsurgeon</code>,俩包在TensorRt里面都有whl，<code>pip install xx.whl</code>即可。</p><p><strong>pycuda</strong>：</p><p>需要下载<strong>pycuda</strong>包：<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/">https://www.lfd.uci.edu/~gohlke/pythonlibs/</a><br>本机cuda版本为11.1，因此在上面的网站中我下载了<code>pycuda-2021.1+cuda114-cp38-cp38-win_amd64.whl</code><br>在下载目录中执行<code>pip install pycuda-2021.1+cuda114-cp38-cp38-win_amd64.whl</code>以安装此包（取消代理</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a><strong>测试</strong></h3><ol><li>用VS打开解压包下的TensorRT-7.2.3\samples\sampleMNIST\sample_mnist.sln工程，然后选择重新生成。</li><li>使用python运行TensorRT-7.2.3\data\mnist下的download_pgms.py程序。</li><li>进入TensorRT-7.2.3\bin目录下，使用cmd命令来运行<code>sample_mnist.exe --datadir=your\path\to\TensorRT-7.2.3\data\mnist\</code></li></ol><p>报错发现cuDNN少了cublasLt64_10.dll和Zlib（<a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/">Zlib</a> is a data compression software library that is needed by cuDNN）<br>遂<a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#prerequisites-windows">下载</a>并安装Zlib；在CUDA的bin文件夹下，有个cublasLt64_11.dll，我就copy了一份改名成cublasLt64_10.dll，就不报它的错了。<br>Add the directory path of zlibwapi.dll to the environment variable PATH. 但是还是报错，于是我直接把dll放进CUDA的bin中。<br>但还是错，我开始觉得是cuDNN版本的问题，于是把cuDNN版本从8.3.3换到了<a href="https://developer.nvidia.com/rdp/cudnn-archive">8.4.1</a>，成功运行！</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428712.png" alt="image-20220707113756252"></p><h3 id="win-python38环境-import-tensorrt"><a href="#win-python38环境-import-tensorrt" class="headerlink" title="win+python38环境 import  tensorrt"></a>win+python38环境 import  tensorrt</h3><p>python环境实在导入不了tensorrt，因为按教程要先<code>pip install nvidia-pyindex</code>，但这个包我实在是下不来，最后通过换python版本（3.8.8-&gt;3.9.13)才解决下载pyindex的问题。（历时一下午）</p><p>现在pip下载tensorrt还是不行，尝试了几乎所有的方法都不行，也有看到说win下不支持python版本的trt（所以放弃import tensorrt，直接使用C++runtime）</p><h1 id="模型训练测试及部署"><a href="#模型训练测试及部署" class="headerlink" title="模型训练测试及部署"></a>模型训练测试及部署</h1><h2 id="pytorch训练识别hrrp模型的代码"><a href="#pytorch训练识别hrrp模型的代码" class="headerlink" title="pytorch训练识别hrrp模型的代码"></a>pytorch训练识别hrrp模型的代码</h2><p><code>getTensorFromTXT</code>中，list转tensor或者numpy呢？实践证明tensor精度没有numpy的高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch,os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_from_folder</span>(<span class="params">datasetPath</span>):</span><br><span class="line">    ims, labels, class_list = [], [], []</span><br><span class="line">    g = os.walk(<span class="string">r&quot;E:\207Project\Data\HRRP&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> path, dir_list, file_list <span class="keyword">in</span> g:</span><br><span class="line">        <span class="keyword">for</span> dir_name <span class="keyword">in</span> dir_list:</span><br><span class="line">            class_list.append(dir_name)</span><br><span class="line">    class_index = <span class="built_in">dict</span>(<span class="built_in">zip</span>(class_list, <span class="built_in">range</span>(<span class="built_in">len</span>(class_list))))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;类别对应序号：&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(class_index)</span><br><span class="line">    g = os.walk(<span class="string">r&quot;E:\207Project\Data\HRRP&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> path, dir_list, file_list <span class="keyword">in</span> g:</span><br><span class="line">        <span class="keyword">for</span> file_name <span class="keyword">in</span> file_list:</span><br><span class="line">            <span class="keyword">if</span> (file_name[file_name.rfind(<span class="string">&#x27;.&#x27;</span>) + <span class="number">1</span>:] != <span class="string">&quot;txt&quot;</span>):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            ims.append(os.path.join(path, file_name))</span><br><span class="line">            im_class = path[path.rfind(<span class="string">&#x27;\\&#x27;</span>) + <span class="number">1</span>:]</span><br><span class="line">            labels.append(<span class="built_in">int</span>(class_index[im_class]))</span><br><span class="line">    <span class="keyword">return</span> ims, labels</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):</span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    outtensor=torch.tensor(file_data)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, datasetPath, trainOrtest</span>):</span><br><span class="line">        self.ims, self.labels = load_data_from_folder(datasetPath)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        im = getTensorFromTXT(self.ims[index])</span><br><span class="line">        label = self.labels[index]</span><br><span class="line">        <span class="keyword">return</span> im, label</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.ims)</span><br><span class="line"></span><br><span class="line">train_dataset= mDataset(<span class="string">r&quot;E:\207Project\Data\HRRP&quot;</span>,<span class="number">0</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = <span class="number">1</span>, shuffle = <span class="literal">True</span>, num_workers = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv1d(<span class="number">1</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv1d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4000</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">5</span>) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=F.relu(self.conv1(x))</span><br><span class="line">        x=F.max_pool1d(x,<span class="number">2</span>)</span><br><span class="line">        x=F.relu(self.conv2(x))</span><br><span class="line">        x=F.max_pool1d(x,<span class="number">2</span>)</span><br><span class="line">        x=x.view(-<span class="number">1</span>,<span class="number">4000</span>)</span><br><span class="line">        x=F.relu(self.fc1(x))</span><br><span class="line">        x=self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line"><span class="comment">#device=torch.device(&quot;cpu&quot;)</span></span><br><span class="line">model=CNN().to(device)</span><br><span class="line">optimizer=optim.Adam(model.parameters(),lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model,device,train_loader,optimizer,epoch,losses</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> idx,(t_data,t_target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="built_in">input</span> = Variable(t_data).cuda()</span><br><span class="line">        target = Variable(t_target).cuda().long()</span><br><span class="line">        pred=model(<span class="built_in">input</span>)</span><br><span class="line">        loss=F.nll_loss(pred,target)</span><br><span class="line">        <span class="comment">#Adam</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> idx%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoch:&#123;&#125;,iteration:&#123;&#125;,loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch,idx,loss.item()))</span><br><span class="line">            losses.append(loss.item())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model,device,test_loader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct=<span class="number">0</span><span class="comment">#预测对了几个。</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> idx,(t_data,t_target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">            t_data,t_target=t_data.to(device),t_target.to(device)</span><br><span class="line">            pred=model(t_data)<span class="comment">#batch_size*2</span></span><br><span class="line">            pred_class=pred.argmax(dim=<span class="number">1</span>)<span class="comment">#batch_size*2-&gt;batch_size*1</span></span><br><span class="line">            correct+=pred_class.eq(t_target.view_as(pred_class)).<span class="built_in">sum</span>().item()</span><br><span class="line">    acc=correct/<span class="built_in">len</span>(test_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;accuracy:&#123;&#125;,average_loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(acc,average_loss))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs=<span class="number">5</span></span><br><span class="line">losses=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    train(model,device,train_loader,optimizer,epoch,losses)</span><br><span class="line"></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;E:\\model_only_par.pt&#x27;</span>)</span><br><span class="line">onnx_save_path = <span class="string">&quot;E:\\model.onnx&quot;</span></span><br><span class="line">example_tensor = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>).to(device)</span><br><span class="line">torch.onnx.export(model,  <span class="comment"># model being run</span></span><br><span class="line">                  example_tensor,  <span class="comment"># model input (or a tuple for multiple inputs)</span></span><br><span class="line">                  onnx_save_path,</span><br><span class="line">                  verbose=<span class="literal">False</span>,  <span class="comment"># store the trained parameter weights inside the model file</span></span><br><span class="line">                  training=<span class="literal">False</span>,</span><br><span class="line">                  do_constant_folding=<span class="literal">True</span>,</span><br><span class="line">                  input_names=[<span class="string">&#x27;input&#x27;</span>],</span><br><span class="line">                  output_names=[<span class="string">&#x27;output&#x27;</span>]</span><br><span class="line">                 )</span><br></pre></td></tr></table></figure><h2 id="读取pytorch模型-进行推理识别"><a href="#读取pytorch模型-进行推理识别" class="headerlink" title="读取pytorch模型 进行推理识别"></a>读取pytorch模型 进行推理识别</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch,os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv1d(<span class="number">1</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv1d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4000</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">5</span>)  <span class="comment"># 这个也不一样，因为是2分类问题。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=F.relu(self.conv1(x))</span><br><span class="line">        x=F.max_pool1d(x,<span class="number">2</span>)</span><br><span class="line">        x=F.relu(self.conv2(x))</span><br><span class="line">        x=F.max_pool1d(x,<span class="number">2</span>)</span><br><span class="line">        x=x.view(-<span class="number">1</span>,<span class="number">4000</span>)</span><br><span class="line">        x=F.relu(self.fc1(x))</span><br><span class="line">        x=self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x,dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> )</span><br><span class="line"><span class="comment">#device=torch.device(&quot;cpu&quot;)</span></span><br><span class="line">model=CNN().to(device)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;E:\\model_only_par.pt&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):</span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    outtensor=torch.tensor(file_data)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"><span class="built_in">input</span>=getTensorFromTXT(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\DT\\21.txt&quot;</span>)</span><br><span class="line"><span class="built_in">input</span>=<span class="built_in">input</span>.reshape([<span class="number">1</span>,<span class="number">1</span>,<span class="number">512</span>]).to(device)</span><br><span class="line">output=model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h2 id="调用onnx模型进行推理"><a href="#调用onnx模型进行推理" class="headerlink" title="调用onnx模型进行推理"></a>调用onnx模型进行推理</h2><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211428737.png" alt="image-20220710165612452"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx,torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> rt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):</span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    <span class="comment">#outtensor=torch.tensor(file_data)</span></span><br><span class="line">    outtensor = np.array(file_data, np.float32)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"><span class="built_in">input</span>=getTensorFromTXT(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\Ball_bottom_cone\\21.txt&quot;</span>).numpy()</span><br><span class="line"><span class="built_in">input</span>=<span class="built_in">input</span>.reshape([<span class="number">1</span>,<span class="number">1</span>,<span class="number">512</span>])</span><br><span class="line">sess = rt.InferenceSession(<span class="string">&#x27;E:\\model.onnx&#x27;</span>)</span><br><span class="line">input_name = sess.get_inputs()[<span class="number">0</span>].name</span><br><span class="line">label_name = sess.get_outputs()[<span class="number">0</span>].name</span><br><span class="line">pred_onx = sess.run([label_name], &#123;input_name:<span class="built_in">input</span>.astype(np.float32)&#125;)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(pred_onx)</span><br><span class="line"><span class="built_in">print</span>(np.argmax(pred_onx))</span><br></pre></td></tr></table></figure><h2 id="初次加速推理"><a href="#初次加速推理" class="headerlink" title="初次加速推理"></a>初次加速推理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">Input filename:   E:/model.onnx</span><br><span class="line">ONNX IR version:  0.0.7</span><br><span class="line">Opset version:    9</span><br><span class="line">Producer name:    pytorch</span><br><span class="line">Producer version: 1.10</span><br><span class="line">Domain:</span><br><span class="line">Model version:    0</span><br><span class="line">Doc string:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">[07/09/2022-16:20:16] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">tensorRT load onnx mnist model...</span><br><span class="line">[07/09/2022-16:20:17] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[07/09/2022-16:20:57] [W] [TRT] Try increasing the workspace size to 4194304 bytes to get better performance.</span><br><span class="line">[07/09/2022-16:21:07] [W] [TRT] Try increasing the workspace size to 4194304 bytes to get better performance.</span><br><span class="line">[07/09/2022-16:21:11] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[07/09/2022-16:21:11] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">input_blob_name : input</span><br><span class="line">output_blob_name : output</span><br><span class="line">inputH : 1, inputW: 512</span><br><span class="line">start to infer image...</span><br><span class="line">Inference Done.</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"></span><br><span class="line">-385.375, -181.5, -289.125, -621.375, 0,</span><br><span class="line"></span><br><span class="line">D:\code\CPP\tensorrtProj\build\Debug\tensorrtProj.exe (进程 8908)已退出，代码为 0。</span><br><span class="line">要在调试停止时自动关闭控制台，请启用“工具”-&gt;“选项”-&gt;“调试”-&gt;“调试停止时自动关闭控制台”。</span><br><span class="line">按任意键关闭此窗口. . .</span><br></pre></td></tr></table></figure><h2 id="V2修改tensor为numpy"><a href="#V2修改tensor为numpy" class="headerlink" title="V2修改tensor为numpy"></a>V2修改tensor为numpy</h2><p>torch版本换了一下，换成GPU的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">Input filename:   E:/model.onnx</span><br><span class="line">ONNX IR version:  0.0.7</span><br><span class="line">Opset version:    13</span><br><span class="line">Producer name:    pytorch</span><br><span class="line">Producer version: 1.12.0</span><br><span class="line">Domain:</span><br><span class="line">Model version:    0</span><br><span class="line">Doc string:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">[07/10/2022-18:26:03] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">tensorRT load onnx mnist model...</span><br><span class="line">[07/10/2022-18:26:05] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[07/10/2022-18:27:00] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[07/10/2022-18:27:00] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">input_blob_name : input</span><br><span class="line">output_blob_name : output</span><br><span class="line">inputH : 1, inputW: 512</span><br><span class="line">start to infer image...</span><br><span class="line">Inference Done.</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"></span><br><span class="line">-143.479, -205.623, -4.53847, -619.006, -0.0107473,</span><br></pre></td></tr></table></figure><p>V3使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">loading filename from:E:/model.trt</span><br><span class="line">length:4129238</span><br><span class="line">load engine done</span><br><span class="line">deserializing</span><br><span class="line">[07/11/2022-14:23:37] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">deserialize done</span><br><span class="line">The engine in TensorRT.cpp is not nullptr</span><br><span class="line">tensorRT engine created successfully.</span><br><span class="line">[07/11/2022-14:23:37] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">start to infer </span><br><span class="line">Inference Done.</span><br><span class="line">time = 0.0018287</span><br><span class="line">Output:</span><br><span class="line">-9.46875, -17.5312, -10.1875, -0.000114679, -21.8438,</span><br></pre></td></tr></table></figure><h2 id="C-onnx转trt和推理"><a href="#C-onnx转trt和推理" class="headerlink" title="C++ onnx转trt和推理"></a>C++ onnx转trt和推理</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// stuff we know about the network and the input/output blobs</span></span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_H = <span class="number">1</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_W = <span class="number">512</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> OUTPUT_SIZE = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* INPUT_BLOB_NAME = <span class="string">&quot;data&quot;</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* OUTPUT_BLOB_NAME = <span class="string">&quot;prob&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Load weights from files shared with TensorRT samples.</span></span><br><span class="line"><span class="comment">// TensorRT weight files have a simple space delimited format:</span></span><br><span class="line"><span class="comment">// [type] [size] &lt;data x size in hex&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">getTensorFromTXT</span><span class="params">(std::string data_path,<span class="type">float</span>* y)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> r, n = <span class="number">0</span>; <span class="type">double</span> d; FILE* f;</span><br><span class="line">    <span class="type">float</span> temp[<span class="number">1024</span>];</span><br><span class="line">    f = <span class="built_in">fopen</span>(data_path.<span class="built_in">c_str</span>(), <span class="string">&quot;r&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) <span class="built_in">fscanf</span>(f, <span class="string">&quot;%*[^\n]%*c&quot;</span>); <span class="comment">// 跳两行</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1024</span>; i++) &#123;</span><br><span class="line">        r = <span class="built_in">fscanf</span>(f, <span class="string">&quot;%lf&quot;</span>, &amp;d);</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">1</span> == r) temp[n++] = d;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (<span class="number">0</span> == r) <span class="built_in">fscanf</span>(f, <span class="string">&quot;%*c&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">fclose</span>(f);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">512</span>; i++) &#123;</span><br><span class="line">        y[i] = temp[i*<span class="number">2</span> + <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; features; <span class="comment">//临时特征向量</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; <span class="number">512</span>; ++d)</span><br><span class="line">        features.<span class="built_in">push_back</span>(y[d]);</span><br><span class="line">    <span class="comment">//特征归一化</span></span><br><span class="line">    <span class="type">float</span> dMaxValue = *std::<span class="built_in">max_element</span>(features.<span class="built_in">begin</span>(), features.<span class="built_in">end</span>());  <span class="comment">//求最大值</span></span><br><span class="line">    <span class="type">float</span> dMinValue = *std::<span class="built_in">min_element</span>(features.<span class="built_in">begin</span>(), features.<span class="built_in">end</span>());  <span class="comment">//求最小值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> f = <span class="number">0</span>; f &lt; features.<span class="built_in">size</span>(); ++f) &#123;</span><br><span class="line">        y[f] = (y[f] - dMinValue) / (dMaxValue - dMinValue + <span class="number">1e-8</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    features.<span class="built_in">clear</span>();<span class="comment">//删除容器</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="type">float</span>* input, <span class="type">float</span>* output, <span class="type">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//const ICudaEngine&amp; engine = context.getEngine();</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//// Pointers to input and output device buffers to pass to engine.</span></span><br><span class="line">    <span class="comment">//// Engine requires exactly IEngine::getNbBindings() number of buffers.</span></span><br><span class="line">    <span class="comment">//assert(engine.getNbBindings() == 2);</span></span><br><span class="line">    <span class="type">void</span>* buffers[<span class="number">2</span>] = &#123; <span class="literal">NULL</span>,<span class="literal">NULL</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In order to bind the buffers, we need to know the names of the input and output tensors.</span></span><br><span class="line">    <span class="comment">// Note that indices are guaranteed to be less than IEngine::getNbBindings()</span></span><br><span class="line">    <span class="comment">//const int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME);</span></span><br><span class="line">    <span class="comment">//const int outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME);</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create GPU buffers on device</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">0</span>], batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*for (int i = 0; i &lt; batchSize * INPUT_H * INPUT_W; i++) &#123;</span></span><br><span class="line"><span class="comment">        std::cout &lt;&lt; input[i] &lt;&lt; &quot; &quot;;</span></span><br><span class="line"><span class="comment">    &#125;std::cout &lt;&lt; std::endl&lt;&lt;&quot;输出向量展示完毕&quot;&lt;&lt;std::endl;*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create stream</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(buffers[<span class="number">0</span>], input, batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">//开始推理</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;start to infer ...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    context.<span class="built_in">enqueue</span>(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(output, buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream and buffers</span></span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">0</span>]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">1</span>]));</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference Done.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="type">const</span> std::string&amp; engineFile, IHostMemory*&amp; trtModelStream, ICudaEngine*&amp; engine)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::fstream file;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;loading filename from:&quot;</span> &lt;&lt; engineFile &lt;&lt; std::endl;</span><br><span class="line">    nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">    <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">    file.<span class="built_in">open</span>(engineFile, std::ios::binary | std::ios::in);</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::end);</span><br><span class="line">    <span class="type">int</span> length = file.<span class="built_in">tellg</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;length:&quot;</span> &lt;&lt; length &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::beg);</span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">char</span>[]&gt; <span class="title">data</span><span class="params">(<span class="keyword">new</span> <span class="type">char</span>[length])</span></span>;</span><br><span class="line">    file.<span class="built_in">read</span>(data.<span class="built_in">get</span>(), length);</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;load engine done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserializing&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtRuntime = <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>());</span><br><span class="line">    <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">    engine = trtRuntime-&gt;<span class="built_in">deserializeCudaEngine</span>(data.<span class="built_in">get</span>(), length, <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserialize done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The engine in TensorRT.cpp is not nullptr&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtModelStream = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//IBuilder* builder = createInferBuilder(gLogger);</span></span><br><span class="line">    <span class="comment">//nvinfer1::INetworkDefinition* network = builder-&gt;createNetworkV2(1U &lt;&lt; static_cast&lt;uint32_t&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span></span><br><span class="line">    <span class="comment">//auto parser = nvonnxparser::createParser(*network, gLogger);</span></span><br><span class="line">    <span class="comment">//// 解析ONNX模型</span></span><br><span class="line">    <span class="comment">//std::string onnx_filename = &quot;E:/model.onnx&quot;;</span></span><br><span class="line">    <span class="comment">//parser-&gt;parseFromFile(onnx_filename.c_str(), 2);</span></span><br><span class="line">    <span class="comment">//for (int i = 0; i &lt; parser-&gt;getNbErrors(); ++i)</span></span><br><span class="line">    <span class="comment">//&#123;</span></span><br><span class="line">    <span class="comment">//    std::cout &lt;&lt; parser-&gt;getError(i)-&gt;desc() &lt;&lt; std::endl;</span></span><br><span class="line">    <span class="comment">//&#125;</span></span><br><span class="line">    <span class="comment">//printf(&quot;tensorRT load onnx model...\n&quot;);</span></span><br><span class="line">    <span class="comment">//// 创建推理引擎</span></span><br><span class="line">    <span class="comment">//IBuilderConfig* config = builder-&gt;createBuilderConfig();</span></span><br><span class="line">    <span class="comment">//assert(config != nullptr);</span></span><br><span class="line">    <span class="comment">//config-&gt;setMaxWorkspaceSize(1 &lt;&lt; 22);//4194304</span></span><br><span class="line">    <span class="comment">//config-&gt;setFlag(nvinfer1::BuilderFlag::kFP16);</span></span><br><span class="line">    <span class="comment">//ICudaEngine* engine = builder-&gt;buildEngineWithConfig(*network, *config);</span></span><br><span class="line">    <span class="comment">//assert(engine != nullptr);</span></span><br><span class="line">    <span class="comment">//IExecutionContext* context = engine-&gt;createExecutionContext();</span></span><br><span class="line">    <span class="comment">//assert(context != nullptr);</span></span><br><span class="line">    IHostMemory* modelStream&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    ICudaEngine* engine&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">read_TRT_File</span>(<span class="string">&quot;E:/model.trt&quot;</span>,modelStream, engine)) std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created successfully.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">else</span> std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//// 获取输入与输出名称，格式</span></span><br><span class="line">    <span class="comment">//const char* input_blob_name = network-&gt;getInput(0)-&gt;getName();</span></span><br><span class="line">    <span class="comment">//const char* output_blob_name = network-&gt;getOutput(0)-&gt;getName();</span></span><br><span class="line">    <span class="comment">//printf(&quot;input_blob_name : %s \n&quot;, input_blob_name);</span></span><br><span class="line">    <span class="comment">//printf(&quot;output_blob_name : %s \n&quot;, output_blob_name);</span></span><br><span class="line">    <span class="comment">//const int inputH = network-&gt;getInput(0)-&gt;getDimensions().d[1];</span></span><br><span class="line">    <span class="comment">//const int inputW = network-&gt;getInput(0)-&gt;getDimensions().d[2];</span></span><br><span class="line">    <span class="comment">//printf(&quot;inputH : %d, inputW: %d \n&quot;, inputH, inputW);</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Run inference</span></span><br><span class="line">    <span class="type">float</span> data[<span class="number">512</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="type">float</span> prob[<span class="number">5</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="built_in">getTensorFromTXT</span>(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\Ball_bottom_cone\\21.txt&quot;</span>, data);</span><br><span class="line"></span><br><span class="line">    LARGE_INTEGER t1, t2, tc;</span><br><span class="line">    <span class="built_in">QueryPerformanceFrequency</span>(&amp;tc);</span><br><span class="line">    <span class="built_in">QueryPerformanceCounter</span>(&amp;t1);</span><br><span class="line">    <span class="built_in">doInference</span>(*context, data, prob, <span class="number">1</span>);</span><br><span class="line">    <span class="built_in">QueryPerformanceCounter</span>(&amp;t2);</span><br><span class="line">    <span class="type">double</span> time = (<span class="type">double</span>)(t2.QuadPart - t1.QuadPart) / (<span class="type">double</span>)tc.QuadPart;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;time = &quot;</span> &lt;&lt; time &lt;&lt; std::endl;  <span class="comment">//输出时间（单位：ｓ）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print histogram of the output distribution</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output:\n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; prob[i] &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//Save .trt</span></span><br><span class="line">    <span class="comment">/*nvinfer1::IHostMemory* datas = engine-&gt;serialize();</span></span><br><span class="line"><span class="comment">    std::ofstream file;</span></span><br><span class="line"><span class="comment">    file.open(&quot;E:/model.trt&quot;, std::ios::binary | std::ios::out);</span></span><br><span class="line"><span class="comment">    std::cout &lt;&lt; &quot;writing engine file...&quot; &lt;&lt; std::endl;</span></span><br><span class="line"><span class="comment">    file.write((const char*)datas-&gt;data(), datas-&gt;size());</span></span><br><span class="line"><span class="comment">    std::cout &lt;&lt; &quot;save engine file done&quot; &lt;&lt; std::endl;</span></span><br><span class="line"><span class="comment">    file.close();*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Onnx转trt"><a href="#Onnx转trt" class="headerlink" title="Onnx转trt"></a>Onnx转trt</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    IBuilder* builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br><span class="line">    nvinfer1::INetworkDefinition* network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br><span class="line">    <span class="keyword">auto</span> parser = nvonnxparser::<span class="built_in">createParser</span>(*network, gLogger);</span><br><span class="line">    <span class="comment">// 解析ONNX模型</span></span><br><span class="line">    std::string onnx_filename = <span class="string">&quot;E:/model.onnx&quot;</span>;</span><br><span class="line">    parser-&gt;<span class="built_in">parseFromFile</span>(onnx_filename.<span class="built_in">c_str</span>(), <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; parser-&gt;<span class="built_in">getNbErrors</span>(); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; parser-&gt;<span class="built_in">getError</span>(i)-&gt;<span class="built_in">desc</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;tensorRT load onnx model...\n&quot;</span>);</span><br><span class="line">    <span class="comment">// 创建推理引擎</span></span><br><span class="line">    IBuilderConfig* config = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">    <span class="built_in">assert</span>(config != <span class="literal">nullptr</span>);</span><br><span class="line">    config-&gt;<span class="built_in">setMaxWorkspaceSize</span>(<span class="number">1</span> &lt;&lt; <span class="number">22</span>);<span class="comment">//4194304</span></span><br><span class="line">    config-&gt;<span class="built_in">setFlag</span>(nvinfer1::BuilderFlag::kFP16);</span><br><span class="line">    ICudaEngine* engine = builder-&gt;<span class="built_in">buildEngineWithConfig</span>(*network, *config);</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Save .trt</span></span><br><span class="line">    nvinfer1::IHostMemory* datas = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    std::ofstream file;</span><br><span class="line">    file.<span class="built_in">open</span>(<span class="string">&quot;E:/model.trt&quot;</span>, std::ios::binary | std::ios::out);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;writing engine file...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">write</span>((<span class="type">const</span> <span class="type">char</span>*)datas-&gt;<span class="built_in">data</span>(), datas-&gt;<span class="built_in">size</span>());</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;save engine file done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>pytorch的onnx转trt：</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429339.png" alt="image-20220720214726506"></p><h2 id="读trt文件然后推理"><a href="#读trt文件然后推理" class="headerlink" title="读trt文件然后推理"></a>读trt文件然后推理</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// stuff we know about the network and the input/output blobs</span></span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_H = <span class="number">1</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_W = <span class="number">512</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> OUTPUT_SIZE = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* INPUT_BLOB_NAME = <span class="string">&quot;data&quot;</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* OUTPUT_BLOB_NAME = <span class="string">&quot;prob&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">getTensorFromTXT</span><span class="params">(std::string data_path, <span class="type">float</span>* y)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> r, n = <span class="number">0</span>; <span class="type">double</span> d; FILE* f;</span><br><span class="line">    <span class="type">float</span> temp[<span class="number">1024</span>];</span><br><span class="line">    f = <span class="built_in">fopen</span>(data_path.<span class="built_in">c_str</span>(), <span class="string">&quot;r&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) <span class="built_in">fscanf</span>(f, <span class="string">&quot;%*[^\n]%*c&quot;</span>); <span class="comment">// 跳两行</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1024</span>; i++) &#123;</span><br><span class="line">        r = <span class="built_in">fscanf</span>(f, <span class="string">&quot;%lf&quot;</span>, &amp;d);</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">1</span> == r) temp[n++] = d;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (<span class="number">0</span> == r) <span class="built_in">fscanf</span>(f, <span class="string">&quot;%*c&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">fclose</span>(f);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">512</span>; i++) &#123;</span><br><span class="line">        y[i] = temp[i * <span class="number">2</span> + <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; features; <span class="comment">//临时特征向量</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; <span class="number">512</span>; ++d)</span><br><span class="line">        features.<span class="built_in">push_back</span>(y[d]);</span><br><span class="line">    <span class="comment">//特征归一化</span></span><br><span class="line">    <span class="type">float</span> dMaxValue = *std::<span class="built_in">max_element</span>(features.<span class="built_in">begin</span>(), features.<span class="built_in">end</span>());  <span class="comment">//求最大值</span></span><br><span class="line">    <span class="type">float</span> dMinValue = *std::<span class="built_in">min_element</span>(features.<span class="built_in">begin</span>(), features.<span class="built_in">end</span>());  <span class="comment">//求最小值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> f = <span class="number">0</span>; f &lt; features.<span class="built_in">size</span>(); ++f) &#123;</span><br><span class="line">        y[f] = (y[f] - dMinValue) / (dMaxValue - dMinValue + <span class="number">1e-8</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    features.<span class="built_in">clear</span>();<span class="comment">//删除容器</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="type">float</span>* input, <span class="type">float</span>* output, <span class="type">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//const ICudaEngine&amp; engine = context.getEngine();</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//// Pointers to input and output device buffers to pass to engine.</span></span><br><span class="line">    <span class="comment">//// Engine requires exactly IEngine::getNbBindings() number of buffers.</span></span><br><span class="line">    <span class="comment">//assert(engine.getNbBindings() == 2);</span></span><br><span class="line">    <span class="type">void</span>* buffers[<span class="number">2</span>] = &#123; <span class="literal">NULL</span>,<span class="literal">NULL</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In order to bind the buffers, we need to know the names of the input and output tensors.</span></span><br><span class="line">    <span class="comment">// Note that indices are guaranteed to be less than IEngine::getNbBindings()</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">0</span>], batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="comment">/*for (int i = 0; i &lt; batchSize * INPUT_H * INPUT_W; i++) &#123;</span></span><br><span class="line"><span class="comment">        std::cout &lt;&lt; input[i] &lt;&lt; &quot; &quot;;</span></span><br><span class="line"><span class="comment">    &#125;std::cout &lt;&lt; std::endl&lt;&lt;&quot;输出向量展示完毕&quot;&lt;&lt;std::endl;*/</span></span><br><span class="line">    <span class="comment">// Create stream</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line">    <span class="comment">// DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(buffers[<span class="number">0</span>], input, batchSize * INPUT_H * INPUT_W * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">//开始推理</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;start to infer ...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    context.<span class="built_in">enqueue</span>(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(output, buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream and buffers</span></span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">0</span>]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">1</span>]));</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference Done.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="type">const</span> std::string&amp; engineFile, IHostMemory*&amp; trtModelStream, ICudaEngine*&amp; engine)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::fstream file;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;loading filename from:&quot;</span> &lt;&lt; engineFile &lt;&lt; std::endl;</span><br><span class="line">    nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">    <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">    file.<span class="built_in">open</span>(engineFile, std::ios::binary | std::ios::in);</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::end);</span><br><span class="line">    <span class="type">int</span> length = file.<span class="built_in">tellg</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;length:&quot;</span> &lt;&lt; length &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::beg);</span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">char</span>[]&gt; <span class="title">data</span><span class="params">(<span class="keyword">new</span> <span class="type">char</span>[length])</span></span>;</span><br><span class="line">    file.<span class="built_in">read</span>(data.<span class="built_in">get</span>(), length);</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;load engine done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserializing&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtRuntime = <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>());</span><br><span class="line">    <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">    engine = trtRuntime-&gt;<span class="built_in">deserializeCudaEngine</span>(data.<span class="built_in">get</span>(), length, <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserialize done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The engine in TensorRT.cpp is not nullptr&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtModelStream = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    IHostMemory* modelStream&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    ICudaEngine* engine&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">read_TRT_File</span>(<span class="string">&quot;E:/model.trt&quot;</span>, modelStream, engine)) std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created successfully.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">else</span> std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="comment">// Run inference</span></span><br><span class="line">    <span class="type">float</span> data[<span class="number">512</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="type">float</span> prob[<span class="number">5</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="built_in">getTensorFromTXT</span>(<span class="string">&quot;E:\\207Project\\Data\\HRRP\\Ball_bottom_cone\\21.txt&quot;</span>, data);</span><br><span class="line"></span><br><span class="line">    LARGE_INTEGER t1, t2, tc;</span><br><span class="line">    <span class="built_in">QueryPerformanceFrequency</span>(&amp;tc);</span><br><span class="line">    <span class="built_in">QueryPerformanceCounter</span>(&amp;t1);</span><br><span class="line">    <span class="built_in">doInference</span>(*context, data, prob, <span class="number">1</span>);</span><br><span class="line">    <span class="built_in">QueryPerformanceCounter</span>(&amp;t2);</span><br><span class="line">    <span class="type">double</span> time = (<span class="type">double</span>)(t2.QuadPart - t1.QuadPart) / (<span class="type">double</span>)tc.QuadPart;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;time = &quot;</span> &lt;&lt; time &lt;&lt; std::endl;  <span class="comment">//输出时间（单位：ｓ）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print histogram of the output distribution</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output:\n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; prob[i] &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="设置为动态输入"><a href="#设置为动态输入" class="headerlink" title="设置为动态输入"></a>设置为动态输入</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    nvinfer1::Dims mPredictionInputDims;  <span class="comment">//!&lt; The dimensions of the input of the model.</span></span><br><span class="line">    nvinfer1::Dims mPredictionOutputDims; <span class="comment">//!&lt; The dimensions of the output of the model.</span></span><br><span class="line"></span><br><span class="line">    IBuilder* builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br><span class="line">    <span class="comment">//Creating the preprocessing network</span></span><br><span class="line">    <span class="keyword">auto</span> preprocessorNetwork = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">int32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br><span class="line">    <span class="keyword">auto</span> input = preprocessorNetwork-&gt;<span class="built_in">addInput</span>(<span class="string">&quot;input&quot;</span>, nvinfer1::DataType::kFLOAT, Dims4&#123; <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span> &#125;);</span><br><span class="line">    <span class="keyword">auto</span> resizeLayer = preprocessorNetwork-&gt;<span class="built_in">addResize</span>(*input);</span><br><span class="line">    resizeLayer-&gt;<span class="built_in">setOutputDimensions</span>(mPredictionInputDims);</span><br><span class="line">    preprocessorNetwork-&gt;<span class="built_in">markOutput</span>(*resizeLayer-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>));</span><br><span class="line">    <span class="comment">//create an empty full-dims network, and parser</span></span><br><span class="line">    nvinfer1::INetworkDefinition* network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br><span class="line">    <span class="keyword">auto</span> parser = nvonnxparser::<span class="built_in">createParser</span>(*network, gLogger);</span><br><span class="line">    <span class="comment">//parse the model file to populate the network</span></span><br><span class="line">    std::string onnx_filename = <span class="string">&quot;E:/tfmodel_speciInput.onnx&quot;</span>;</span><br><span class="line">    parser-&gt;<span class="built_in">parseFromFile</span>(onnx_filename.<span class="built_in">c_str</span>(), <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; parser-&gt;<span class="built_in">getNbErrors</span>(); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; parser-&gt;<span class="built_in">getError</span>(i)-&gt;<span class="built_in">desc</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;tensorRT load onnx model...\n&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//configure optimization profile &amp; preprocess engine</span></span><br><span class="line">    <span class="keyword">auto</span> preprocessorConfig = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">    <span class="keyword">auto</span> profile = builder-&gt;<span class="built_in">createOptimizationProfile</span>();</span><br><span class="line">    profile-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMIN, Dims4&#123; <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span> &#125;);</span><br><span class="line">    profile-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kOPT, Dims4&#123; <span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span> &#125;);</span><br><span class="line">    profile-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMAX, Dims4&#123; <span class="number">1</span>, <span class="number">1</span>, <span class="number">56</span>, <span class="number">56</span> &#125;);</span><br><span class="line">    preprocessorConfig-&gt;<span class="built_in">addOptimizationProfile</span>(profile);</span><br><span class="line">    <span class="comment">//Create an optimization profile for calibration</span></span><br><span class="line">    <span class="keyword">auto</span> profileCalib = builder-&gt;<span class="built_in">createOptimizationProfile</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> calibBatchSize&#123; <span class="number">256</span> &#125;;</span><br><span class="line">    profileCalib-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMIN, Dims4&#123; calibBatchSize, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span> &#125;);</span><br><span class="line">    profileCalib-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kOPT, Dims4&#123; calibBatchSize, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span> &#125;);</span><br><span class="line">    profileCalib-&gt;<span class="built_in">setDimensions</span>(input-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMAX, Dims4&#123; calibBatchSize, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span> &#125;);</span><br><span class="line">    preprocessorConfig-&gt;<span class="built_in">setCalibrationProfile</span>(profileCalib);</span><br><span class="line">    <span class="comment">//Run engine build with config</span></span><br><span class="line">    <span class="keyword">auto</span> proprocessEngine=builder-&gt;<span class="built_in">buildEngineWithConfig</span>(*preprocessorNetwork, *preprocessorConfig);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建推理引擎</span></span><br><span class="line">    IBuilderConfig* config = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">    <span class="built_in">assert</span>(config != <span class="literal">nullptr</span>);</span><br><span class="line">    config-&gt;<span class="built_in">setMaxWorkspaceSize</span>(<span class="number">1</span> &lt;&lt; <span class="number">22</span>);<span class="comment">//4194304</span></span><br><span class="line">    config-&gt;<span class="built_in">setFlag</span>(nvinfer1::BuilderFlag::kFP16);</span><br><span class="line">    ICudaEngine* engine = builder-&gt;<span class="built_in">buildEngineWithConfig</span>(*network, *config);</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Save .trt</span></span><br><span class="line">    nvinfer1::IHostMemory* datas = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    std::ofstream file;</span><br><span class="line">    file.<span class="built_in">open</span>(<span class="string">&quot;E:/tfmodel_speciInput.trt&quot;</span>, std::ios::binary | std::ios::out);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;writing engine file...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">write</span>((<span class="type">const</span> <span class="type">char</span>*)datas-&gt;<span class="built_in">data</span>(), datas-&gt;<span class="built_in">size</span>());</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;save engine file done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="TensorFlow模型保存与载入"><a href="#TensorFlow模型保存与载入" class="headerlink" title="TensorFlow模型保存与载入"></a>TensorFlow模型保存与载入</h2><p>TF1使用Session 图模式，需要先定义图再执行，流行于工业界。TF2则是Eager模式，同Pytorch，写到哪执行到哪儿，在TF2通过Keras搭配<code>tf.function</code>的训练推理性能很差。</p><h3 id="保存为ckpt模型"><a href="#保存为ckpt模型" class="headerlink" title="保存为ckpt模型"></a>保存为ckpt模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">&quot;w1-name&quot;</span>)</span><br><span class="line">w2 = tf.Variable(tf.constant(<span class="number">3.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">&quot;w2-name&quot;</span>)</span><br><span class="line"></span><br><span class="line">a = tf.placeholder(dtype=tf.float32, name=<span class="string">&quot;a-name&quot;</span>)</span><br><span class="line">b = tf.placeholder(dtype=tf.float32, name=<span class="string">&quot;b-name&quot;</span>)</span><br><span class="line"></span><br><span class="line">y = a * w1 + b * w2</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="built_in">print</span>(a)  <span class="comment"># Tensor(&quot;a-name:0&quot;, dtype=float32)</span></span><br><span class="line">    <span class="built_in">print</span>(b)  <span class="comment"># Tensor(&quot;b-name:0&quot;, dtype=float32)</span></span><br><span class="line">    <span class="built_in">print</span>(y)  <span class="comment"># Tensor(&quot;add:0&quot;, dtype=float32)</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(y, feed_dict=&#123;a: <span class="number">10</span>, b: <span class="number">10</span>&#125;))</span><br><span class="line">    saver.save(sess, <span class="string">&quot;./model/model.ckpt&quot;</span>)</span><br></pre></td></tr></table></figure><p>TensorFlow模型会保存在后缀为.ckpt的文件中。保存后在save这个文件夹中实际会出现3个文件，因为TensorFlow会将计算图的结构和图上参数取值分开保存。</p><ul><li><code>model.ckpt.meta</code>文件保存了TensorFlow计算图的结构，可以理解为神经网络的网络结构</li><li><code>model.ckpt</code>文件保存了TensorFlow程序中每一个变量的取值</li><li><code>checkpoint</code>文件保存了一个目录下所有的模型文件列表</li></ul><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429773.png" alt="image-20220727160501389"></p><h3 id="加载ckpt模型"><a href="#加载ckpt模型" class="headerlink" title="加载ckpt模型"></a>加载ckpt模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">&quot;./model/model.ckpt.meta&quot;</span>)</span><br><span class="line">graph = tf.get_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 Tensor 名获取变量</span></span><br><span class="line">a = graph.get_tensor_by_name(<span class="string">&quot;a-name:0&quot;</span>)</span><br><span class="line">b = graph.get_tensor_by_name(<span class="string">&quot;b-name:0&quot;</span>)</span><br><span class="line">y = graph.get_tensor_by_name(<span class="string">&quot;add:0&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">&quot;./model/model.ckpt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(sess.run(y, feed_dict=&#123;a: <span class="number">10</span>, b: <span class="number">10</span>&#125;))</span><br></pre></td></tr></table></figure><h1 id="TensorFlow模型部署记录"><a href="#TensorFlow模型部署记录" class="headerlink" title="TensorFlow模型部署记录"></a>TensorFlow模型部署记录</h1><blockquote><p>TensorRT和Tensorflow的数据格式不一样，Tensorflow是<strong>NHWC</strong>格式，即channel_last，而TensorRT中是<strong>NCHW</strong>格式，即channel_first，比如一张RGB图像，在Tensorflow中表示为（224， 224， 3），在TensorRT中就是（3，224， 224）。所以使用TensorRT时，请一定确认图像的格式。</p></blockquote><p>TensorFlow模型保存为三种格式：saved_model、checkpoint、graphdef。其中graphdef保存得到的.pb文件网络模型中均为冻结了的常量。</p><p>saved_model–&gt;onnx–&gt;trt    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#saved_model格式保存，保存生成的是一个文件夹my_model</span></span><br><span class="line">model.save(<span class="string">&#x27;saved_model/my_model&#x27;</span>) </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="language-bash">saved_model--&gt;onnx</span></span><br><span class="line">python -m tf2onnx.convert --saved-model saved_model/my_model --output saved_model/tfmodel.onnx</span><br></pre></td></tr></table></figure><p>最后在C++用onnx生成trt引擎的时候报错，因为是输入量为动态的。saved model保存的是一整个训练图，并且参数没有冻结。而只用于模型推理serving并不需要完整的训练图，并且参数不冻结无法进行转TensorRT等极致优化。当然也可以saved_model-&gt;frozen pb-&gt;saved model来同时利用两者的优点。</p><blockquote><p>补：后来发现命令行直接使用<code>trtexec --onnx=E:/tfmodel.onnx --saveEngine=E:/tfmodel.trt</code>也转成了，就是没测是不是能推理。</p></blockquote><p>两种解决办法：</p><p>一：写成动态的(尝试了没成功，参考<a href="https://github.com/NVIDIA/TensorRT/blob/main/samples/sampleDynamicReshape/README.md">TensorRTSample</a>)</p><p>二：设置成静态的，写死的NCHW；  可以通过保存为.pb类型的模型，pb转onnx再转trt</p><p>在这之前我尝试了通过修改onnx输入层的维度，把本来的unk改成了确定的1。但还是识别为动态的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx.checker</span><br><span class="line"><span class="keyword">import</span> onnx.utils</span><br><span class="line"><span class="keyword">from</span> onnx.tools <span class="keyword">import</span> update_model_dims</span><br><span class="line"></span><br><span class="line">model = onnx.load(<span class="string">&#x27;E:/tfmodel.onnx&#x27;</span>)</span><br><span class="line"><span class="comment"># 此处可以理解为获得了一个维度 “引用”，通过该 “引用“可以修改其对应的维度</span></span><br><span class="line">dim_proto0 = model.graph.<span class="built_in">input</span>[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 将该维度赋值为字符串，其维度不再为和dummy_input绑定的值</span></span><br><span class="line">dim_proto0.dim_param = <span class="string">&#x27;1&#x27;</span></span><br><span class="line">dim_proto_0 = model.graph.output[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">0</span>]</span><br><span class="line">dim_proto_0.dim_param = <span class="string">&#x27;1&#x27;</span></span><br><span class="line">onnx.save(model, <span class="string">&#x27;E:/tfmodel_hardInput0.onnx&#x27;</span>)</span><br></pre></td></tr></table></figure><p>但解决不了问题，还是会报错   <strong>↑</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#TensorFlow在目标文件夹下保存为.pb模型</span></span><br><span class="line">tf.keras.models.save_model(model,<span class="string">&quot;E:/tfmodels/&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="language-bash">.pb--&gt;onnx</span></span><br><span class="line">python -m tf2onnx.convert --graphdef tensorflow-model-graphdef-file --output model.onnx --inputs input0:0,input1:0 --outputs output0:0</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-graphdef：需要进行转换的pb模型</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">--output：转换后的onnx模型名称</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">-inputs：pb模型输入层的名字</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">--outputs：pb模型输出层的名字</span></span><br></pre></td></tr></table></figure><p><strong>↑</strong>  问题在于pb转onnx的时候需要提供输入输出层的结点名称。因此需要使用TensorFlow自带的工具：**<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#using-the-graph-transform-tool">summarize_graph</a><strong>，summarize_graph可以查看网络节点，在只有一个固化的权重文件而不知道具体的网络结构时非常有用。下载及使用教程</strong>↓**：</p><p>首先需要<a href="https://docs.bazel.build/versions/main/install-windows.html">安装<strong>bazel</strong></a>，它的功能类似于make。然后下载TensorFlow的源码（推荐使用git clone）。进入源码目录中，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel build tensorflow/tools/graph_transforms:summarize_graph</span><br></pre></td></tr></table></figure><p>没成功，报错：<code>ERROR: An error occurred during the fetch of repository &#39;local_execution_config_python&#39;:</code><br>根据他的<a href="https://github.com/tensorflow/tensorflow/issues/48264">解决办法</a>，是<a href="https://www.tensorflow.org/install/source_windows">TensorFlow构建</a>的问题,这个网页里提到的都要做到，下载缺少的<a href="https://www.msys2.org/">MSYS2</a>（我是缺少了这个）。（在捣鼓MSY32时，记得使用管理员权限）。其中在configure.py的时候，ROCm和CUDA不能同时选，否则有错。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">PS D:\code\python\tensorflow\tensorflow&gt; python ./configure.py</span><br><span class="line">You have bazel 6.0.0-pre.20220630.1 installed.</span><br><span class="line">Please specify the location of python. [Default is D:\evn\Python39\python.exe]:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Found possible Python library paths:</span><br><span class="line">  D:\evn\Python39\lib\site-packages</span><br><span class="line">Please input the desired Python library path to use.  Default is [D:\evn\Python39\lib\site-packages]</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with ROCm support? [y/N]: N</span><br><span class="line">No ROCm support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N]: y</span><br><span class="line">CUDA support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with TensorRT support? [y/N]: y</span><br><span class="line">TensorRT support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">WARNING: TensorRT support on Windows is experimental</span><br><span class="line"></span><br><span class="line">Found CUDA 11.1 in:</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib/x64</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include</span><br><span class="line">Found cuDNN 8 in:</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib/x64</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include</span><br><span class="line">Found TensorRT 7 in:</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib</span><br><span class="line">    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please specify a list of comma-separated CUDA compute capabilities you want to build with.</span><br><span class="line">You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as &quot;x.y&quot; or &quot;compute_xy&quot; to include both virtual and binary GPU code, or as &quot;sm_xy&quot; to only include the binary code.</span><br><span class="line">Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities &gt;= 3.5 [Default is: 3.5,7.0]:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is /arch:AVX]:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y</span><br><span class="line">Eigen strong inline overridden.</span><br><span class="line"></span><br><span class="line">Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N</span><br><span class="line">Not configuring the WORKSPACE for Android builds.</span><br><span class="line"></span><br><span class="line">Preconfigured Bazel build configs. You can use any of the below by adding &quot;--config=&lt;&gt;&quot; to your build command. See .bazelrc for more details.</span><br><span class="line">        --config=mkl            # Build with MKL support.</span><br><span class="line">        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).</span><br><span class="line">        --config=monolithic     # Config for mostly static monolithic build.</span><br><span class="line">        --config=numa           # Build with NUMA support.</span><br><span class="line">        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.</span><br><span class="line">        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.</span><br><span class="line">Preconfigured Bazel build configs to DISABLE default on features:</span><br><span class="line">        --config=nogcp          # Disable GCP support.</span><br><span class="line">        --config=nonccl         # Disable NVIDIA NCCL support.</span><br><span class="line">PS D:\code\python\tensorflow\tensorflow&gt;</span><br></pre></td></tr></table></figure><p>然后再执行 还是不行…..：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ERROR: An error occurred during the fetch of repository &#x27;local_config_python&#x27;:</span><br><span class="line">   Traceback (most recent call last):</span><br><span class="line">        File &quot;D:/code/python/tensorflow/tensorflow/third_party/py/python_configure.bzl&quot;, line 271, column 40, in _python_autoconf_impl</span><br><span class="line">                _create_local_python_repository(repository_ctx)</span><br><span class="line">        File &quot;D:/code/python/tensorflow/tensorflow/third_party/py/python_configure.bzl&quot;, line 213, column 33, in _create_local_python_repository</span><br><span class="line">                python_lib = _get_python_lib(repository_ctx, python_bin)</span><br><span class="line">        File &quot;D:/code/python/tensorflow/tensorflow/third_party/py/python_configure.bzl&quot;, line 130, column 21, in _get_python_lib</span><br><span class="line">                result = execute(repository_ctx, [python_bin, &quot;-c&quot;, cmd])</span><br><span class="line">        File &quot;D:/code/python/tensorflow/tensorflow/third_party/remote_config/common.bzl&quot;, line 230, column 13, in execute</span><br><span class="line">                fail(</span><br><span class="line">Error in fail: Repository command failed</span><br></pre></td></tr></table></figure><p><strong>保存为pb：</strong><br>freeze graph（需要output_node_names</p><p>通过代码获取到的output_node_names TensorFlow说不对：<a href="https://github.com/tensorflow/tensorflow/issues/3986#">Freeze graph: node is not in graph</a></p><p>看了很多解决方法，指定了一些output_node_names，但都说图中不存在，因此必须得看pb文件才能确定。</p><p>于是去查看saved_model文件夹下的<code>saved_model.pb</code>文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = &#x27;saved_model/dense121/saved_model/saved_model.pb&#x27;   #请将这里的model.pb文件路径改为自己的</span></span><br><span class="line"><span class="comment"># graph = tf.compat.v1.get_default_graph()</span></span><br><span class="line"><span class="comment"># graph_def = graph.as_graph_def()</span></span><br><span class="line"><span class="comment"># graph_def.ParseFromString(tf.compat.v1.gfile.FastGFile(model, &#x27;rb&#x27;).read())</span></span><br><span class="line"><span class="comment"># tf.graph_util.import_graph_def(graph_def, name=&#x27;graph&#x27;)</span></span><br><span class="line"><span class="comment"># summaryWriter = tf.summary.FileWriter(&#x27;log/&#x27;, graph)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;saved_model/dense121/saved_model/saved_model.pb&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.compat.v1.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        <span class="built_in">print</span> (graph_def)</span><br></pre></td></tr></table></figure><p>报错：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph_def.ParseFromString(f.read())</span><br><span class="line">google.protobuf.message.DecodeError: Error parsing message with type &#x27;tensorflow.GraphDef&#x27;</span><br></pre></td></tr></table></figure><p>编码错误，有人说可能是这个pb不全，所以我干脆…不知道</p><p>ckpt转pb：<a href="https://zhuanlan.zhihu.com/p/64099452">https://zhuanlan.zhihu.com/p/64099452</a></p><p>从NVIDIA里的<a href="https://blogs.nvidia.com.tw/2022/01/04/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/">一个博客</a>成功导出了pb和onnx。</p><blockquote><p>將 TensorFlow 模型轉換成 ONNX 檔案的方式有很多種。其中之一是 ResNet50 一節中解釋的方式。Keras 也擁有本身的 Keras 轉 ONNX 檔案轉換器。有時候，TensorFlow 轉 ONNX 不支援某些層，但是 Keras 轉 ONNX 轉換器支援。視 Keras 框架和使用的層類型而定，可能必須在轉換器之間選擇。</p></blockquote><h2 id="model2pb-py"><a href="#model2pb-py" class="headerlink" title="model2pb.py"></a>model2pb.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">keras_to_pb</span>(<span class="params">model, output_filename, output_node_names</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This is the function to convert the keras model to pb.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">       model: The keras model.</span></span><br><span class="line"><span class="string">       output_filename: The output .pb file name.</span></span><br><span class="line"><span class="string">       output_node_names: The output nodes of the network (if None,</span></span><br><span class="line"><span class="string">       the function gets the last layer name as the output node).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sess = tf.compat.v1.keras.backend.get_session()</span><br><span class="line">    graph = sess.graph</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> graph.as_default():</span><br><span class="line">        <span class="comment"># Get names of input and output nodes.</span></span><br><span class="line">        in_name = model.layers[<span class="number">0</span>].get_output_at(<span class="number">0</span>).name.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_node_names <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            output_node_names = [model.layers[-<span class="number">1</span>].get_output_at(<span class="number">0</span>).name.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">        graph_def = graph.as_graph_def()</span><br><span class="line">        frozen_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(</span><br><span class="line">            sess,</span><br><span class="line">            graph_def,</span><br><span class="line">            output_node_names)</span><br><span class="line"></span><br><span class="line">    sess.close()</span><br><span class="line">    wkdir = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    tf.compat.v1.train.write_graph(frozen_graph_def, wkdir, output_filename, as_text=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> in_name, output_node_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="comment"># Disable eager execution in tensorflow 2 is required.</span></span><br><span class="line">    tf.compat.v1.disable_eager_execution()</span><br><span class="line">    <span class="comment"># Set learning phase to Test.</span></span><br><span class="line">    tf.compat.v1.keras.backend.set_learning_phase(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load ResNet50 model pre-trained on imagenet</span></span><br><span class="line">    model = tf.keras.applications.ResNet50(</span><br><span class="line">        include_top=<span class="literal">True</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>, input_tensor=<span class="literal">None</span>,</span><br><span class="line">        input_shape=<span class="literal">None</span>, pooling=<span class="literal">None</span>, classes=<span class="number">1000</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert keras ResNet50 model to .pb file</span></span><br><span class="line">    in_tensor_name, out_tensor_names = keras_to_pb(model, args.output_pb_file, <span class="literal">None</span>)</span><br><span class="line">    <span class="built_in">print</span>(in_tensor_name)</span><br><span class="line">    <span class="built_in">print</span>(out_tensor_names)</span><br><span class="line">    <span class="comment"># # You can also use keras2onnx</span></span><br><span class="line">    <span class="comment"># onnx_model = keras2onnx.convert_keras(model, model.name, target_opset=11)</span></span><br><span class="line">    <span class="comment"># keras2onnx.save_model(onnx_model, &quot;resnet.onnx&quot;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--output_pb_file&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;saved_model/dense121/pb_model/resnet50.pb&#x27;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    main(args)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">input_1</span></span><br><span class="line"><span class="string">[&#x27;predictions/Softmax&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#得到pb文件后执行以下命令得到onnx</span></span><br><span class="line"><span class="comment">#python -m tf2onnx.convert  --input saved_model/dense121/pb_model/resnet50.pb --inputs input_1:0 --outputs predictions/Softmax:0 --output saved_model/dense121/onnx_model/resnet50.onnx --opset 11</span></span><br><span class="line"><span class="comment">#python -m tf2onnx.convert  --input saved_model/dense121/pb_model/lxb.pb --inputs Placeholder --outputs save/restore_all --output saved_model/dense121/onnx_model/lxb.onnx --opset 11</span></span><br><span class="line"><span class="comment">#python -m tf2onnx.convert  --input E:/cifar10.pb --inputs Input:0 --outputs Identity:0 --output E:/cifar10.onnx --opset 11</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>从onnx文件中得到输入向量维度信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> engine <span class="keyword">as</span> eng</span><br><span class="line"><span class="keyword">from</span> onnx <span class="keyword">import</span> ModelProto</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">engine_name = <span class="string">&#x27;semantic.plan&#x27;</span></span><br><span class="line">onnx_path = <span class="string">&quot;semantic.onnx&quot;</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">model = ModelProto()</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(onnx_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">  model.ParseFromString(f.read())</span><br><span class="line"></span><br><span class="line">d0 = model.graph.<span class="built_in">input</span>[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">1</span>].dim_value</span><br><span class="line">d1 = model.graph.<span class="built_in">input</span>[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">2</span>].dim_value</span><br><span class="line">d2 = model.graph.<span class="built_in">input</span>[<span class="number">0</span>].<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">3</span>].dim_value</span><br><span class="line">shape = [batch_size , d0, d1 ,d2]</span><br><span class="line">engine = eng.build_engine(onnx_path, shape= shape)</span><br><span class="line">eng.save_engine(engine, engine_name)</span><br></pre></td></tr></table></figure><h2 id="onnx转trt"><a href="#onnx转trt" class="headerlink" title="onnx转trt"></a>onnx转trt</h2><p>此时onnx模型的输入向量维度可以通过netron看到是**<code>float32[unk__1220,224,224,3]</code>**,格式是TF的NHWC.</p><blockquote><p>（<a href="https://blog.csdn.net/qq_29007291/article/details/116135737">trtexec的用法</a>，<a href="https://blog.csdn.net/HW140701/article/details/120360642">TensorRT - 自带工具trtexec的参数使用说明</a>，<a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec">官方介绍文档</a>，<a href="https://blog.csdn.net/u011622208/article/details/120132973?spm=1001.2014.3001.5502">测试博客</a>）</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./trtexec --onnx=xxx.onnx --saveEngine=xxx.trt --workspace=1024 --minShapes=inputx:1x3x480x640 --optShapes=inputx:16x3x480x640 --maxShapes=inputx:32x3x480x640 --fp16</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=dense121_output100.onnx --saveEngine=dense121_output100.trt --workspace=4096 --minShapes=input_1:0:1x224x224x3 --optShapes=input_1:0:1x224x224x3 --maxShapes=input_1:0:32x224x224x3 --fp16</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=cifar10.onnx --saveEngine=cifar10.trt --workspace=4096 --minShapes=Input:0:1x32x32x3 --optShapes=Input:0:1x32x32x3 --maxShapes=Input:0:50x32x32x3 --fp16</span><br></pre></td></tr></table></figure><p>关于报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[W] Dynamic dimensions required for input: input_1:0, but no shapes were provided. Automatically overriding shape to: 1x224x224x3</span><br><span class="line"><span class="meta">#</span><span class="language-bash">这是因为Shapes参数处，输入节点的名字有错误，应该是input_1:0而不是input_1。直接和netron上显示的结点name保持一致即可</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[E] [TRT] input_1:0: for dimension number 1 in profile 0 does not match network definition (got min=3, opt=3, max=3), expected min=opt=max=224).</span><br><span class="line"><span class="meta">#</span><span class="language-bash">Shapes参数1x3x224x224改成1x224x224x3即可</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ERROR: builtin_op_importers.cpp:2593 In function importResize:</span><br><span class="line">[8] Assertion failed: (mode != &quot;nearest&quot; || nearest_mode == &quot;floor&quot;) &amp;&amp; &quot;This version of TensorRT only supports floor nearest_mode!&quot;</span><br><span class="line">[07/28/2022-12:54:39] [E] Failed to parse onnx file</span><br><span class="line"><span class="meta">#</span><span class="language-bash">模型中resize(nearest-ceil model)算子不支持</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[E] [TRT] C:\source\rtSafe\cuda\cudaConvolutionRunner.cpp (483) - Cudnn Error in nvinfer1::rt::cuda::CudnnConvolutionRunner::executeConv: 2 (CUDNN_STATUS_ALLOC_FAILED)</span><br><span class="line"><span class="meta">#</span><span class="language-bash">--workspace参数设置的太大了  调小一点</span></span><br></pre></td></tr></table></figure><ul><li>onnx: 输入的onnx模型</li><li>saveEngine：转换好后保存的tensorrt engine</li><li>workspace：使用的gpu内存，有时候不够，需要手动增大点   单位是MB</li><li>minShapes：动态尺寸时的最小尺寸，格式为<strong>NCHW</strong>，需要给定输入node的名字，</li><li>optShapes：推理测试的尺寸，trtexec会执行推理测试，该shape就是测试时的输入shape</li><li>maxShapes：动态尺寸时的最大尺寸，这里只有batch是动态的，其他维度都是写死的</li><li>fp16：float16推理</li></ul><p>【要点】动态输入的onnx此时需要指定输入的shape范围，<strong>注意</strong>只是范围，得到的trt经过deserialize得到engine，在调用engine时需要指定维度。否则报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[E] [TRT] Parameter check failed at: engine.cpp::nvinfer1::rt::ShapeMachineContext::resolveSlots::1318, condition: allInputDimensionsSpecified(routine)</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/XCCCCZ/article/details/123009816">解决办法：</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查看engine的输入输出维度</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; engine-&gt;<span class="built_in">getNbBindings</span>(); i++)</span><br><span class="line">&#123;</span><br><span class="line">    nvinfer1::Dims dims = engine-&gt;<span class="built_in">getBindingDimensions</span>(i);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;index %d, dims: (&quot;</span>,i);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; dims.nbDims; d++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (d &lt; dims.nbDims - <span class="number">1</span>)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d,&quot;</span>, dims.d[d]);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, dims.d[d]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;)\n&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以DenseNet121的trt文件为例，以上程序输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index 0, dims: (-1,224,224,3)</span><br><span class="line">index 1, dims: (-1,100)</span><br></pre></td></tr></table></figure><p>所以我们得把输入的动态维度写死，在python里，在调用engine推理前做这样的设置即可:<code>context.set_binding_shape(0, (BATCH, 3, INPUT_H, INPUT_W))</code>，C++代码里应该调用IExecutionContext类型的实例的setBindingDimensions(int bindingIndex, Dims dimensions)方法。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//确定动态维度</span></span><br><span class="line">nvinfer1::Dims dims4;</span><br><span class="line">dims4.d[<span class="number">0</span>] = <span class="number">1</span>;    <span class="comment">// replace dynamic batch size with 1</span></span><br><span class="line">dims4.d[<span class="number">1</span>] = <span class="number">224</span>;</span><br><span class="line">dims4.d[<span class="number">2</span>] = <span class="number">224</span>;</span><br><span class="line">dims4.d[<span class="number">3</span>] = <span class="number">3</span>;</span><br><span class="line">dims4.nbDims = <span class="number">4</span>;</span><br><span class="line">context-&gt;<span class="built_in">setBindingDimensions</span>(<span class="number">0</span>, dims4);</span><br></pre></td></tr></table></figure><p>然后再执行推理就可以了。</p><p>总体思路是：拿到一个对维度未知的模型engine文件后，首先读入文件内容并做deserialize获得engine。<br>然后调用getBindingDimensions()查看engine的输入输出维度(如果知道维度就不用)。<br>在调用context-&gt;executeV2()做推理前把维度值为-1的动态维度值替换成具体的维度并调用context-&gt;setBindingDimensions()设置具体维度，然后在数据填入input buffer准备好后调用context-&gt;executeV2()做推理即可:</p><p>为什么是V2，V1V2有什么区别：</p><blockquote><p>execute/enqueue are for <strong>implicit</strong> batch networks, and executeV2/enqueue<strong>V2 are for explicit</strong> batch networks. The V2 versions don’t take a batch_size argument since it’s taken from the explicit batch dimension of the network / or from the optimization profile if used.</p><p>In TensorRT 7, the ONNX parser <strong>requires</strong> that you <strong>create an explicit batch network</strong>, so you’ll have to use V2 methods.</p></blockquote><h2 id="以CIFAR10为例，训练模型并部署测试（C-）"><a href="#以CIFAR10为例，训练模型并部署测试（C-）" class="headerlink" title="以CIFAR10为例，训练模型并部署测试（C++）"></a>以CIFAR10为例，训练模型并部署测试（C++）</h2><p>参考文章：</p><blockquote><p><a href="https://blog.csdn.net/ea8d1n3/article/details/123430217">tensorflow2 cifar10 模型训练 demo</a><br><a href="https://blogs.nvidia.com.tw/2022/01/04/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/">使用 TensorFlow、ONNX 和 NVIDIA TensorRT 加快深度學習推論</a><br><a href="https://zhuanlan.zhihu.com/p/64099452">[深度学习] TensorFlow中模型的freeze_graph</a><br><a href="https://blog.csdn.net/u011026329/article/details/79190347">TensorFlow模型保存和加载方法</a><br><a href="https://blog.csdn.net/daydream13580130043/article/details/110405077">使用TF实现DenseNet并在CIFAR10数据集上进行分类任务</a><br><a href="https://support.huawei.com/enterprise/zh/doc/EDOC1100164830/74e1ab0c">TensorFlow网络模型移植&amp;训练指南 01</a><br><a href="https://blog.csdn.net/thehappysheep/article/details/106247808">tensorflow 模型持久化（ckpt转pb模型）</a><br><a href="https://blog.csdn.net/IT_xiao_bai/article/details/108953938">Keras训练的h5文件转pb文件并用Tensorflow加载</a><br><a href="https://daimajiaoliu.com/daima/569e03e71656401">TensorFlow2.0模型格式转换为.pb格式</a><br><a href="https://cloud.tencent.com/developer/article/1800743">推理演示 | 八步助你搞定tensorRT C++ SDK调用！</a><br><a href="https://blog.csdn.net/u011622208/article/details/120132973?spm=1001.2014.3001.5502">【tensorrt】——trtexec动态batch支持与batch推理耗时评测</a><br><a href="https://leimao.github.io/blog/Save-Load-Inference-From-TF2-Frozen-Graph/">Save, Load and Inference From TensorFlow 2.x Frozen Graph</a></p></blockquote><h3 id="成功的单样本测试"><a href="#成功的单样本测试" class="headerlink" title="成功的单样本测试"></a>成功的单样本测试</h3><p>保存了<strong>随机初始化权重</strong>的pb模型：<code>dense121_output100.pb</code>，根据上面的流程转成了<code>dense121_output100.trt</code>引擎文件。python调取pb对网上下载的一个plane图片做推理，代码如下：（<a href="https://blog.csdn.net/pengpengloveqiaoqiao/article/details/110391113">参考</a>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line"><span class="keyword">with</span> gfile.FastGFile(<span class="string">r&#x27;D:\code\python\pycharmProject\PytorchProj\saved_model\dense121\pb_model\dense121_output100.pb&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    graph_def = tf.GraphDef()</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line">    sess.graph.as_default()</span><br><span class="line">    tf.import_graph_def(graph_def, name=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    opname = [tensor.name <span class="keyword">for</span> tensor <span class="keyword">in</span> tf.get_default_graph().as_graph_def().node]</span><br><span class="line">    <span class="built_in">print</span>(opname)   <span class="comment">#查看pb nodename</span></span><br><span class="line"><span class="comment"># 获取输入tensor</span></span><br><span class="line">x = tf.get_default_graph().get_tensor_by_name(</span><br><span class="line">    <span class="string">&quot;input_1:0&quot;</span>)  <span class="comment"># 不知道输入名时通过节点名查，一般情况下是每一个节点tf.get_default_graph().as_graph_def().node[0].name,名字构成后有个:0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input:&quot;</span>, x)</span><br><span class="line"><span class="comment"># 获取预测tensor</span></span><br><span class="line">pred = tf.get_default_graph().get_tensor_by_name(</span><br><span class="line">    <span class="string">&quot;predictions/Softmax:0&quot;</span>)  <span class="comment"># tf.get_default_graph().as_graph_def().node[-1].name，有可能不是是最后一一个</span></span><br><span class="line"><span class="built_in">print</span>(pred)</span><br><span class="line"></span><br><span class="line">tx=cv2.imread(<span class="string">&quot;E:/plane.jpg&quot;</span>)</span><br><span class="line">pre = sess.run(pred, feed_dict=&#123;x: tx.reshape(<span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>) / <span class="number">255</span>&#125;)  <span class="comment"># 预测直接run输出，传入输入</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Prediction: &quot;</span> + <span class="built_in">str</span>(pre))</span><br><span class="line"><span class="built_in">print</span>(pre.<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;input_1&#x27;</span>, <span class="string">&#x27;zero_padding2d/Pad/paddings&#x27;</span>, <span class="string">&#x27;zero_padding2d/Pad&#x27;</span>, <span class="string">&#x27;conv1/conv/kernel&#x27;</span>, </span><br><span class="line">。。。 </span><br><span class="line"><span class="string">&#x27;predictions/MatMul/ReadVariableOp&#x27;</span>, <span class="string">&#x27;predictions/MatMul&#x27;</span>, <span class="string">&#x27;predictions/BiasAdd/ReadVariableOp&#x27;</span>, <span class="string">&#x27;predictions/BiasAdd&#x27;</span>, <span class="string">&#x27;predictions/Softmax&#x27;</span>]</span><br><span class="line"><span class="built_in">input</span>: Tensor(<span class="string">&quot;input_1:0&quot;</span>, shape=(<span class="literal">None</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>), dtype=float32)</span><br><span class="line">Tensor(<span class="string">&quot;predictions/Softmax:0&quot;</span>, shape=(<span class="literal">None</span>, <span class="number">100</span>), dtype=float32)</span><br><span class="line"></span><br><span class="line">Prediction: [[<span class="number">0.00983849</span> <span class="number">0.01052127</span> <span class="number">0.00983031</span> <span class="number">0.01043278</span> <span class="number">0.00971659</span> <span class="number">0.01052411</span></span><br><span class="line">  <span class="number">0.00976097</span> <span class="number">0.01017774</span> <span class="number">0.01003768</span> <span class="number">0.01022743</span> <span class="number">0.01027896</span> <span class="number">0.00987673</span></span><br><span class="line">  <span class="number">0.01036694</span> <span class="number">0.00980142</span> <span class="number">0.01010968</span> <span class="number">0.01021501</span> <span class="number">0.00979544</span> <span class="number">0.00993549</span></span><br><span class="line">  <span class="number">0.00994751</span> <span class="number">0.01062134</span> <span class="number">0.0098254</span>  <span class="number">0.01007287</span> <span class="number">0.0099517</span>  <span class="number">0.01028203</span></span><br><span class="line">  <span class="number">0.00993329</span> <span class="number">0.01002692</span> <span class="number">0.01005279</span> <span class="number">0.01040414</span> <span class="number">0.00987132</span> <span class="number">0.00988404</span></span><br><span class="line">  <span class="number">0.01029295</span> <span class="number">0.01014602</span> <span class="number">0.00990441</span> <span class="number">0.00971152</span> <span class="number">0.00996019</span> <span class="number">0.00965257</span></span><br><span class="line">  <span class="number">0.01010645</span> <span class="number">0.00970931</span> <span class="number">0.00982063</span> <span class="number">0.00973994</span> <span class="number">0.01010571</span> <span class="number">0.00984999</span></span><br><span class="line">  <span class="number">0.00968821</span> <span class="number">0.01060284</span> <span class="number">0.00984734</span> <span class="number">0.01027847</span> <span class="number">0.00975892</span> <span class="number">0.00997673</span></span><br><span class="line">  <span class="number">0.00992283</span> <span class="number">0.00980057</span> <span class="number">0.01023249</span> <span class="number">0.00982915</span> <span class="number">0.01070345</span> <span class="number">0.00975009</span></span><br><span class="line">  <span class="number">0.00978433</span> <span class="number">0.01057807</span> <span class="number">0.0097995</span>  <span class="number">0.00960496</span> <span class="number">0.01003811</span> <span class="number">0.0094706</span></span><br><span class="line">  <span class="number">0.00983578</span> <span class="number">0.00977461</span> <span class="number">0.01003506</span> <span class="number">0.00966216</span> <span class="number">0.01028053</span> <span class="number">0.01002804</span></span><br><span class="line">  <span class="number">0.01030125</span> <span class="number">0.01011671</span> <span class="number">0.00976537</span> <span class="number">0.0093752</span>  <span class="number">0.00992731</span> <span class="number">0.00997646</span></span><br><span class="line">  <span class="number">0.01008964</span> <span class="number">0.00983203</span> <span class="number">0.00982056</span> <span class="number">0.01011153</span> <span class="number">0.01021339</span> <span class="number">0.01072151</span></span><br><span class="line">  <span class="number">0.00976963</span> <span class="number">0.01050529</span> <span class="number">0.01019201</span> <span class="number">0.01032242</span> <span class="number">0.01020801</span> <span class="number">0.00998539</span></span><br><span class="line">  <span class="number">0.00993438</span> <span class="number">0.00952398</span> <span class="number">0.00938275</span> <span class="number">0.00991478</span> <span class="number">0.01002662</span> <span class="number">0.01032722</span></span><br><span class="line">  <span class="number">0.01019795</span> <span class="number">0.00952248</span> <span class="number">0.00968466</span> <span class="number">0.0100937</span>  <span class="number">0.00989739</span> <span class="number">0.009971</span></span><br><span class="line">  <span class="number">0.01018309</span> <span class="number">0.00970648</span> <span class="number">0.01000668</span> <span class="number">0.00979022</span>]]</span><br><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure><blockquote><p>关于查看.pbnodename</p><p>除了上面的<code>opname = [tensor.name for tensor in tf.get_default_graph().as_graph_def().node]</code></p><p>还可以通过<code>tf.train.write_graph(sess.graph_def, &#39;./pb_model&#39;, &#39;lxbmodel.pb&#39;)</code>生成模型文件（文本文档），打开就能看结点。</p><p>还可以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.get_default_graph().as_graph_def())</span><br><span class="line"><span class="comment">#返回各个计算节点的详细信息,下面展示其中一个节点的信息</span></span><br></pre></td></tr></table></figure></blockquote><p>上面的代码中对图片的处理只是cv.imread后<code>tx.reshape(1, 224, 224, 3) / 255&#125;</code>，C++中（mat数据转换<a href="https://stackoverflow.com/questions/26681713/convert-mat-to-array-vector-in-opencv">参考</a>）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cv::Mat image = cv::<span class="built_in">imread</span>(<span class="string">&quot;E:/plane.jpg&quot;</span>);</span><br><span class="line">cv::Mat img2;</span><br><span class="line">image.<span class="built_in">convertTo</span>(img2, CV_32F);</span><br><span class="line">img2 = img2 / <span class="number">255</span>;</span><br><span class="line">std::vector&lt;<span class="type">float</span>&gt; vecHeight;</span><br><span class="line">vecHeight.<span class="built_in">assign</span>((<span class="type">float</span>*)img2.data, (<span class="type">float</span>*)img2.data + img2.<span class="built_in">total</span>() * img2.<span class="built_in">channels</span>());</span><br><span class="line"><span class="type">float</span>* input = <span class="keyword">new</span> <span class="type">float</span>[vecHeight.<span class="built_in">size</span>()];</span><br><span class="line"><span class="keyword">if</span> (!vecHeight.<span class="built_in">empty</span>())&#123;</span><br><span class="line">    <span class="built_in">memcpy</span>(input, &amp;vecHeight[<span class="number">0</span>], vecHeight.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//input即要传给context的float[]</span></span><br></pre></td></tr></table></figure><p>这个array打印出来是和python无差的。现在做到输入数据一致了。目前python调<code>.pb</code>和C++调<code>.trt</code>对<code>plane.jpg</code>图像推理得到的概率向量一致。使用的C++代码：（其中要注意容易错过的简单错误 比如<code>INPUT_H</code>等全局变量要改。）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvonnxparser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_runtime_api.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;logging.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;Windows.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(status) \</span></span><br><span class="line"><span class="meta">    do\</span></span><br><span class="line"><span class="meta">    &#123;\</span></span><br><span class="line"><span class="meta">        auto ret = (status);\</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (ret != 0)\</span></span><br><span class="line"><span class="meta">        &#123;\</span></span><br><span class="line"><span class="meta">            std::cerr &lt;&lt; <span class="string">&quot;Cuda failure: &quot;</span> &lt;&lt; ret &lt;&lt; std::endl;\</span></span><br><span class="line"><span class="meta">            abort();\</span></span><br><span class="line"><span class="meta">        &#125;\</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// stuff we know about the network and the input/output blobs</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INPUT_C = <span class="number">3</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_H = <span class="number">224</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> INPUT_W = <span class="number">224</span>;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">int</span> OUTPUT_SIZE = <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* INPUT_BLOB_NAME = <span class="string">&quot;data&quot;</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* OUTPUT_BLOB_NAME = <span class="string">&quot;prob&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> Logger gLogger;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="type">float</span>* input, <span class="type">float</span>* output, <span class="type">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">void</span>* buffers[<span class="number">2</span>] = &#123; <span class="literal">NULL</span>,<span class="literal">NULL</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">0</span>], batchSize * INPUT_H * INPUT_W * INPUT_C* <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line"></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(buffers[<span class="number">0</span>], input, batchSize * INPUT_H * INPUT_W * INPUT_C* <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;start to infer ...&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    context.<span class="built_in">enqueue</span>(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(output, buffers[<span class="number">1</span>], batchSize * OUTPUT_SIZE * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream and buffers</span></span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">0</span>]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(buffers[<span class="number">1</span>]));</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Inference Done.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="type">const</span> std::string&amp; engineFile, IHostMemory*&amp; trtModelStream, ICudaEngine*&amp; engine)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::fstream file;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;loading filename from:&quot;</span> &lt;&lt; engineFile &lt;&lt; std::endl;</span><br><span class="line">    nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">    <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">    file.<span class="built_in">open</span>(engineFile, std::ios::binary | std::ios::in);</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::end);</span><br><span class="line">    <span class="type">int</span> length = file.<span class="built_in">tellg</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;length:&quot;</span> &lt;&lt; length &lt;&lt; std::endl;</span><br><span class="line">    file.<span class="built_in">seekg</span>(<span class="number">0</span>, std::ios::beg);</span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">char</span>[]&gt; <span class="title">data</span><span class="params">(<span class="keyword">new</span> <span class="type">char</span>[length])</span></span>;</span><br><span class="line">    file.<span class="built_in">read</span>(data.<span class="built_in">get</span>(), length);</span><br><span class="line">    file.<span class="built_in">close</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;load engine done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserializing&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtRuntime = <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>());</span><br><span class="line">    <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">    engine = trtRuntime-&gt;<span class="built_in">deserializeCudaEngine</span>(data.<span class="built_in">get</span>(), length, <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;deserialize done&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The engine in TensorRT.cpp is not nullptr&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    trtModelStream = engine-&gt;<span class="built_in">serialize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span>&#123;</span><br><span class="line">    IHostMemory* modelStream&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    ICudaEngine* engine&#123; <span class="literal">nullptr</span> &#125;;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">read_TRT_File</span>(<span class="string">&quot;D:/code/python/pycharmProject/PytorchProj/saved_model/dense121/onnx_model/dense121_output100.trt&quot;</span>, modelStream, engine)) std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created successfully.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">//if (read_TRT_File(&quot;E:/SampleONNX-master/mobilenetv2.trt&quot;, modelStream, engine)) std::cout &lt;&lt; &quot;tensorRT engine created successfully.&quot; &lt;&lt; std::endl;</span></span><br><span class="line">    <span class="keyword">else</span> std::cout &lt;&lt; <span class="string">&quot;tensorRT engine created failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    IExecutionContext* context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">    <span class="built_in">assert</span>(context != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="comment">//查看engine的输入输出维度</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; engine-&gt;<span class="built_in">getNbBindings</span>(); i++)&#123;</span><br><span class="line">        nvinfer1::Dims dims = engine-&gt;<span class="built_in">getBindingDimensions</span>(i);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;index %d, dims: (&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; dims.nbDims; d++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (d &lt; dims.nbDims - <span class="number">1</span>)</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%d,&quot;</span>, dims.d[d]);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, dims.d[d]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;)\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//确定动态维度</span></span><br><span class="line">    nvinfer1::Dims dims4;</span><br><span class="line">    dims4.d[<span class="number">0</span>] = <span class="number">1</span>;    <span class="comment">// replace dynamic batch size with 1</span></span><br><span class="line">    dims4.d[<span class="number">1</span>] = <span class="number">224</span>;</span><br><span class="line">    dims4.d[<span class="number">2</span>] = <span class="number">224</span>;</span><br><span class="line">    dims4.d[<span class="number">3</span>] = <span class="number">3</span>;</span><br><span class="line">    dims4.nbDims = <span class="number">4</span>;</span><br><span class="line">    context-&gt;<span class="built_in">setBindingDimensions</span>(<span class="number">0</span>, dims4);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//get data</span></span><br><span class="line">    <span class="type">float</span> data[<span class="number">512</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="type">float</span> prob[<span class="number">100</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    cv::Mat image = cv::<span class="built_in">imread</span>(<span class="string">&quot;E:/plane.jpg&quot;</span>);</span><br><span class="line">    cv::Mat img2;</span><br><span class="line">    image.<span class="built_in">convertTo</span>(img2, CV_32F);</span><br><span class="line">    img2 = img2 / <span class="number">255</span>;</span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; vecHeight;</span><br><span class="line">    <span class="comment">//这里多维的mat文件转一维的float是在图像数据连续的情况下，等价于三层逐层压进去，具体可以看上方参考博客</span></span><br><span class="line">    vecHeight.<span class="built_in">assign</span>((<span class="type">float</span>*)img2.data, (<span class="type">float</span>*)img2.data + img2.<span class="built_in">total</span>() * img2.<span class="built_in">channels</span>());</span><br><span class="line">    <span class="type">float</span>* input = <span class="keyword">new</span> <span class="type">float</span>[vecHeight.<span class="built_in">size</span>()];</span><br><span class="line">    <span class="keyword">if</span> (!vecHeight.<span class="built_in">empty</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memcpy</span>(input, &amp;vecHeight[<span class="number">0</span>], vecHeight.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">500</span>; i++) &#123;</span><br><span class="line">        std::cout &lt;&lt; input[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Run inference</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">doInference</span>(*context, input, prob, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print histogram of the output distribution</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Output:\n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)&#123;</span><br><span class="line">        std::cout &lt;&lt; prob[i] &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy the engine</span></span><br><span class="line">    context-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    engine-&gt;<span class="built_in">destroy</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>加载cifar10数据集的dataloader，查看它的数据维度：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; batch : test_data_loader)&#123;</span><br><span class="line">    torch::Tensor inputs_tensor = batch.data;</span><br><span class="line">    torch::Tensor labels_tensor = batch.target;</span><br><span class="line">    torch::Tensor outputs_tensor;</span><br><span class="line">    <span class="type">float</span> outputs[<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">auto</span> a_size = inputs_tensor.<span class="built_in">sizes</span>();</span><br><span class="line">    <span class="type">int</span> num_ = inputs_tensor.<span class="built_in">numel</span>();</span><br><span class="line">    std::cout &lt;&lt; a_size &lt;&lt; std::endl &lt;&lt; num_ &lt;&lt; std::endl &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">auto</span> a_size2 = labels_tensor.<span class="built_in">sizes</span>();</span><br><span class="line">    <span class="type">int</span> num_2 = labels_tensor.<span class="built_in">numel</span>();</span><br><span class="line">    std::cout &lt;&lt; a_size2 &lt;&lt; std::endl &lt;&lt; num_2 &lt;&lt; std::endl &lt;&lt; std::endl;&#125;</span><br></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>参考<a href="https://bouzouitina-hamdi.medium.com/transfer-learning-with-keras-using-densenet121-fffc6bb0c233">这篇博客</a>，训练一个基于keras.application中的DenseNet网络的、处理Cifar10的模型，保存为了<code>.h5</code>格式。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> keras <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> datasets, layers, models</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">X, Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;pre-processes the data&quot;&quot;&quot;</span></span><br><span class="line">    X_p = X_p = K.applications.densenet.preprocess_input(X)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;one hot encode target values&quot;&quot;&quot;</span></span><br><span class="line">    Y_p = K.utils.to_categorical(Y, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> X_p, Y_p</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;load dataset&quot;&quot;&quot;</span></span><br><span class="line">(trainX, trainy), (testX, testy) = K.datasets.cifar10.load_data()</span><br><span class="line">x_train, y_train = preprocess_data(trainX, trainy)</span><br><span class="line">x_test, y_test = preprocess_data(testX, testy)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; USE DenseNet121&quot;&quot;&quot;</span></span><br><span class="line">OldModel = K.applications.DenseNet121(include_top=<span class="literal">False</span>,input_tensor=<span class="literal">None</span>,weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> OldModel.layers[:<span class="number">149</span>]:</span><br><span class="line">    layer.trainable = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> OldModel.layers[<span class="number">149</span>:]:</span><br><span class="line">    layer.trainable = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">model = K.models.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;a lambda layer that scales up the data to the correct size&quot;&quot;&quot;</span></span><br><span class="line">model.add(K.layers.Lambda(<span class="keyword">lambda</span> x:K.backend.resize_images(x,height_factor=<span class="number">7</span>,width_factor=<span class="number">7</span>,data_format=<span class="string">&#x27;channels_last&#x27;</span>)))</span><br><span class="line"></span><br><span class="line">model.add(OldModel)</span><br><span class="line">model.add(K.layers.Flatten())</span><br><span class="line">model.add(K.layers.BatchNormalization())</span><br><span class="line">model.add(K.layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(K.layers.Dropout(<span class="number">0.7</span>))</span><br><span class="line">model.add(K.layers.BatchNormalization())</span><br><span class="line">model.add(K.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(K.layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(K.layers.BatchNormalization())</span><br><span class="line">model.add(K.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(K.layers.Dropout(<span class="number">0.3</span>))</span><br><span class="line">model.add(K.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;callbacks&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># cbacks =  K.callbacks.CallbackList()</span></span><br><span class="line"><span class="comment"># cbacks.append(K.callbacks.ModelCheckpoint(filepath=&#x27;cifar10.h5&#x27;,monitor=&#x27;val_accuracy&#x27;,save_best_only=True))</span></span><br><span class="line"><span class="comment"># cbacks.append(K.callbacks.EarlyStopping(monitor=&#x27;val_accuracy&#x27;,patience=2))</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="string">&quot;&quot;&quot;train&quot;&quot;&quot;</span></span><br><span class="line">model.fit(x=x_train,y=y_train,batch_size=<span class="number">128</span>,epochs=<span class="number">5</span>,validation_data=(x_test, y_test))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.save(<span class="string">&#x27;cifar10.h5&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="h5模型转pb"><a href="#h5模型转pb" class="headerlink" title="h5模型转pb"></a>h5模型转pb</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework.convert_to_constants <span class="keyword">import</span> convert_variables_to_constants_v2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_h5to_pb</span>():</span><br><span class="line">    model = tf.keras.models.load_model(<span class="string">&quot;E:/cifar10.h5&quot;</span>,<span class="built_in">compile</span>=<span class="literal">False</span>)</span><br><span class="line">    model.summary()</span><br><span class="line">    full_model = tf.function(<span class="keyword">lambda</span> Input: model(Input))</span><br><span class="line">    full_model = full_model.get_concrete_function(tf.TensorSpec(model.inputs[<span class="number">0</span>].shape, model.inputs[<span class="number">0</span>].dtype))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get frozen ConcreteFunction</span></span><br><span class="line">    frozen_func = convert_variables_to_constants_v2(full_model)</span><br><span class="line">    frozen_func.graph.as_graph_def()</span><br><span class="line"></span><br><span class="line">    layers = [op.name <span class="keyword">for</span> op <span class="keyword">in</span> frozen_func.graph.get_operations()]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Frozen model layers: &quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">        <span class="built_in">print</span>(layer)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Frozen model inputs: &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(frozen_func.inputs)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Frozen model outputs: &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(frozen_func.outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save frozen graph from frozen ConcreteFunction to hard drive</span></span><br><span class="line">    tf.io.write_graph(graph_or_graph_def=frozen_func.graph,</span><br><span class="line">                      logdir=<span class="string">&quot;E:/&quot;</span>,</span><br><span class="line">                      name=<span class="string">&quot;cifar10.pb&quot;</span>,</span><br><span class="line">                      as_text=<span class="literal">False</span>)</span><br><span class="line">convert_h5to_pb()</span><br><span class="line"></span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">--------------------------------------------------</span><br><span class="line">Frozen model inputs: </span><br><span class="line">[&lt;tf.Tensor <span class="string">&#x27;Input:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>) dtype=float32&gt;]</span><br><span class="line">Frozen model outputs: </span><br><span class="line">[&lt;tf.Tensor <span class="string">&#x27;Identity:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">10</span>) dtype=float32&gt;]</span><br></pre></td></tr></table></figure><h3 id="pb转onnx"><a href="#pb转onnx" class="headerlink" title="pb转onnx"></a>pb转onnx</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python -m tf2onnx.convert  --input E:/cifar10.pb --inputs Input:0 --outputs Identity:0 --output E:/cifar10.onnx --opset 11</span><br><span class="line"></span><br><span class="line">python -m tf2onnx.convert  --input E:/cifar102.pb --inputs Input:0 --outputs Identity:0 --output E:/cifar102.onnx --opset 11</span><br><span class="line"></span><br><span class="line">python -m tf2onnx.convert  --input cifar10fix.pb --inputs Input:0 --outputs Identity:0 --output cifar10fix.onnx --opset 11</span><br></pre></td></tr></table></figure><h3 id="onnx转trt-1"><a href="#onnx转trt-1" class="headerlink" title="onnx转trt"></a>onnx转trt</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=cifar10.onnx --saveEngine=cifar10.trt --workspace=4096 --minShapes=Input:0:1x32x32x3 --optShapes=Input:0:1x32x32x3 --maxShapes=Input:0:50x32x32x3 --fp16</span><br><span class="line"></span><br><span class="line">trtexec --onnx=afs.onnx --saveEngine=afs.trt --workspace=4096 --minShapes=Input:0:1x5 --optShapes=Input:0:1x5 --maxShapes=Input:0:50x5 --fp16</span><br><span class="line"></span><br><span class="line">trtexec --onnx=dense121_6class.onnx --saveEngine=dense121_6class500.trt --workspace=3072 --minShapes=Input:0:1x128x64x1 --optShapes=Input:0:20x128x64x1 --maxShapes=Input:0:400x128x64x1 --fp16</span><br></pre></td></tr></table></figure><p>失败，shell报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">Input filename:   cifar10.onnx</span><br><span class="line">ONNX IR version:  0.0.6</span><br><span class="line">Opset version:    11</span><br><span class="line">Producer name:    tf2onnx</span><br><span class="line">Producer version: 1.11.1 1915fb</span><br><span class="line">Domain:</span><br><span class="line">Model version:    0</span><br><span class="line">Doc string:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">[07/28/2022-12:54:39] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">ERROR: builtin_op_importers.cpp:2593 In function importResize:</span><br><span class="line">[8] Assertion failed: (mode != &quot;nearest&quot; || nearest_mode == &quot;floor&quot;) &amp;&amp; &quot;This version of TensorRT only supports floor nearest_mode!&quot;</span><br><span class="line">[07/28/2022-12:54:39] [E] Failed to parse onnx file</span><br><span class="line">[07/28/2022-12:54:39] [E] Parsing model failed</span><br><span class="line">[07/28/2022-12:54:39] [E] Engine creation failed</span><br><span class="line">[07/28/2022-12:54:39] [E] Engine set up failed</span><br><span class="line">&amp;&amp;&amp;&amp; FAILED TensorRT.trtexec </span><br></pre></td></tr></table></figure><p>这是因为目前TensorRt的BUG：<a href="https://github.com/NVIDIA/TensorRT/issues/974#issuecomment-754323987">#974 (comment)</a>，不支持resize_image。（不支持的还有NonZero op is not supported in TRT yet。）</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211429730.png" alt="image-20220728203714761"></p><p>代码里使用的<code>keras.backend.resize_images</code><a href="https://docs.w3cub.com/tensorflow~2.3/keras/backend/resize_images">这个方法</a>使用的是 the <code>nearest</code> model + <code>half_pixel</code> + <code>round_prefer_ceil</code>，</p><p>一模一样的<a href="https://github.com/NVIDIA/TensorRT/issues/1061">issue</a> 。</p><p>解决：Lambda式子改成<code>model.add(K.layers.Lambda(lambda x:tf.image.resize(x,[224,224])))</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[07/29/2022-17:25:34] [I] Host Latency</span><br><span class="line">[07/29/2022-17:25:34] [I] min: 1.82153 ms (end to end 2.79663 ms)</span><br><span class="line">[07/29/2022-17:25:34] [I] max: 7.05655 ms (end to end 13.8956 ms)</span><br><span class="line">[07/29/2022-17:25:34] [I] mean: 1.93649 ms (end to end 3.66704 ms)</span><br><span class="line">[07/29/2022-17:25:34] [I] median: 1.90527 ms (end to end 3.60721 ms)</span><br><span class="line">[07/29/2022-17:25:34] [I] percentile: 2.2793 ms at 99% (end to end 4.26883 ms at 99%)</span><br><span class="line">[07/29/2022-17:25:34] [I] throughput: 0 qps</span><br><span class="line">[07/29/2022-17:25:34] [I] walltime: 3.00986 s</span><br><span class="line">[07/29/2022-17:25:34] [I] Enqueue Time</span><br><span class="line">[07/29/2022-17:25:34] [I] min: 0.943115 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] max: 1.9104 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] median: 0.970215 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] GPU Compute</span><br><span class="line">[07/29/2022-17:25:34] [I] min: 1.79199 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] max: 7.01645 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] mean: 1.89984 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] median: 1.86963 ms</span><br><span class="line">[07/29/2022-17:25:34] [I] percentile: 2.24359 ms at 99%</span><br><span class="line">[07/29/2022-17:25:34] [I] total compute time: 2.96756 s</span><br><span class="line">&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin\trtexec.exe --onnx=cifar10.onnx --saveEngine=cifar10.trt --workspace=4096 --minShapes=Input:0:1x32x32x3 --optShapes=Input:0:1x32x32x3 --maxShapes=Input:0:50x32x32x3 --fp16</span><br></pre></td></tr></table></figure><p>现在就得到了trt，可以开始跑测试集了~</p><blockquote><p>可能遇到的问题</p><p><a href="https://stackoverflow.com/questions/66355477/could-not-load-library-cudnn-ops-infer64-8-dll-error-code-126-please-make-sure">Please make sure cudnn_ops_infer64_8.dll is in your library path</a></p><p>Could not load library cudnn_cnn_infer64_8.dll. Error code 1455<br>Please make sure cudnn_cnn_infer64_8.dll is in your library path!<br>or <strong>context null</strong><br>原因：内存不足，重启VS或者电脑就OK。</p></blockquote><p>得到的准确率很低，然后跑出来的准确率还不是固定的。<br>看代码发现是float[]、vector、tensor相互转换的时候出了问题，还包括GPU内存拷贝上。</p><p>目前，output[]转output_tensor是一定有问题的，输出的值不一样。<br>其次是，每次得到output[]都不一样，有时会有nan的结果。比如像</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">outputs_vector</span><span class="params">(outputs, outputs + <span class="keyword">sizeof</span>(outputs) / <span class="keyword">sizeof</span>(<span class="type">float</span>))</span></span>;</span><br></pre></td></tr></table></figure><p>想把outputs[]转成vector，但转后只有前几个数一致。于是我通过<code>torch::form_blob</code>将outputs[]直接转成了tensor，而不是以vector为中介。</p><p>nan的原因是我初始化input[]之后没有给它赋值就传给context了。</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211430005.png" alt="image-20220803111138976"></p><p>happy个锤锤</p><p>目前多次测试得到的loss和acc不变了，这么低我想原因要么是精度问题要么是数组载入有问题，下一步打算c++和python测同样的几个样本看得到的输出向量情况，如果有出入大概率是精度问题。</p><h2 id="AFS模型保存转换记录"><a href="#AFS模型保存转换记录" class="headerlink" title="AFS模型保存转换记录"></a>AFS模型保存转换记录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m tf2onnx.convert  --input lxbtest.pb --inputs Placeholder:0 --outputs save/restore_all:0 --output lxbtest.onnx --opset 11</span><br></pre></td></tr></table></figure><p>把学波保存的pb模型转为onnx时报错：<code>ValueError: Input 0 of node save/AssignVariableOp was passed int32 from Variable:0 incompatible with expected resource.</code>。把学波保存的ckpt读完转成pb，然后再转onnx一样报错。然后发现，读取pb中计算图结点名称这样的一个操作都不行，也报上面的错。而其他正常的<code>.pb</code>文件则不会。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#打印出pb文件中所有node的name  一般来说，opname[0]就是输出结点name，opname[-1]则是输出节点name。</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line"><span class="keyword">with</span> gfile.FastGFile(<span class="string">r&#x27;saved_model/dense121/pb_model/dense121_output100.pb&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    graph_def = tf.GraphDef()</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line">    sess.graph.as_default()</span><br><span class="line">    tf.import_graph_def(graph_def, name=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    opname = [tensor.name <span class="keyword">for</span> tensor <span class="keyword">in</span> tf.get_default_graph().as_graph_def().node]</span><br><span class="line">    <span class="built_in">print</span>(opname)</span><br></pre></td></tr></table></figure><p>原因分析：<a href="https://github.com/onnx/tensorflow-onnx/issues/1152">不是所有的graph都能冻结</a>，可以冻结推理图但训练图不行，因为训练图除了执行变量读取外还有变量赋值。The error is saying that a node (likely a variable assignment node) was given a float (the frozen value of the variable) but was expecting a resource (the mutable variable).</p><h2 id="HRRP模型转换记录"><a href="#HRRP模型转换记录" class="headerlink" title="HRRP模型转换记录"></a>HRRP模型转换记录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">Input filename:   dense121_6class.onnx</span><br><span class="line">ONNX IR version:  0.0.6</span><br><span class="line">Opset version:    11</span><br><span class="line">Producer name:    tf2onnx</span><br><span class="line">Producer version: 1.11.1 1915fb</span><br><span class="line">Domain:</span><br><span class="line">Model version:    0</span><br><span class="line">Doc string:</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">[08/03/2022-17:53:29] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">[08/03/2022-17:53:30] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[08/03/2022-18:05:17] [I] [TRT] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[08/03/2022-18:05:17] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.3.0 but loaded cuBLAS/cuBLAS LT 11.2.1</span><br><span class="line">[08/03/2022-18:05:17] [I] Engine built in 715.435 sec.</span><br><span class="line"></span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.61235 ms - Host latency: 1.65127 ms (end to end 3.09309 ms, enqueue 0.889355 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.72634 ms - Host latency: 1.76223 ms (end to end 3.32732 ms, enqueue 0.8948 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.61448 ms - Host latency: 1.65164 ms (end to end 3.10913 ms, enqueue 0.896533 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.71921 ms - Host latency: 1.75725 ms (end to end 3.31399 ms, enqueue 0.932813 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Average on 10 runs - GPU latency: 1.61421 ms - Host latency: 1.64998 ms (end to end 3.10967 ms, enqueue 0.88999 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] Host Latency</span><br><span class="line">[08/03/2022-18:05:21] [I] min: 1.56018 ms (end to end 1.66077 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] max: 2.76453 ms (end to end 4.55258 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] mean: 1.72398 ms (end to end 3.229 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] median: 1.65479 ms (end to end 3.11627 ms)</span><br><span class="line">[08/03/2022-18:05:21] [I] percentile: 2.35864 ms at 99% (end to end 4.04886 ms at 99%)</span><br><span class="line">[08/03/2022-18:05:21] [I] throughput: 0 qps</span><br><span class="line">[08/03/2022-18:05:21] [I] walltime: 3.00615 s</span><br><span class="line">[08/03/2022-18:05:21] [I] Enqueue Time</span><br><span class="line">[08/03/2022-18:05:21] [I] min: 0.859131 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] max: 2.17993 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] median: 0.897461 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] GPU Compute</span><br><span class="line">[08/03/2022-18:05:21] [I] min: 1.52576 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] max: 2.72894 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] mean: 1.68527 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] median: 1.61768 ms</span><br><span class="line">[08/03/2022-18:05:21] [I] percentile: 2.32031 ms at 99%</span><br><span class="line">[08/03/2022-18:05:21] [I] total compute time: 2.96102 s</span><br><span class="line">&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin\trtexec.exe --onnx=dense121_6class.onnx --saveEngine=dense121_6class.trt --workspace=4096 --minShapes=Input:0:1x128x64x1 --optShapes=Input:0:1x128x64x1 --maxShapes=Input:0:100x128x64x1 --fp16</span><br></pre></td></tr></table></figure><p>拿到的<code>hdf5</code>模型，按之前的步骤转到trt很顺利，就是onnx to trt时间比较长。</p><h1 id="Pytorch模型的转换部署"><a href="#Pytorch模型的转换部署" class="headerlink" title="Pytorch模型的转换部署"></a>Pytorch模型的转换部署</h1><p>128reduce模型是<code>AlexNet_128</code>，256reduce模型是<code>IncrementalModel(256)</code><br>128模型是<code>AlexNet_128</code>，256模型是<code>AlexNet_256</code></p><p>首先是对pt文件到onnx的转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch_model = torch.load(<span class="string">&quot;save.pt&quot;</span>) <span class="comment"># pytorch模型加载</span></span><br><span class="line">batch_size = <span class="number">1</span>  <span class="comment">#批处理大小</span></span><br><span class="line">input_shape = (<span class="number">3</span>,<span class="number">244</span>,<span class="number">244</span>)   <span class="comment">#输入数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set the model to inference mode</span></span><br><span class="line">torch_model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">x = torch.randn(batch_size,*input_shape)<span class="comment"># 生成张量</span></span><br><span class="line">export_onnx_file = <span class="string">&quot;test.onnx&quot;</span><span class="comment"># 目的ONNX文件名</span></span><br><span class="line">torch.onnx.export(torch_model,</span><br><span class="line">                    x,</span><br><span class="line">                    export_onnx_file,</span><br><span class="line">                    opset_version=<span class="number">10</span>,</span><br><span class="line">                    do_constant_folding=<span class="literal">True</span>,<span class="comment"># 是否执行常量折叠优化</span></span><br><span class="line">                    input_names=[<span class="string">&quot;input&quot;</span>],<span class="comment"># 输入名</span></span><br><span class="line">                    output_names=[<span class="string">&quot;output&quot;</span>],<span class="comment"># 输出名</span></span><br><span class="line">                    dynamic_axes=&#123;<span class="string">&quot;input&quot;</span>:&#123;<span class="number">0</span>:<span class="string">&quot;batch_size&quot;</span>&#125;,<span class="comment"># 批处理变量</span></span><br><span class="line">                                  <span class="string">&quot;output&quot;</span>:&#123;<span class="number">0</span>:<span class="string">&quot;batch_size&quot;</span>&#125;&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>onnx到trt：</p><p>要注意，上面导出onnx时指定的批处理变量名要和下面转trt命令中的保持一致。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec.exe --explicitBatch --workspace=3072 --minShapes=input:1x1x128x1 --optShapes=input:20x1x128x1 --maxShapes=input:512x1x128x1 --onnx=increment_6_128_save_reduce.onnx --saveEngine=temp.trt --fp16</span><br></pre></td></tr></table></figure><h1 id="QT中配置并运行"><a href="#QT中配置并运行" class="headerlink" title="QT中配置并运行"></a>QT中配置并运行</h1><p>动态链接库不仅要在<code>LIBS += \</code>后面添加CUDA和TensorRT的lib文件夹路径，还要手动添加其中的必要库，否则在使用TensorRt推理时会报各种LNK无法解析的外部符号错误：</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211430725.png" alt="image-20220712131604604"></p><p>以上的错误手动添加<code>-lcudart</code>、<code>-lnvinfer</code>两个库就解决了。</p><h1 id="关于Python"><a href="#关于Python" class="headerlink" title="关于Python"></a>关于Python</h1><h2 id="Embedding-Python"><a href="#Embedding-Python" class="headerlink" title="Embedding Python"></a>Embedding Python</h2><p><a href="https://blog.csdn.net/zong596568821xp/article/details/115690713">起步代码博客</a></p><p>环境上 只是在编译命令添加了链接库：<code>g++ test2.cpp -o test2 -ID:/evn/Python39/include -LD:/evn/Python39/libs -lpython39</code></p><p>注意一些函数在不同的python版本中也不同，比如<a href="https://docs.python.org/3.9/c-api/call.html#c.PyObject_CallObject">PyObject_CallObject</a>的用法之类的。</p><p>有参数传递的调用：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// test2.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;Python.h&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">Py_Initialize</span>(); <span class="comment">//1、初始化python接口</span></span><br><span class="line">    <span class="comment">//初始化使用的变量</span></span><br><span class="line">    PyObject* pModule = <span class="literal">NULL</span>;</span><br><span class="line">    PyObject* pFunc = <span class="literal">NULL</span>;</span><br><span class="line">    PyObject* pName = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="comment">//2、初始化python系统文件路径，保证可以访问到 .py文件</span></span><br><span class="line">    <span class="built_in">PyRun_SimpleString</span>(<span class="string">&quot;import sys&quot;</span>);</span><br><span class="line">    <span class="built_in">PyRun_SimpleString</span>(<span class="string">&quot;sys.path.append(&#x27;./&#x27;)&quot;</span>);</span><br><span class="line">    <span class="comment">//3、调用python文件名。当前的测试python文件名是 myadd.py</span></span><br><span class="line">    <span class="comment">// 在使用这个函数的时候，只需要写文件的名称就可以了。不用写后缀。</span></span><br><span class="line">    pModule = <span class="built_in">PyImport_ImportModule</span>(<span class="string">&quot;myadd&quot;</span>);</span><br><span class="line">    <span class="comment">//4、调用函数</span></span><br><span class="line">    pFunc = <span class="built_in">PyObject_GetAttrString</span>(pModule, <span class="string">&quot;AdditionFc&quot;</span>);</span><br><span class="line">    <span class="comment">//5、给python传参数</span></span><br><span class="line">    <span class="comment">// 函数调用的参数传递均是以元组的形式打包的,2表示参数个数</span></span><br><span class="line">    <span class="comment">// 如果AdditionFc中只有一个参数时，写1就可以了</span></span><br><span class="line">    PyObject* pArgs = <span class="built_in">PyTuple_New</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="comment">// 0：第一个参数，传入 int 类型的值 2</span></span><br><span class="line">    <span class="built_in">PyTuple_SetItem</span>(pArgs, <span class="number">0</span>, <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;i&quot;</span>, <span class="number">2</span>)); </span><br><span class="line">    <span class="comment">// 1：第二个参数，传入 int 类型的值 4</span></span><br><span class="line">    <span class="built_in">PyTuple_SetItem</span>(pArgs, <span class="number">1</span>, <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;i&quot;</span>, <span class="number">4</span>)); </span><br><span class="line">    <span class="comment">// 6、使用C++的python接口调用该函数</span></span><br><span class="line">    PyObject* pReturn = <span class="built_in">PyEval_CallObject</span>(pFunc, pArgs);</span><br><span class="line">    <span class="comment">// 7、接收python计算好的返回值</span></span><br><span class="line">    <span class="type">int</span> nResult;</span><br><span class="line">    <span class="comment">// i表示转换成int型变量。</span></span><br><span class="line">    <span class="comment">// 在这里，最需要注意的是：PyArg_Parse的最后一个参数，必须加上“&amp;”符号</span></span><br><span class="line">    <span class="built_in">PyArg_Parse</span>(pReturn, <span class="string">&quot;i&quot;</span>, &amp;nResult);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;return result is &quot;</span> &lt;&lt; nResult &lt;&lt; endl;</span><br><span class="line">    <span class="comment">//8、结束python接口初始化</span></span><br><span class="line">    <span class="built_in">Py_Finalize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myadd.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AdditionFc</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Now is in python module&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; + &#123;&#125; = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(a, b, a+b))</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br></pre></td></tr></table></figure><p>虽然使用了<code>PyRun_SimpleString(&quot;import sys&quot;);PyRun_SimpleString(&quot;sys.path.append(&#39;./&#39;)&quot;);</code>但是会优先搜索本目录下同名的py文件进行调用，所以<strong>注意不要重名了</strong>。<br>现在发现一个导致不能调用的原因，py文件中import了环境中没有包，这个包你可能是在虚拟环境中安了，但global中没有。<br>这之后就搞定了，下面在cpp中调用 调用onnx执行推理的py文件。代码如下，正常执行。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// test.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;Python.h&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">Py_Initialize</span>(); </span><br><span class="line"></span><br><span class="line">    PyObject* pModule = <span class="literal">NULL</span>;</span><br><span class="line">    PyObject* pFunc = <span class="literal">NULL</span>;</span><br><span class="line">    PyObject* pName = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">PyRun_SimpleString</span>(<span class="string">&quot;import sys&quot;</span>);</span><br><span class="line">    <span class="built_in">PyRun_SimpleString</span>(<span class="string">&quot;sys.path.append(&#x27;D:/code/python/pycharmProject/PytorchProj/&#x27;)&quot;</span>);</span><br><span class="line">    <span class="comment">//PyRun_SimpleString(&quot;sys.path.append(&#x27;&#x27;)&quot;);</span></span><br><span class="line"></span><br><span class="line">    pModule = <span class="built_in">PyImport_ImportModule</span>(<span class="string">&quot;test3&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span>( pModule == <span class="literal">NULL</span> )&#123;</span><br><span class="line">cout &lt;&lt;<span class="string">&quot;pModule not found&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">    pFunc = <span class="built_in">PyObject_GetAttrString</span>(pModule, <span class="string">&quot;doinfer&quot;</span>);</span><br><span class="line">    PyObject* pArgs = <span class="built_in">PyTuple_New</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="type">char</span> datapath[]=<span class="string">&quot;E:/207Project/Data/HRRP/Ball_bottom_cone/00.txt&quot;</span>;</span><br><span class="line">    <span class="type">char</span> modelpath[]=<span class="string">&quot;E:/tfmodels/model.onnx&quot;</span>;</span><br><span class="line">    <span class="built_in">PyTuple_SetItem</span>(pArgs, <span class="number">0</span>, <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;s&quot;</span>, datapath)); </span><br><span class="line">    <span class="built_in">PyTuple_SetItem</span>(pArgs, <span class="number">1</span>, <span class="built_in">Py_BuildValue</span>(<span class="string">&quot;s&quot;</span>, modelpath)); </span><br><span class="line">    PyObject* pReturn = <span class="built_in">PyObject_CallObject</span>(pFunc, pArgs);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nResult;</span><br><span class="line">    <span class="comment">// i表示转换成int型变量。</span></span><br><span class="line">    <span class="comment">// PyArg_Parse的最后一个参数，必须加上“&amp;”符号</span></span><br><span class="line">    <span class="built_in">PyArg_Parse</span>(pReturn, <span class="string">&quot;i&quot;</span>, &amp;nResult);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;return result is &quot;</span> &lt;&lt; nResult &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">Py_Finalize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#test3.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> rt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTensorFromTXT</span>(<span class="params">filePath</span>):<span class="comment">#仅限于旧数据   数字与数字之间是空格</span></span><br><span class="line">    file_data=[]</span><br><span class="line">    t=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">if</span>(t&lt; <span class="number">2</span>):</span><br><span class="line">                t+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            a=line[line.rfind(<span class="string">&quot; &quot;</span>)+<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span>(a!=<span class="string">&quot;&quot;</span>):</span><br><span class="line">                file_data.append(<span class="built_in">float</span>(a))</span><br><span class="line">    <span class="comment">#outtensor=torch.tensor(file_data)</span></span><br><span class="line">    outtensor = np.array(file_data, np.float32)</span><br><span class="line">    outtensor=(outtensor-<span class="built_in">min</span>(outtensor)) / (<span class="built_in">max</span>(outtensor) - <span class="built_in">min</span>(outtensor))</span><br><span class="line">    outtensor = outtensor.reshape([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    <span class="keyword">return</span> outtensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">doinfer</span>(<span class="params">a,b</span>):</span><br><span class="line">    inputfile_path=<span class="built_in">str</span>(a)</span><br><span class="line">    modelfile_path=<span class="built_in">str</span>(b)</span><br><span class="line">    <span class="built_in">input</span> = getTensorFromTXT(inputfile_path)</span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.reshape([<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    sess = rt.InferenceSession(modelfile_path)</span><br><span class="line">    input_name = sess.get_inputs()[<span class="number">0</span>].name</span><br><span class="line">    label_name = sess.get_outputs()[<span class="number">0</span>].name</span><br><span class="line">    pred_onx = sess.run([label_name], &#123;input_name: <span class="built_in">input</span>.astype(np.float32)&#125;)[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(pred_onx)</span><br><span class="line">    <span class="built_in">print</span>(np.argmax(pred_onx))</span><br><span class="line">    <span class="keyword">return</span> np.argmax(pred_onx)</span><br><span class="line">doinfer(<span class="string">&quot;E:/207Project/Data/HRRP/Cone/00.txt&quot;</span>,<span class="string">&quot;E:/tfmodels/model.onnx&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="模型转换的脚本编写"><a href="#模型转换的脚本编写" class="headerlink" title="模型转换的脚本编写"></a>模型转换的脚本编写</h2><p>参考：</p><p><a href="https://blog.51cto.com/zhou123/1312791">python学习——python中执行shell命令</a><br><a href="https://tendcode.com/article/python-shell/">Python 命令行参数的3种传入方式</a></p><h1 id="常用API"><a href="#常用API" class="headerlink" title="常用API"></a>常用API</h1><h3 id="tensor转vector"><a href="#tensor转vector" class="headerlink" title="tensor转vector"></a>tensor转vector</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">output</span><span class="params">(output_tensor.data_ptr&lt;<span class="type">float</span>&gt;(),output_tensor.data_ptr&lt;<span class="type">float</span>&gt;()+output_tensor.numel())</span></span>;</span><br></pre></td></tr></table></figure><h3 id="vector与数组相互转"><a href="#vector与数组相互转" class="headerlink" title="vector与数组相互转"></a>vector与数组相互转</h3><p><a href="https://blog.csdn.net/Sagittarius_Warrior/article/details/54089242">https://blog.csdn.net/Sagittarius_Warrior/article/details/54089242</a></p><h3 id="vector转数组"><a href="#vector转数组" class="headerlink" title="vector转数组"></a>vector转数组</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> *buffer = <span class="keyword">new</span> <span class="type">float</span>[vecHeight.<span class="built_in">size</span>()];</span><br><span class="line"><span class="keyword">if</span> (!vecHeight.<span class="built_in">empty</span>())&#123;</span><br><span class="line">    <span class="built_in">memcpy</span>(buffer, &amp;vecHeight[<span class="number">0</span>], vecHeight.<span class="built_in">size</span>()*<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>1，vector作为动态数组，它的实现方法是：预先分配一个内存块，当感觉不够用的时候，再分配一个更大的内存块，然后自动将之前的数据拷贝到新的内存块中。</p><p>所以，出于效率考虑，如果实现知道待存储的数据长度，可以使用resize函数开辟足够的内存，避免后续的内存拷贝。</p><p>2，如果数组的元素是字符，建议使用string，而不是vector<char>。</p><h3 id="CV-Mat转Vector"><a href="#CV-Mat转Vector" class="headerlink" title="CV.Mat转Vector"></a>CV.Mat转Vector</h3><p><a href="https://stackoverflow.com/questions/26681713/convert-mat-to-array-vector-in-opencv">https://stackoverflow.com/questions/26681713/convert-mat-to-array-vector-in-opencv</a></p><h3 id="找到tensor转float数组的方法了！！！"><a href="#找到tensor转float数组的方法了！！！" class="headerlink" title="找到tensor转float数组的方法了！！！"></a>找到tensor转float数组的方法了！！！</h3><blockquote><p><a href="https://blog.csdn.net/weixin_43742643/article/details/116307036">LibTorch使用 accessor 快速访问 tensor</a></p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * x 的类型为 CPUFloatType &#123; 100, 100 &#125; </span></span><br><span class="line"><span class="comment"> * x_data.size(0) = 100</span></span><br><span class="line"><span class="comment"> * x_data.size(1) = 100</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">auto</span> x_data = x.<span class="built_in">accessor</span>&lt;<span class="type">float</span>, <span class="number">2</span>&gt;();</span><br><span class="line"><span class="comment">/* 访问单个元素 */</span></span><br><span class="line"><span class="type">float</span> x = x_data[<span class="number">50</span>][<span class="number">50</span>];</span><br><span class="line"><span class="comment">/* x_data.data() 是数据首地址 */</span></span><br><span class="line"><span class="type">float</span> array[<span class="number">100</span>][<span class="number">100</span>];</span><br><span class="line"><span class="built_in">memcpy</span>(array, x_data.<span class="built_in">data</span>(), <span class="number">100</span>*<span class="number">100</span>*<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="float数组转tensor"><a href="#float数组转tensor" class="headerlink" title="float数组转tensor"></a>float数组转tensor</h3><p>用torch::form_blob</p><h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h1 id="一些Cmake文件"><a href="#一些Cmake文件" class="headerlink" title="一些Cmake文件"></a>一些Cmake文件</h1><h2 id="经典文件（3070）"><a href="#经典文件（3070）" class="headerlink" title="经典文件（3070）"></a>经典文件（3070）</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.1</span> FATAL_ERROR)</span><br><span class="line"><span class="keyword">project</span>(cmakeProj)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_STANDARD <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#option(CUDA_USE_STATIC_CUDA_RUNTIME OFF)</span></span><br><span class="line"><span class="comment">#set(CMAKE_CXX_FLAGS &quot;-fsanitize=undefined -fsanitize=address&quot;)</span></span><br><span class="line"><span class="keyword">set</span>(CUDA_NVRTC_SHORTHASH <span class="string">&quot;XXXXXXXX&quot;</span>) <span class="comment">#resolve Failed to compute shorthash for libnvrtc.so</span></span><br><span class="line"><span class="comment">#find_package(CUDA REQUIRED)</span></span><br><span class="line"><span class="comment"># cuda</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include&quot;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorRT</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;D:/evn/TensorRT-7.2.3.4.Windows10.x86_64.cuda-11.1.cudnn8.1/TensorRT-7.2.3.4/include&quot;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;D:/evn/TensorRT-7.2.3.4.Windows10.x86_64.cuda-11.1.cudnn8.1/TensorRT-7.2.3.4/lib&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#OpenCV</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_PREFIX_PATH <span class="string">&quot;D:/evn/OpenCV/opencv-3.4.13-exe/opencv/build&quot;</span>)</span><br><span class="line"><span class="comment">#set(OpenCV_DIR /home/User/opencv/build/)</span></span><br><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;OPENCV_INCLUDE_DIRS&#125;</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment">#libtorch</span></span><br><span class="line"><span class="keyword">set</span>(Torch_DIR <span class="string">&quot;D:/evn/libtorch-1.8.2+cu111/libtorch&quot;</span>)</span><br><span class="line"><span class="keyword">find_package</span>(Torch REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="comment">#matlab</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;D:/softs/MATLAB/R2022a/extern/include&quot;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;D:/softs/MATLAB/R2022a/extern/lib/win64/microsoft&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;E:/207Project/GUI207_V2.0/lib/TRANSFER&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Python</span></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;D:/evn/Python39/include&quot;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="string">&quot;D:/evn/Python39/libs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(cmakeProj main.cpp ToHRRP.h)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_compile_features</span>(cmakeProj PUBLIC cxx_range_for)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(cmakeProj <span class="variable">$&#123;OpenCV_LIBS&#125;</span> <span class="variable">$&#123;TORCH_LIBRARIES&#125;</span> nvinfer libmat libmx libmex libeng  mclmcr mclmcrrt ToHRRP python39)</span><br></pre></td></tr></table></figure><h1 id="实时监测分类"><a href="#实时监测分类" class="headerlink" title="实时监测分类"></a>实时监测分类</h1><h2 id="关于Socket："><a href="#关于Socket：" class="headerlink" title="关于Socket："></a>关于Socket：</h2><p>代码参考：<a href="https://blog.csdn.net/qq_44184049/article/details/122291617">socket编程TCP/IP通信（windows下，C++实现）</a></p><p>环境配置参考：<a href="https://blog.csdn.net/Tona_ZM/article/details/82014294">vs C++实现Socket通信、添加ws2_32.lib 静态链接库</a>、<a href="https://blog.csdn.net/oliver_xi/article/details/115366307">用VScode 在Windows下写简单的socket通讯</a></p><p>理论参考：<a href="https://www.cnblogs.com/yskn/p/9335608.html">c++ 实时通信系统(基础知识TCP/IP篇)</a>、<a href="https://blog.csdn.net/JMW1407/article/details/108637540">计算机网络——网络字节序(大端字节序（Big Endian）\小端字节序（Little Endian）)</a></p><h2 id="关于多线程："><a href="#关于多线程：" class="headerlink" title="关于多线程："></a>关于多线程：</h2><h3 id="C-Thread"><a href="#C-Thread" class="headerlink" title="C++Thread"></a>C++Thread</h3><h3 id="QThread"><a href="#QThread" class="headerlink" title="QThread"></a>QThread</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//下面两种链接方式都可以捏</span></span><br><span class="line"><span class="built_in">connect</span>(inferThread, &amp;InferThread::sigInferResult,<span class="keyword">this</span>,&amp;MonitorPage::showInferResult);</span><br><span class="line"><span class="built_in">connect</span>(inferThread, <span class="built_in">SIGNAL</span>(<span class="built_in">sigInferResult</span>(QString)),<span class="keyword">this</span>,<span class="built_in">SLOT</span>(<span class="built_in">showInferResult</span>(QString)));</span><br></pre></td></tr></table></figure><h3 id="多线程之间传信号"><a href="#多线程之间传信号" class="headerlink" title="多线程之间传信号"></a>多线程之间传信号</h3><blockquote><p><a href="https://www.365seal.com/y/YNv9zwOVGd.html">【Qt】 Qt中实时更新UI程序示例</a></p></blockquote><p><strong>BUG：</strong></p><p>在子线程中连续调用terminal print 会导致没报错的错误（线程堵住？</p><p>在线程中连续使用matOpen会在第508次时打开失败。</p><h2 id="关于C-调用python"><a href="#关于C-调用python" class="headerlink" title="关于C++调用python"></a>关于C++调用python</h2><p>调用python函数，C传数据矩阵给python，Python绘制。参考博客：<a href="https://blog.csdn.net/weixin_41202834/article/details/118058413">c++调用python脚本，指针快速传递</a></p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202303211433541.png" alt="ixcuvgas"></p><h1 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h1><p>不能调用GPU训练的问题历程：</p><p>一开始是在cuda11.1的环境，cudnn版本忘了。conda TensorFlow2.3.0的环境下import TensorFlow然后<code>tf.config.list_physical_devices(&#39;GPU&#39;)</code>返回了空的设备列表。创了2.2.0的环境，提示说少各种cuxx.dll，网上找来往release目录放，最终还是有个dll提示缺少（虽然已经在了）。</p><p>于是转向根本问题：cuda、cudnn和TensorFlow的版本关系。参考博客<a href="https://blog.csdn.net/p_memory/article/details/121872480">快速配置tensorflow gpu环境（使用conda安装CUDA）</a></p><p>竟然可以在conda下隔离cuda环境，使用python3.8+cuda11.0+tensorflow2.4.0遂成功（conda install cudnn是不行的，不用单独下它。没关系）</p>]]></content>
      
      
      <categories>
          
          <category> 开发记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 记录 </tag>
            
            <tag> 项目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>精读SHAP</title>
      <link href="/2023/01/29/SHAP/"/>
      <url>/2023/01/29/SHAP/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Unified-Approach-to-Interpreting-Model-Predictions"><a href="#A-Unified-Approach-to-Interpreting-Model-Predictions" class="headerlink" title="A Unified Approach to Interpreting Model Predictions"></a>A Unified Approach to Interpreting Model Predictions</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>定义了<strong>加性特征归属度</strong>一类方法，是现有六种方法的集合。</li><li>加性特征归属度一类方法存在唯一的解决方案；提出SHAP值作为统一的特征重要性度量。</li><li>提出了SHAP值的估计方法，证明它是符合人认知的，通过对比可知它能更有效地区分模型输出类。</li></ul><hr><span id="more"></span><h2 id="Additive-Feature-Attribution-Methods"><a href="#Additive-Feature-Attribution-Methods" class="headerlink" title="Additive Feature Attribution Methods"></a>Additive Feature Attribution Methods</h2><p>这是一类将分类模型$f$预测的任何解释视为解释模型$g$本身的方法。（Local Method）</p><p>包含一个原始输入到简化输入的映射关系$x=h_x\left(x^{\prime}\right)$，</p><p>Local Method极力做到$g\left(z^{\prime}\right) \approx f\left(h_x\left(z^{\prime}\right)\right)$，$z^{\prime} \approx x^{\prime}$。</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301290952122.png" alt="image-20230129095255058"></p><p>LIME、DeepLIFT、Layer-Wise Relevance Propagation、Classic Shapley Value Estimation(包含以下三种)都满足上定义。</p><h3 id="Shapley-regression-values"><a href="#Shapley-regression-values" class="headerlink" title="Shapley regression values"></a><strong>Shapley regression values</strong></h3><p>每个特征值的Shapley值是该特征值对预测的贡献，通过对所有可能的特征值组合进行加权和求和得到：<br>$$<br>\phi_i=\sum_{S \subseteq F \backslash{i}} \frac{|S| !(|F|-|S|-1) !}{|F| !}\left[f_{S \cup{i}}\left(x_{S \cup{i}}\right)-f_S\left(x_S\right)\right]<br>$$<br>$\phi_0=f_{\varnothing}(\varnothing)$时，<em>Shapley regression values</em>满足Definition的等式(1)。</p><p>这样计算得到的Shapley值是唯一满足<strong>效率性（Efficiency）</strong>，<strong>对称性（Symmetry）</strong>，<strong>虚拟性（Dummy）</strong>和<strong>可加性（Additivity）</strong>四个属性的归因方法，它们可以一起被视为公平支出的定义。</p><p><strong>效率性</strong>：特征贡献的累加等于 x 的预测和预测平均值的差值。<br>$$<br>\sum_{j=1}^p \phi_j=\hat{f}(x)-E_X(\hat{f}(X))<br>$$<br><strong>对称性</strong>：如果两个特征值 j 和 k 的贡献对所有可能的联盟贡献相同，则它们的贡献应该相同。<br>$$<br>\begin{array}{lc}<br>\text { if } &amp; \operatorname{val}\left(S \cup\left{x_j\right}\right)=\operatorname{val}\left(S \cup\left{x_k\right}\right) \<br>\text { for all } &amp; S \subseteq\left{x_1, \cdots, x_p\right} \backslash\left{x_j, x_k\right} \<br>\text { then } &amp; \phi_j=\phi_k<br>\end{array}<br>$$<br><strong>虚拟性</strong>：一个不改变预测值的特征 j ，无论它添加到哪个特征值联盟中，Shapley值都应该为0。<br>$$<br>if \quad \operatorname{val}\left(S \cup\left{x_j\right}\right)=\operatorname{val}(S)\<br>forall \quad S \subseteq\left{x_1, \cdots, x_p\right}\<br>then \quad \phi_j=0$<br>$$<br><strong>可加性</strong>：对于具有组合支出的情景，相应的Shapley值应为：$\phi_j=\phi_j^{+}$</p><h3 id="Shapley-sampling-values"><a href="#Shapley-sampling-values" class="headerlink" title="Shapley sampling values"></a><strong>Shapley sampling values</strong></h3><p>通过抽样去近似估计<em>Shapley regression values</em>，因为精确的Shapley值必须通过使用和不使用第$i$个特征的所有可能特征联盟$S$来估计，当特征数较多时，可能的联盟数量会随着特征的增加而呈指数增长，<a href="https://link.springer.com/article/10.1007/s10115-013-0679-x">2014年</a>提出蒙特卡罗采样的近似值（<em>Shapley sampling values</em>）。</p><h3 id="Quantitative-Input-Influence"><a href="#Quantitative-Input-Influence" class="headerlink" title="Quantitative Input Influence"></a>Quantitative Input Influence</h3><p>A broader framework that addresses more than feature attributions.</p><hr><h2 id="Simple-Properties-Uniquely-Determine-Additive-Feature-Attributions"><a href="#Simple-Properties-Uniquely-Determine-Additive-Feature-Attributions" class="headerlink" title="Simple Properties Uniquely Determine Additive Feature Attributions"></a>Simple Properties Uniquely Determine Additive Feature Attributions</h2><p><em>Additive Feature Attribution Methods</em>存在一个唯一的能同时具有三种理想属性的解决方案：</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301291035953.png" alt="image-20230129103559903"></p><p>其满足：</p><p><strong>Properties1 Local accuracy局部准确</strong></p><p>对于每一个样本，各个特征的归因值与常数归因值之和等于模型的输出值f(x)<br>$$<br>f(x)=g\left(x^{\prime}\right)=\phi_0+\sum_{i=1}^M \phi_i x_i^{\prime}<br>$$<br><strong>Properties2 Missingness缺失性</strong></p><p>缺失性表示缺失特征的归因值为零，这里的缺失不是结构化数据的某个特征值是空，而是某个特征在实例中观察不到。对于一个结构化数据的实例，所有的$x_i^{\prime}$都为1。<br>$$<br>x_i^{\prime}=0 \Longrightarrow \phi_i=0<br>$$<br><strong>Properties3 Consistency一致性</strong><br>$$<br>\begin{array}{lc}<br>\text { if } &amp; \quad f_x^{\prime}\left(z^{\prime}\right)-f_x^{\prime}\left(z^{\prime} \backslash j\right) \geq f_x\left(z^{\prime}\right)-f_x\left(z^{\prime} \backslash j\right) \<br>\text { for all inputs } &amp; \quad z^{\prime} \in{0,1}^M\<br>\text { then } &amp; \quad \phi_j\left(f^{\prime}, x\right) \geq \phi_j(f, x)<br>\end{array}<br>$$<br>一致性属性表示，如果模型发生更改，使得特征值的边际贡献增加或保持不变（与其他特征无关），则归因值也会增加或保持不变。</p><blockquote><p><a href="https://link.zhihu.com/?target=https://link.springer.com/article/10.1007/BF01769885">Young</a>认为Shapley值的可加性和虚拟性可以用单调性代替，而一致性其实就是单调性，并且在论文中认为对于机器学习模型，单调性意味着对称性，所以一致性对应Shapley值的可加性、虚拟性和对称性。具体解释见<a href="https://link.zhihu.com/?target=https://stats.stackexchange.com/questions/389162/characteristics-of-shapley-values/389261%23389261">stackexchange</a>。</p></blockquote><hr><h2 id="SHAP-SHapley-Additive-exPlanation-Values"><a href="#SHAP-SHapley-Additive-exPlanation-Values" class="headerlink" title="SHAP (SHapley Additive exPlanation) Values"></a>SHAP (SHapley Additive exPlanation) Values</h2><p>是隶属于加性特征归属度方法的，不同于shapley回归的是预测解释为引入条件期望的每个特征的影响$\phi_j$的总和。</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301291834538.png" alt="image-20230129183424452"></p><p>定义$f_x(S)=E\left[f(x) \mid x_S\right]$，S是输入特征可能的子集合（Shapley值中提到的联盟），$E\left[f(x) \mid x_S\right]$是输入特征的子集S的条件期望值（Shapley值提到的val函数）。</p><ul><li>S为空集时，$\phi_0=f_x(\varnothing)=E[f(x)]$，即为模型预测值的期望，可以用训练样本的模型预测值的平均值近似.</li><li>接下来S顺序加入特征$x_{1}$，此时$\phi_1=f_x\left(\left{x_1\right}\right)-f_x(\varnothing)=E\left[f(x) \mid x_1\right]-E[f(x)]$，即$\left{x_1=a_1\right}$时的模型预测值期望 - 模型预测值期望.</li><li>然后S顺序加入特征$x_{2}$，此时$\phi_2=f_x\left(\left{x_1, x_2\right}\right)-f_x\left(\left{x_1\right}\right)=E\left[f(x) \mid x_1, x_2\right]-E\left[f(x) \mid x_1\right]$，即$\left{ x_1=a_1,x_2=a_2 \right} $时的模型预测值期望 -  $\left{ x_1=a_1 \right}$ 时的模型预测值期望.</li><li>直至加入最后一个特征$ x4 $，此时 $\phi_4=f_x(\left{ x_1,x_2,x_3,x_4 \right})-f_x(\left{ x_1,x_2,x_3 \right} )=E[f(x)|x_1,x_2,x_3,x_4]-E[f(x)|x_1,x_2,x_3] $，即$\left{ x_1=a_1,x_2=a_2,x_3=a_3,x_4=a_4 \right} $时的模型预测值期望 - $\left{ x_1=a_1,x_2=a_2,x_3=a_3 \right} $时的模型预测值期望，此时为四个特征单一排序下的预测值，其实就是样本的预测值.</li></ul><p>当模型是非线性的或输入特征不是独立时，SHAP值应该对所有可能的特征排序计算加权平均值。SHAP将这些条件期望与从博弈论的经典Shapley值组合到每个特征的归因值$\phi_j$ 中，也就是根据下式进行计算:<br>$$<br>\phi_j=\sum_{S \subseteq\left{x_1, \cdots, x_p\right} \backslash\left{x_j\right}} \frac{|S| !(p-|S|-1) !}{p !}\left(f_x\left(S \cup\left{x_j\right}\right)-f_x(S)\right)<br>$$</p><h3 id="Kernel-SHAP-Linear-LIME-Shapley-values"><a href="#Kernel-SHAP-Linear-LIME-Shapley-values" class="headerlink" title="Kernel SHAP (Linear LIME + Shapley values)"></a>Kernel SHAP (Linear LIME + Shapley values)</h3><p>以上提到的六种加性特征归属度方法除了Classic Shapley Value Estimation本就满足，其他都只满足Properties2。</p><blockquote><p>The following section proposes a unified approach that improves previous methods, preventing them from unintentionally violating Properties 1 and 3.</p></blockquote><p>对LIME修正使其满足Local accuracy和Consistency：</p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301292050642.png" alt="image-20230129205004552"></p><p>KernelSHAP是完全模型无关的，可以搭配任何算法去近似shaply值。</p><p>SHAP interaction valus</p><p>是shap值的拓展，可以展示各个特征之间的组合影响关系，比如通过一个矩阵显示出来。本来KernelSHAP的速度就慢，若要计算交互值最好使用treeSHAP否则会极慢。</p>]]></content>
      
      
      <categories>
          
          <category> 学习记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 精读论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023年做的腊肠!</title>
      <link href="/2023/01/26/%E8%85%8A%E8%82%A0%E5%88%B6%E4%BD%9C/"/>
      <url>/2023/01/26/%E8%85%8A%E8%82%A0%E5%88%B6%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>2023年初在家做了川广两式的腊肠，还是比较成功的，下面是制作流程和配比。</p><span id="more"></span><h2 id="广式（甜口）"><a href="#广式（甜口）" class="headerlink" title="广式（甜口）"></a>广式（甜口）</h2><ol><li>猪前腿肉洗净切片，肥瘦比根据喜好（比如3:7）。</li><li>加料。设总肉量n斤，则：<pre><code>盐：10n克白砂糖：20n克生抽：10n克高度白酒：15n克    （自己用的是牛栏山</code></pre></li><li>加完料，单一方向搅拌上劲。</li><li>盖上保鲜膜腌制一个小时至一夜。</li><li>灌肠  </li><li>放80度热水烫三四秒，拿出用牙签扎孔，开始风干，视天气，一般七到十天就可以吃了。</li></ol><h2 id="川式（五香辣）"><a href="#川式（五香辣）" class="headerlink" title="川式（五香辣）"></a>川式（五香辣）</h2><p>设总肉量n斤，则：</p><table><thead><tr><th>配料</th><th>重量/g</th></tr></thead><tbody><tr><td>盐</td><td>9n</td></tr><tr><td>白砂糖</td><td>8n</td></tr><tr><td>生抽</td><td>10n</td></tr><tr><td>高度白酒</td><td>10n</td></tr><tr><td>五香粉</td><td>4n</td></tr><tr><td>胡椒粉</td><td>n</td></tr><tr><td>花椒粉</td><td>n</td></tr><tr><td>味精</td><td>n</td></tr><tr><td>姜末</td><td>5n</td></tr></tbody></table><p>其他同广式。</p>]]></content>
      
      
      <categories>
          
          <category> 美食 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 美食 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>精读LIME</title>
      <link href="/2023/01/12/LIME/"/>
      <url>/2023/01/12/LIME/</url>
      
        <content type="html"><![CDATA[<h1 id="“Why-Should-I-Trust-You-”"><a href="#“Why-Should-I-Trust-You-”" class="headerlink" title="“Why Should I Trust You?”"></a>“Why Should I Trust You?”</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301122128844.png" alt="Untitled" style="zoom: 33%;" /><p>提出了一个可以解释任意分类模型预测结果的方法（局部代理模型）。</p><span id="more"></span><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>一句话介绍：训练一个基本的可解释模型使其在单个实例邻域附近的预测能力逼近原模型，通过训练这么一个可解释的模型来解释原模型在此实例上的预测。</p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301152030057.png" alt="image-20230115203022973" style="zoom: 33%;" /><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301152030778.png" alt="image-20230115203049738" style="zoom: 33%;" /><h2 id="Core1"><a href="#Core1" class="headerlink" title="Core1"></a>Core1</h2><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301141142018.png" alt="image-20230114114225945" style="zoom:50%;" />$$\xi(x)=\underset{g \in G}{\operatorname{argmin}} \mathcal{L}\left(f, g, \pi_x\right)+\Omega(g)$$<p>$$<br>\mathcal{L}\left(f, g, \pi_x\right)=\sum_{z, z^{\prime} \in \mathcal{Z}} \pi_x(z)\left(f(z)-g\left(z^{\prime}\right)\right)^2<br>$$</p><p>$$<br>\pi_x(z)=\exp \left(-D(x, z)^2 / \sigma^2\right)<br>$$</p><p>流程： </p><p>• 选择想要对其黑盒预测进行解释的感兴趣实例。  x</p><p>• 扰动数据集并获得这些新点的黑盒预测。Z</p><p>• 根据新样本与目标实例的接近程度对其进行加权。</p><p>• 在新数据集上训练加权的，可解释的模型。 </p><p>• 通过解释局部模型来解释预测。</p><h3 id="A-Interpretable-Data-Representations"><a href="#A-Interpretable-Data-Representations" class="headerlink" title="A. Interpretable Data Representations"></a>A. Interpretable Data Representations</h3><p>可解释的解释方法（interpretable explanation）需要以一种人能理解的表征方式，而不能直接是模型使用的feature。<br>作者将原数据x映射成interpretable version $x^{\prime}$</p><p>e.g. 图像分类中$x^{\prime}$可以是像素块（邻近像素组成的像素块）<strong>是否出现</strong>的二进制向量<br>e.g. 文本分类中$x^{\prime}$可以是表示词<strong>是否出现</strong>的二进制向量</p><p>后话：<br>向量长度也就是特征数K由Lasso算法确定和限制 ，这影响到解释模型g的复杂程度$\Omega(g)$<br>这种表征存在解释力不足的情况</p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301131520962.png" alt="image-20230113152005866" style="zoom:50%;" /><h3 id="B-Fidelity-Interpretability-Trade-off"><a href="#B-Fidelity-Interpretability-Trade-off" class="headerlink" title="B. Fidelity-Interpretability Trade-off"></a>B. Fidelity-Interpretability Trade-off</h3><p>f是待解释的原模型；g是待求解的可解释模型。<br>用基本的可解释模型g在模型f的局部做逼近，g可以是线性模型、决策树、下降规则集等。<br>（因为要做到模型无关的可解释，所以不对f做任何假设。<br>$\pi_x$ 是实例x的邻域，$\Omega(g)$ 是模型g复杂程度的度量（线性模型的非零权重数量、决策树的深度）<br>则我们的目的：在可理解的前提下尽量减小在由 $\pi_x$ 定义的某一Local field上g与f的差距即$\mathcal{L}$。<br>$$<br>\xi(x)=\underset{g \in G}{\operatorname{argmin}} \mathcal{L}\left(f, g, \pi_x\right)+\Omega(g)<br>$$</p><blockquote><p>we must minimize $\mathcal{L}$ while having $\Omega(g)$ be low enough to be interpretable by humans.</p></blockquote><h3 id="C-Sampling-for-Local-Exploration"><a href="#C-Sampling-for-Local-Exploration" class="headerlink" title="C. Sampling for Local Exploration"></a>C. Sampling for Local Exploration</h3><p>通过 $\pi_x$ 的加权得到$\mathcal{L}$，在实例x附近<strong>抽样</strong>得到z。</p><p>抽样方式：均匀随机抽取x中的非零元素（抽样次数也是均匀随机的）<br>$$<br>\pi_x(z)=\exp \left(-D(x, z)^2 / \sigma^2\right)<br>$$</p><p>其中D可以是cosine distance for text, L2 distance for images</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> kernel <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kernel</span>(<span class="params">d, kernel_width</span>):</span><br><span class="line">        <span class="keyword">return</span> np.sqrt(np.exp(-(d ** <span class="number">2</span>) / kernel_width ** <span class="number">2</span>))</span><br></pre></td></tr></table></figure><h3 id="D-Sparse-Linear-Explanations"><a href="#D-Sparse-Linear-Explanations" class="headerlink" title="D. Sparse Linear Explanations"></a>D. Sparse Linear Explanations</h3><p>论文中使用线性模型簇作为G。所以有 $g\left(z^{\prime}\right)=w_g \cdot z^{\prime}$</p><p>$$<br>\mathcal{L}\left(f, g, \pi_x\right)=\sum_{z, z^{\prime} \in \mathcal{Z}} \pi_x(z)\left(f(z)-g\left(z^{\prime}\right)\right)^2<br>$$<br>为了保证可解释性，需要对可解释表征加以限制K，在文本分类中K即bag of words中words的数量，在图像分类中即超级像素的数量。K由用户设定，论文中为定数。</p><p>Ω 的选取导致优化方程难以直接求解，作者使用Lasso（使用正则化路径）选择K个特征，然后最小二乘去让g近似f。</p><p><strong>缺陷：</strong></p><p>（interpretable representations）可解释表征存在解释力不足的问题，不能解释<br>（G）基本可解释模型比如线性模型，在原模型即便在局部也完全非线性时就失效了。<br>解释的不稳定，在模拟环境中两个很接近的点的解释差异很⼤。</p><h2 id="Core2"><a href="#Core2" class="headerlink" title="Core2"></a>Core2</h2><p>目的：</p><p>挑B个单独的实例解释去对模型有个整体的理解，实例们应尽可能地覆盖那些重要的特征，减少冗余。</p><p>用一个n*d的explanation matrix W表示对每个实例而言可解释成分的局部重要性。<br>$I_{j}$表示第j列的可解释成分在解释空间中的整体重要性，在文本应用中$I_j=\sqrt{\sum_{i=1}^n \mathcal{W}<em>{i j}}$，即一列求和。<br>下图中，$I</em>{2}&gt;I_{1}$是显然的，因为f2被用来解释更多的实例。</p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301141132629.png" alt="image-20230114113241480" style="zoom: 50%;" /><p>W对重要特征的覆盖率定义为集合函数c：computes the total importance of the features that appear in at least one instance in a set V .<br>$$<br>c(V, \mathcal{W}, I)=\sum_{j=1}^{d^{\prime}} \mathbb{1}<em>{\left[\exists i \in V: \mathcal{W}</em>{i j}&gt;0\right]} I_j<br>$$<br>去找一个覆盖率最高的实例集合V定义为Pick函数：<br>$$<br>\operatorname{Pick}(\mathcal{W}, I)=\underset{V,|V| \leq B}{\operatorname{argmax}} c(V, \mathcal{W}, I)<br>$$<br>此最大化加权覆盖函数是个NP-hard问题，所以使用贪心算法求V，即如果增加了实例i导致覆盖率提高，就将其并入V；<br>$$<br>V \leftarrow V \cup \operatorname{argmax}_i c(V \cup{i}, \mathcal{W}, I)<br>$$<br>SP-LIME算法就可以总结成以下：</p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301141550943.png" alt="image-20230114155052859" style="zoom: 60%;" />]]></content>
      
      
      <categories>
          
          <category> 学习记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 精读论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PicGo图床+Typora+Hexo</title>
      <link href="/2023/01/02/PicGo%E5%9B%BE%E5%BA%8A+Typora+Hexo/"/>
      <url>/2023/01/02/PicGo%E5%9B%BE%E5%BA%8A+Typora+Hexo/</url>
      
        <content type="html"><![CDATA[<h2 id="配置PicGo"><a href="#配置PicGo" class="headerlink" title="配置PicGo"></a>配置PicGo</h2><p><a href="https://picgo.github.io/PicGo-Doc/zh/guide/#picgo-is-here">下载地址</a>，打开软件配置仓库。</p><p>最好是专门建一个Github图床仓库，只用来放图片，仓库需设为Public。</p><p>*设定仓库名：Github用户名/仓库名</p><p>*设定分支名：分支名</p><p>*设定Token：Token为Github生成的Token。详情可参考<a href="https://blog.dgut.top/2020/07/10/hexo-pic/">博客Q</a>。</p><p>将它设置为默认图床。</p><p>前车之鉴：上传的图片名如果已经存在，则会<a href="">失败</a>，所以你可以在PicGo设置里开启时间戳重命名。</p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301021051972.png" alt="image-20230102105109905" style="zoom:67%;" /><p>打开Typora的设置，做以下设置。</p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301021103511.png" alt="image-20230102110349432" style="zoom: 50%;" /><p>这样之后你压根不用使用PicGo，QQ截一张图插到Typora的md文件里Typora就会自动将其上传，正常发布博客即可。</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 配置流程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GoodNotes+Google Drive+Notion文献跨平台同步管理方案</title>
      <link href="/2023/01/01/%E6%96%87%E7%8C%AE%E5%90%8C%E6%AD%A5/"/>
      <url>/2023/01/01/%E6%96%87%E7%8C%AE%E5%90%8C%E6%AD%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="文献同步与管理By-GoodNotes-amp-Google-Drive-amp-Notion-（Win）"><a href="#文献同步与管理By-GoodNotes-amp-Google-Drive-amp-Notion-（Win）" class="headerlink" title="文献同步与管理By GoodNotes&amp;Google Drive&amp;Notion （Win）"></a>文献同步与管理By GoodNotes&amp;Google Drive&amp;Notion （Win）</h1><p><strong>需求：</strong><u>在iPad上学习、批注文献，Windows端使用Notion对文献集中管理。文献要实时同步，Win端要能看到最新的批注情况。</u></p><p>探索了一段时间，尝试过Foxit、Notability、GoodNotes、PDFViewer、iCloud甚至百度云，但综合价格、批注习惯和生态等因素，用GoodNotes+Google Drive对文献进行同步是相对最适合我的。（0$哈哈哈）</p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301021028693.png" alt="image-20230101132249422" style="zoom:80%;" /><span id="more"></span><h2 id="文献同步"><a href="#文献同步" class="headerlink" title="文献同步"></a>文献同步</h2><ol><li><strong>GoodNotes开启GoogleDrive备份。</strong>Notes中所有内容会被同结构地备份到GoogleDrive下自动生成的GoodNotes文件夹中，至此实现了文件的云端备份。<br>顺其自然的，在电脑端直接浏览GoogleDrive里的文献就是最新批注的。<br>使用网页版GoogleDrive体验不如桌面版的，缓冲时间很不友好，有时网络问题甚至可能打不开了，因此：</li><li><strong>下载GoogleDrive Desktop，并将同步模式设置为镜像模式。</strong>此时你就可以指定一个目录镜像地存放Drive中所有文件。<br>之所以不使用Stream这种节省空间的模式是因为他会把文件挂到<code>xx/My Drive/</code>下，路径中有个带空格的My Drive！路径带空格根本忍不了，甚至如果你没有把系统改成英文的（如果你是家庭版Windows还不能改成英文系统！），安装下来的GoogleDrive只会是中文版的，他会把文件挂到<code>xx/我的云盘/</code>下，路径带中文！关键这个路径名不能被更改。</li></ol><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301021029094.png" alt="image-20230101125610629" style="zoom:50%;" /><p>我选择自定义的文件路径为：<code>E:/Google/Drive/</code>，那我GoodNotes里的论文就在本地的<code>E:/Google/Drive/GoodNotes/</code>文件夹下同步存在着。GoodNotes修改论文后自动同步到Drive里，电脑的Drive自动同步后，点击<code>E:/Google/Drive/GoodNotes/xxx.pdf</code>看到的就是最新的批注论文。</p><p>但是在电脑端对GoodNotes文件夹下的pdf修改后，GoodNotes是看不到修改的，并且GoodNotes对其修改后会覆盖掉。这个问题我这个方案是无解的，没办法，这受限于GoodNotes操作的文件本质是<em>GoodNotes File</em>而非PDF，鱼和熊掌不可兼得。</p><h2 id="文献管理"><a href="#文献管理" class="headerlink" title="文献管理"></a>文献管理</h2><p>使用Notion管理文献，需要在使用Notion时跳转到最新的批注文献（本地），比如上文中的<code>E:/Google/Drive/GoodNotes/xxx.pdf</code>，但是非会员限制单个文件&lt;5MB，会员好贵的！！！而且就真的是上传上去了，不能同步更新了。所以通过在Notion中嵌入本地文件链接，直接通过链接打开文件。使用Ngnix。</p><h3 id="下载并配置Nginx"><a href="#下载并配置Nginx" class="headerlink" title="下载并配置Nginx"></a>下载并配置Nginx</h3><p><a href="https://nginx.org/en/download.html">下载地址</a>，我选择的是Stable的<a href="https://nginx.org/download/nginx-1.22.1.zip">nginx/Windows-1.22.1</a>。下载并解压。</p><p>双击执行文件夹中nginx.exe，浏览器中输入并转到localhost，正常会显示Nginx的欢迎界面。  </p><p>下面配置安装目录下的<code>conf/nginx.conf</code>，主要是配置location。打开后在原server的location字段下添加新的location字段：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">location</span> <span class="string">/goodnotes/</span> &#123;</span><br><span class="line"><span class="string">alias</span>   <span class="string">E:/Google/Drive/GoodNotes/;</span><span class="comment"># 这里最后一个/要加上不然404</span></span><br><span class="line"><span class="string">autoindex</span> <span class="string">on;</span>  <span class="comment"># 开启自动适配全资源</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>这里alias和root是有区别的，我用了alias，root与alias主要区别在于nginx如何解释location后面的uri，这使两者分别以不同的方式将请求映射到服务器文件上。参考<a href="https://www.cnblogs.com/qingshan-tang/p/12763522.html">nginx配置静态资源访问</a></em></p><p>然后终端中输入<code>nginx -s stop</code>停止服务后<code>start nginx</code>开启服务。</p><blockquote><p>按说<code>nginx -s reload</code>就可以起到更新conf文件后重新加载nginx服务的作用，但是我这边实践证明reload多少有问题，conf没有得到更新还是老内容，所以还是先stop再start了。参考了<a href="https://blog.csdn.net/weixin_40908748/article/details/110140072">解决nginx退出后却依然能访问页面的问题</a></p><p><code>tasklist /fi &quot;IMAGENAME eq nginx.exe&quot;</code> 查看所有运行了的Nginx进程<br><code>taskkill /f /pid 16708</code>杀死PID16708的进程</p></blockquote><p>此时浏览器中输入<code>localhost/goodnotes/Interpretable/xxx.pdf</code>就可以打开本机的<code>E:/Google/Drive/GoodNotes/Interpretable/xxx.pdf</code>了。</p><p>设置开机自启动nginx服务：右键nginx.exe生成快捷方式，将快捷方式剪切到系统的启动文件夹下。(Win+R输入<strong>shell:startup</strong>跳转过去)</p><h3 id="Notion链接“本地”文献"><a href="#Notion链接“本地”文献" class="headerlink" title="Notion链接“本地”文献"></a>Notion链接“本地”文献</h3><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301021029195.png" alt="image-20230101145258586"></p><p><img src="https://raw.githubusercontent.com/Wyatthy/ForPicGo/main/images/202301021029325.gif" alt="demo"></p><p>至此，只实现了对pdf类型的跳转打开，期间尝试了Nginx - Shell Script CGI，但是没成功，网上大部分都是Linux或者苹果的博客。后续我会再试的。</p><blockquote><p>参考文章</p><p><a href="https://four2.site/articles/id19422.html">在 Notion 中插入本地文件和目录链接</a><br><a href="https://me.jinchuang.org/archives/114.html">Nginx支持web界面执行bash|python等系统命令和脚本</a><br><a href="https://www.cnblogs.com/qingshan-tang/p/12763522.html">nginx配置静态资源访问</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 配置流程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/03/31/OpenCV+Libtorch+VC/"/>
      <url>/2022/03/31/OpenCV+Libtorch+VC/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.1024sou.com/article/630721.html">https://www.1024sou.com/article/630721.html</a> Libtorch</p><p><a href="https://blog.csdn.net/qq_41175905/article/details/80560429">https://blog.csdn.net/qq_41175905/article/details/80560429</a> 、<a href="https://blog.csdn.net/clover_my/article/details/89521568">https://blog.csdn.net/clover_my/article/details/89521568</a>  OpenCV+VS</p><h2 id="c10-Error，位于内存位置xxx-处-原因汇总："><a href="#c10-Error，位于内存位置xxx-处-原因汇总：" class="headerlink" title="c10::Error，位于内存位置xxx 处  原因汇总："></a>c10::Error，位于内存位置xxx 处  原因汇总：</h2><p>1.读取数据集的部分，少读了图片，有没读到的图片被处理时遍历到</p><p>2.你的BatchSize不是train_dataset_size的整除数</p><p>3.第一层全连接的参数设置的不对，注意算完连接数后还得乘个batchsize才是真的数量。</p><p>VS切换CUDA版本<a href="https://www.cnblogs.com/xingzhensun/p/9154094.html">https://www.cnblogs.com/xingzhensun/p/9154094.html</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>美剧台词</title>
      <link href="/2022/02/09/%E7%BE%8E%E5%89%A7%E5%8F%B0%E8%AF%8D/"/>
      <url>/2022/02/09/%E7%BE%8E%E5%89%A7%E5%8F%B0%E8%AF%8D/</url>
      
        <content type="html"><![CDATA[<p>My heart is riding on fumes<br>我的心在燃烧</p><p>It is a soothing balm to my spent,scorched soul.<br>它是对我枯竭、焦灼的灵魂的抚慰剂</p>]]></content>
      
      
      <categories>
          
          <category> 英语 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Error汇总</title>
      <link href="/2022/02/09/%E5%90%84%E7%A7%8Derror/"/>
      <url>/2022/02/09/%E5%90%84%E7%A7%8Derror/</url>
      
        <content type="html"><![CDATA[<p>ImportError: cannot import name ‘_check_savefig_extra_args’ from ‘matplotlib.backend_bases’</p><p>重启jupyter</p><span id="more"></span>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>cuda入门</title>
      <link href="/2022/02/09/cuda/"/>
      <url>/2022/02/09/cuda/</url>
      
        <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p>首先要搞明白CUDA、CUDAToolkit….的关系</p><p>安装CUDA（cudatoolkit好像会顺带安上）</p><p>安装Pytorch</p><p>​    在<a href="https://pytorch.org/get-started/locally/#windows-installation">Pytorch官网</a>根据电脑环境生成<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes">合适的</a>命令下载直接<code>pip install torch</code>会下错成CPU版(报错<code>Torch not compiled with CUDA enabled</code> 当时我看了官方站的<a href="https://discuss.pytorch.org/t/torch-not-compiled-with-cuda-enabled/112467">问答</a>才明白）</p><p>验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="language-bash">&gt;&gt; import torch</span></span><br><span class="line"><span class="meta">&gt;</span><span class="language-bash">&gt;&gt; torch.cuda.is_available()</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;</span><span class="language-bash">&gt;&gt; torch.cuda.device_count()</span></span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span><span class="language-bash">&gt;&gt;</span></span><br></pre></td></tr></table></figure><p>共享存储器的应用：矩阵乘法</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixMulKernel</span><span class="params">(<span class="type">float</span>* Md, <span class="type">float</span>* Nd, <span class="type">float</span>* Pd, <span class="type">int</span> Width)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> Mds[TILE_WIDTH][TILE_WIDTH];</span><br><span class="line">    __shared__ <span class="type">float</span> Nds[TILE_WIDTH][TILE_WIDTH];</span><br><span class="line">    <span class="type">int</span> bx = blockIdx.x;  <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">int</span> tx = threadIdx.x; <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">int</span> Row = by * TILE_WIDTH + ty;</span><br><span class="line">    <span class="type">int</span> Col = bx * TILE_WIDTH + tx;</span><br><span class="line">    <span class="type">float</span> Pvalue = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> m = <span class="number">0</span>; m &lt; Width/TILE_WIDTH; ++m) &#123;</span><br><span class="line">        <span class="comment">//ty tx都是局限于线程所在的线程块   把Width宽的输入矩阵分成m个阶段 避免共享内存不够用</span></span><br><span class="line">        Mds[ty][tx] = Md[Row*Width + (m*TILE_WIDTH + tx)];</span><br><span class="line">        Nds[ty][tx] = Nd[Col + (m*TILE_WIDTH + ty)*Width];</span><br><span class="line">        __syncthreads();<span class="comment">//此处同步 将m阶段的Mds和Nds都填满了（TILE_WIDTH*TILE_WIDTH个线程共同的努力）</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; TILE_WIDTH; ++k)</span><br><span class="line">            Pvalue += Mds[ty][k] * Nds[k][tx];</span><br><span class="line">        __synchthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    Pd[Row*Width+Col] = Pvalue;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo配置</title>
      <link href="/2022/02/09/hexo%E9%85%8D%E7%BD%AE/"/>
      <url>/2022/02/09/hexo%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="入手"><a href="#入手" class="headerlink" title="入手"></a>入手</h1><h2 id="一、安装好Node-js和Hexo"><a href="#一、安装好Node-js和Hexo" class="headerlink" title="一、安装好Node.js和Hexo"></a>一、安装好Node.js和Hexo</h2><p>Hexo基于Node.js，下载地址：<a href="https://nodejs.org/en/download/">Download | Node.js</a> 下载安装包，注意安装Node.js会包含环境变量及npm的安装，安装后，检测是否安装成功，在命令行中输入 node -v 和npm -v以检测。</p><p>到这了，安装Hexo的环境已经全部搭建完成。</p><p>使用npm命令安装Hexo，输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli </span><br></pre></td></tr></table></figure><span id="more"></span><h2 id="二、"><a href="#二、" class="headerlink" title="二、"></a>二、</h2><blockquote><p>站点配置文件：<code>D:\Blog\_config.yml</code></p><p>主题配置文件：<code>D:\Blog\themes\next\_config.yml</code></p><p>命令简写：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hexo n <span class="string">&quot;我的博客&quot;</span> == hexo new <span class="string">&quot;我的博客&quot;</span> <span class="comment">#新建文章</span></span><br><span class="line">hexo g == hexo generate <span class="comment">#生成</span></span><br><span class="line">hexo s == hexo server <span class="comment">#启动服务预览</span></span><br><span class="line">hexo d == hexo deploy <span class="comment">#部署</span></span><br><span class="line">hexo server <span class="comment">#Hexo会监视文件变动并自动更新，无须重启服务器</span></span><br><span class="line">hexo server -s <span class="comment">#静态模式</span></span><br><span class="line">hexo server -p 5000 <span class="comment">#更改端口</span></span><br><span class="line">hexo server -i 192.168.1.1 <span class="comment">#自定义 IP</span></span><br><span class="line">hexo clean <span class="comment">#清除缓存，若是网页正常情况下可以忽略这条命令</span></span><br></pre></td></tr></table></figure></blockquote><p>新建仓库，命名为：<code>xxx.github.io</code>（固定用法）</p><p>本地建文件夹，进入目录初始化Hexo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>此时可以使用 <code>hexo s</code>进行预览</p><p>将我们的Hexo与GitHub关联起来，打开站点的配置文件_config.yml，翻到最后修改为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: git@github.com:Wyatthy/wyatthy.github.io.git</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure><p>其实就是给hexo d 这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。最后安装Git部署插件，输入命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>这时，我们分别输入三条命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean </span><br><span class="line">hexo g </span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>hexo d 就是部署网站命令。完成后，打开浏览器，在地址栏输入你的放置个人网站的仓库路径，即 <a href="http://xxxx.github.io/">http://xxxx.github.io</a></p><h2 id="三、"><a href="#三、" class="headerlink" title="三、"></a>三、</h2><h3 id="添加tags页面"><a href="#添加tags页面" class="headerlink" title="添加tags页面"></a>添加tags页面</h3><p>在主题配置文件的menu setting 中把tags属性去掉注释</p><p>创建一个<code>tags</code>页面：在<code>Hexo\Blog</code>目录下右键打开<code>Git Bush</code>，输入<code>hexo new page tags</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">theigrams@Theigrams MINGW64 /c/D/Hexo/Blog</span><br><span class="line">$ hexo new page tags</span><br><span class="line">INFO  Created: C:\D\Hexo\Blog\<span class="built_in">source</span>\tags\index.md</span><br></pre></td></tr></table></figure><p>这时候你的<code>source/</code>下会生成 <code>tags/index.md</code> 文件，我们将其打开，然后在最后面加上两行：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">type:</span> <span class="string">&quot;tags&quot;</span></span><br><span class="line"><span class="attr">comments:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>这时候你要为你的文章打上标签就可以在文章的头部写上：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tags:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Tag1</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Tag2</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Tag3</span></span><br></pre></td></tr></table></figure><h3 id="添加分类和自我介绍"><a href="#添加分类和自我介绍" class="headerlink" title="添加分类和自我介绍"></a>添加分类和自我介绍</h3><p>同上，先把 <code>categories</code> <code>about</code> 前的注释符 <code>#</code> 删了，然后创建页面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo new page categories</span><br><span class="line">hexo new page about</span><br></pre></td></tr></table></figure><p>打开<code>categories/index.md</code> 和 <code>about/index.md</code> 文件，在最下面加上分别两行：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">type: &quot;categories&quot;</span><br><span class="line">comments: false</span><br><span class="line">type: &quot;about&quot;</span><br><span class="line">comments: false</span><br></pre></td></tr></table></figure><p>这时候你就可以给你的文章归类存档了，使用方式就是在你的文章的头部加上：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">categories:</span><br><span class="line">    - 分类1</span><br></pre></td></tr></table></figure><h3 id="正文预览"><a href="#正文预览" class="headerlink" title="正文预览"></a>正文预览</h3><p>文章插入<code>&lt;!-- more --&gt;</code></p><h1 id="个性化设置"><a href="#个性化设置" class="headerlink" title="个性化设置"></a>个性化设置</h1><h2 id="修改文档底部分类样式"><a href="#修改文档底部分类样式" class="headerlink" title="修改文档底部分类样式"></a>修改文档底部分类样式</h2><p>修改模板<code>/themes/next/layout/_macro/post.swig</code>，搜索 <code>rel=&quot;tag&quot;&gt;#</code>或者<code>tag_indicate</code>，将 <code>#</code> 换成或者将<code>tag_indicate</code> Set成<code>&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;</code></p><h2 id="去掉博客底部powered-by"><a href="#去掉博客底部powered-by" class="headerlink" title="去掉博客底部powered by"></a>去掉博客底部powered by</h2><p>把<code>\themes\next\layout\_partials\footer.swig</code>中的<code>if theme.footer.powered</code>句段用<code>&#123;# #&#125;</code>注释掉</p><h2 id="开启站点统计"><a href="#开启站点统计" class="headerlink" title="开启站点统计"></a>开启站点统计</h2><p>我是用的是百度统计，注册百度统计后绑定你的博客网址，得到代码，拿到</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hm.src = &quot;https://hm.baidu.com/hm.js?xxxxxxxxx&quot;;</span><br></pre></td></tr></table></figure><p>注：Next主题已对百度统计进行配置优化，只需要配置主题配置文件即可</p><p>代码段中的<code>xxxxxxxxxxx</code>，将其添加到主题配置文件<code>_config.yml</code>的<code>baidu_analytics</code>后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Baidu Analytics</span><br><span class="line">baidu_analytics: xxxxxxxxxxxxxxx # &lt;app_id&gt;</span><br></pre></td></tr></table></figure><p>就好了</p><h2 id="添加站内搜索"><a href="#添加站内搜索" class="headerlink" title="添加站内搜索"></a>添加站内搜索</h2><p>安装<code>hexo-generator-searchdb</code>插件  <a href="https://github.com/wzpan/hexo-generator-search">github地址</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure><p>编辑<code>_config.yml</code>站点配置文件，新增以下内容到任意位置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure><p>编辑<code>themes/next/_config.yml</code> 主题配置文件，启用本地搜索功能,将<code>local_search:</code>下面的<code>enable:</code>的值，改成<code>true</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Local search</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure><h1 id="移植到新电脑"><a href="#移植到新电脑" class="headerlink" title="移植到新电脑"></a>移植到新电脑</h1><p>完成入手篇的第一节即下载完hexo之后在新的博客路径下执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>Hexo博客系统中影响迁移的只有一部分文件和文件夹，所以只需要备份它们即可</p><table><thead><tr><th>文件（夹）</th><th>说明</th></tr></thead><tbody><tr><td>scaffolds/</td><td>博客文章的模版</td></tr><tr><td>source/</td><td>所有博客文章，以及about、tags、categories等page</td></tr><tr><td>themes/</td><td>网站的主题所在文件夹</td></tr><tr><td>.gitignore</td><td>在push时需要忽略的文件和文件夹</td></tr><tr><td>_config.yml</td><td>站点配置文件</td></tr><tr><td>package.json</td><td>依赖包的名称和版本号</td></tr></tbody></table><p>将原来博客文件夹中的<br>_config.yml<br>theme/<br>source/<br><em>scaffolds/</em><br><em>package.json</em><br><em>.gitignore</em><br>复制到新的博客路径下</p><p>这就好了，先hexo g生成一下再hexo s预览一下看看效果  –End</p>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 配置流程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
